\documentclass[11pt,a4paper,twoside]{article}

\usepackage[T1]{fontenc} % sonst geht \hyphenation nicht mit Umlauten
\usepackage[latin1]{inputenc} % man kann schreiben äöüß, statt "a"o"u"s
%\usepackage[utf8]{inputenc} % wie oben, aber UTF-8 als Encoding statt ISO-8859-1 (latin1)
\usepackage[ngerman,english]{babel} % deutsche Trennregeln, "Inhaltsverzeichnis" etc.
%\usepackage{ngerman} % Alternative zum Babel-Paket oben
\usepackage{mathptmx} % Times-Roman-Schrift (auch für mathematische Formeln)
\usepackage{framed}
\usepackage{longtable}
\usepackage{tabu}


% Zum Setzen von URLs
\usepackage{color}
\definecolor{darkred}{rgb}{.25,0,0}
\definecolor{darkgreen}{rgb}{0,.2,0}
\definecolor{darkmagenta}{rgb}{.2,0,.2}
\definecolor{darkcyan}{rgb}{0,.15,.15}
\usepackage[plainpages=false,bookmarks=true,bookmarksopen=true,colorlinks=true,
  linkcolor=darkred,citecolor=darkgreen,filecolor=darkmagenta,
  menucolor=darkred,urlcolor=darkcyan]{hyperref}

% pdflatex: Bilder in den Formaten .jpeg, .png und .pdf
% latex: Bilder im .eps-Format
\usepackage{graphicx}
\usepackage{subcaption}
\usepackage{sidecap}
\usepackage{listings}

\usepackage{fancyhdr} % Positionierung der Seitenzahlen
\fancyhead[LE,RO,LO,RE]{}
\fancyfoot[CE,CO,RE,LO]{}
\fancyfoot[LE,RO]{\Roman{page}}
\renewcommand{\headrulewidth}{0pt}
\setlength{\headheight}{13.6pt} % behebt headheight Warning

% Korrektes Format für Nummerierung von Abbildungen (figure) und
% Tabellen (table): <Kapitelnummer>.<Abbildungsnummer>
\makeatletter
\@addtoreset{figure}{section}
\renewcommand{\thefigure}{\thesection.\arabic{figure}}
\@addtoreset{table}{section}
\renewcommand{\thetable}{\thesection.\arabic{table}}
\makeatother

\sloppy % Damit LaTeX nicht so viel über "overfull hbox" u.Ä. meckert

% Ränder
\addtolength{\topmargin}{-16mm}
\setlength{\oddsidemargin}{25mm}
\setlength{\evensidemargin}{35mm}
\addtolength{\oddsidemargin}{-1in}
\addtolength{\evensidemargin}{-1in}
\setlength{\textwidth}{15cm}
\addtolength{\textheight}{34mm}
%______________________________________________________________________

\begin{document}

\pagestyle{empty} % Vorerst keine Seitenzahlen
\pagenumbering{alph} % Unsichtbare alphabetische Nummerierung

\begin{center}
\textsc{Ludwig-Maximilians-Universität München}\\
Department ``Institut für Informatik''\\
Lehr- und Forschungseinheit Medieninformatik\\
Prof.\ Dr.\ Heinrich Hußmann

\vspace{5cm}
{\large\textbf{Masterarbeit}}\vspace{.5cm}

{\LARGE Entwicklung eines Systems zur Nutzung von VR-Brillen im Unterricht}\vspace{1cm}

{\large Veronika Fuchsberger}\\\href{mailto:veronika.fuchsberger@campus.lmu.de}{veronika.fuchsberger@campus.lmu.de}

\end{center}
\vfill

\begin{tabular}{ll}
Bearbeitungszeitraum: & 01.08.2018 bis 30.01.2018\\
Betreuer: & StR Christoph Krichenbauer\\
Verantw. Hochschullehrer: & Prof. Dr. Heinrich Hußmann
\end{tabular}
%______________________________________________________________________

\clearpage
\section*{Zusammenfassung}
Die Arbeit setzt sich mit dem Einsatz von Virtual Reality-Headsets im Schulunterricht auseinander. Dazu wurden aktuelle VR-Headsets verglichen und nach Preissegmenten und Anwendungsfunktionen eingeordnet. Zudem wurde bisher existierende Software, die Virtual Reality für den Schulunterricht zugänglich macht diskutiert und bewertet. Herausforderungen für Virtual Reality-Systeme im Generellen und im Speziellen für den Einsatz im  Schulunterricht werden diskutiert.
Weiterhin geht die Arbeit auf verschiedenen Inhaltstypen für VR-Systeme ein, welche Quellen es für diese Inhalte gibt und wie sie bearbeitet und selbst erstellt werden können.

Anschließend wurde basierend auf den zuvor erörterten Anforderungen ``VRClassroom'' entwickelt. Eine Software die Lehrkräfte im Unterricht nutzen können, um 360°-Inhalte zu präsentieren. Das System besteht aus zwei Komponenten: Die Lehrkraft kann an einem Computer Inhalte laden, die dann in einer Web-App auf VR-Brillen der Schüler gezeigt werden. Die Lehrkräfte führen dabei das komplette VR-Erlebnis für die Schüler und können 360°-Fotos und -Videos laden und Markierungen darauf setzen. Das Video wird bei allen Schülern synchron abgespielt und kann von der Lehrkraft pausiert werden. Außerdem können 3D-Modelle in die App geladen werden, die die Lehrkraft skalieren und drehen kann und an interessanten Stellen Markierungen setzen. Um zu sehen, welche Geräte verbunden sind, wird in der Lehrkraft ein Liste mit allen verbundenen Geräten und einem Aktivitätsindikator angezeigt. 

Anschließend wurde eine Online-Befragung mit Lehrkräften durchgeführt, um die entwickelte Software zu evaluieren. Den Teilnehmern wurde ein Video gezeigt, das die Nutzung von VRClassroom erklärt und anschließend ein Fragebogen vorgelegt. Die Mehrheit der Teilnehmer war dem System gegenüber sehr aufgeschlossen und konnte sich gut vorstellen VRClassroom zukünftig im Unterricht zu einzusetzen, um 360°-Inhalte zu zeigen.

Abschließend wird diskutiert wie Zukunft von Virtual Reality im Schulunterricht aussehen könnte. Dazu werden auch mögliche Weiterentwicklungen von VRClassroom besprochen.

\selectlanguage{english}
\section*{Abstract}
This thesis' topic is the use of virtual reality headsets in education. The first chapter gives an overview of recent VR headsets and compares them in functionality, retail price and other characteristics. Afterwards, existing VR software for education is discussed and assessed.

Based on the gathered information VRClassroom was developed. VRClassroom is a software which allows teachers to present their students 360° content while keeping an eye on their class. The software consists of two components: The teacher app, which is used to select the content, and the students app, presenting the content selected by the teacher.
The teacher app lists all connected student devices and their activity status, shows the current content and offers controls to interact with it. Teachers can present 360° photos, 360° videos and 3D models using VRClassroom. Google StreetView panoramas can be imported, too. 360° videos are played synchronously across all devices once the teacher hits play. 3D models can be scaled and rotated by the teacher. Additionally, the teacher can place markers on all content types.

The resulting software was evaluated with 59 teachers. They were presented with a video explaining the use of VRClassroom and asked to answer a questionnaire afterwards. The majority of participants liked VRClassroom and could see themselves using VRClassroom in their future lessons.

The final chapters discusses the future of virtual reality in education and possible improvements to VRClassroom.

\selectlanguage{ngerman}
\clearpage

\subsection*{Lizenzierung}
Diese Arbeit ist unter einer Creative Commons Namensnennung - Weitergabe unter gleichen Bedingungen 4.0 International (CC BY-SA 4.0) Lizenz veröffentlicht.
Der Quellcode zum Softwareprojekt VRClassroom steht unter der GNU General Public License v3.0.

\vfill % Sorgt dafür, dass das Folgende an das Seitenende rutscht

\noindent Ich erkläre hiermit, dass ich die vorliegende Arbeit
selbstständig angefertigt, alle Zitate als solche kenntlich gemacht
sowie alle benutzten Quellen und Hilfsmittel angegeben habe.

\bigskip\noindent München, \today

\vspace{4ex}\noindent\makebox[7cm]{\dotfill}

%______________________________________________________________________

\cleardoublepage
\pagestyle{fancy}
\pagenumbering{roman} % Römische Seitenzahlen
\setcounter{page}{1}

% Inhaltsverzeichnis erzeugen
\tableofcontents

%Abbildungsverzeichnis erzeugen - normalerweise nicht nötig
%\cleardoublepage
%\listoffigures
%______________________________________________________________________

\cleardoublepage

% Arabische Seitenzahlen
\pagenumbering{arabic}
\setcounter{page}{1}
% Geändertes Format für Seitenränder, arabische Seitenzahlen
\fancyhead[LE,RO]{\rightmark}
\fancyhead[LO,RE]{\leftmark}
\fancyfoot[LE,RO]{\thepage}

\section{Einleitung}
Mit den fallenden Preisen für Endgeräte wird Virtual Reality für ein breiteres Publikum zugänglich. Dadurch ergeben sich auch neue Einsatzfelder für die Nutzung von VR-Anwendungen im Alltag.
Einer dieser Bereiche ist die Nutzung von Virtual Reality im Schulunterricht: Eine Klasse mit Geräten auszustatten, erlaubt es Schülerinnen und Schülern so auf eine neue Art lernen können und Inhalte besser zu verstehen.
Welche Ansätze es für die Nutzung von Virtual Reality bereits existieren werden später in dieser Arbeit behandelt. Zunächst werden die Begriffe Virtual Reality und Augmented Reality differenziert und erläutert, wie VR-Headsets aufgebaut sind.

\subsection{Definition von Virtual Reality}
Virtual Reality, kurz VR, bezeichnet eine virtuelle Welt, die dem Nutzer präsentiert wird. In dieser kann er interagieren und die Gegenstände um sich herum erforschen. Die virtuelle Welt ist dabei komplett unabhängig von der physischen Welt. VR versucht ein möglichst großes Gefühl der Präsenz in der gezeigten Welt zu erzeugen. \cite{Stone2017}
Augmented Reality (AR) dagegen integriert virtuelle Objekte in den realen Raum. Dazu wird in ein Kamerabild an eine bestimmte Stelle ein Objekt gezeichnet, das dort physisch nicht existiert, oder eine halbtransparente Brille verwendet, die virtuelle Inhalte über das Sichtfeld legt. AR wird meist für 3D-Modelle verwendet, während VR auch 360°-Fotos oder -Videos anzeigen kann.

\subsection{Aufbau von VR-Headsets}
Ein VR-System ist ein Gerät, dass VR-Inhalte anzeigen kann und die Interaktion damit erlaub. Das können zum Einen so genannte Head-mounted Displays (HMD), auch bekannt als VR-Brillen oder VR-Headsets, sein oder eine CAVE (Cave Automatic Virtual Environment). Eine CAVE ist ein Raum, bei dem alle Seiten Displays sind, die virtuelle Inhalte anzeigen.
Da eine CAVE ein gesamter Raum ist, der pro Nutzer benötigt wird, ist sie für VR-Anwendungen im Unterricht nicht geeignet. Daher werden in dieser Arbeit ausschließlich VR-Headsets behandelt.

Ein VR-Headset besteht aus einer Art Brille, in der sich ein Display und Linsen befinden und mit Riemen am Kopf befestigt wird. Die Linsen dienen dazu das auf dem Display gezeigte Bild auf ein weiteres Blickfeld zu bringen und einen angenehmen Fokuspunkt für die Augen zu schaffen, der weiter entfernt liegt als das eigentliche Display. Zusätzlich sind meist noch verschiedene Sensoren verbaut, mit denen die Kopf-Bewegungen und Position des Nutzers registriert werden.
Bei einem VR-Headset sieht der Nutzer nur die ihm gezeigten Inhalte und nicht seine tatsächliche Umgebung. Darin unterscheiden sie sich von den Augmented Reality Headsets.

Die meisten VR-Headsets haben zusätzlich einen Controller, mit dem der Nutzer mit der VR-Welt interagieren kann.

\subsection{Forschung zu Virtual Reality in der Bildung}
In der Vergangenheit wurden bereits einige Forschungsarbeiten zu AR oder VR im Bildungsbereich veröffentlicht.

Juan et al entwickelten ein einfaches AR-System, mit dem Schülerinnen und Schüler die verschiedenen Organe im Körper lernen sollten. Dazu wurden physische Reißverschlüsse auf Bretter angebracht, die im geöffneten Zustand im AR-System die Organe an dieser Stelle des Körpers anzeigen. Mit einer Studie fanden sie heraus, dass die Kinder die Anwendungen gleich gerne mochte, wenn sie ein HMD trugen oder die Anwendung auf einem Bildschirm sahen. \cite{Juan2008}

In einer anderen Arbeit beschäftigten sich die Forscher mit einem Lernsystem für Studenten des Bauingenieurwesen. Dabei konnten diese sich innerhalb der VR-Anwendung in einem Haus bewegen, das in jedem Raum einen anderen Lerninhalt präsentierte. 
Die Testpersonen konnten allerdings nur knapp 30 Minuten lang das Headset tragen, wodurch die Zeit zum Lernen nach der Eingewöhnung in das System sehr kurz war. Tests zeigten, dass die Testpersonen trotz der kurzen Zeit einige Lehrinhalte behalten haben. \cite{Chou1997}

In ihrer Arbeit von 2006 lernten Kerawalla et al, dass bei der Nutzung von AR-System in der Bildung der Inhalt soweit flexibel sein muss, dass die Lehrkraft ihn an die Klassen anpassen kann. Außerdem stellten sie fest, dass es wichtig ist, die Schüler die gezeigten Inhalte erforschen zu lassen, um das volle Potenzial der Technologie auszunutzen. \cite{Kerawalla2006}

Bun et al entwickelten ein System, mit dem anatomische Inhalte erlernt werden sollten. Sie verglichen dabei verschiedene VR-Headsets und stellten fest, das besonders positional Tracking das Gefühl der Immersion verstärkt.
Auch hier stellten Autoren fest, dass VR einen positiven Einfluss auf das Lernen hat. \cite{Bun2015}

In ``Games, motivation, and learning: A research and practice model. Simulation \& gaming'' und ``A contribution to the understanding of what makes young students genuinely engaged in computer-based learning tasks'' zeigte sich den Wissenschaftlern, dass Lernen in VR sich besonders positiv auf die Motivation der Nutzer auswirkte. \cite{garris2002games} \cite{ott2009contribution}

Außerdem fanden Leite, Svinicki und Shi heraus, dass VR verbesserte Lernergebnisse für Nutzer produziert, die visuell und auditiv Lernen. \cite{leite2010attempted}

Virvou und Katsionis haben in ihrer Arbeit ein VR-Geografie-Lernspiel entwickelt und anschließen eine Studie durchgeführt, um zu evaluieren, ob sowohl neue Nutzer als auch erfahrene VR-Spieler Spaß am Lernen in VR haben. Sie fanden heraus, dass alle Schüler Spaß daran hatten und das Spiel auch zum Lernen hilfreich fanden. Die Schüler waren durch das System zudem sehr viel motivierter die Inhalte zu erlernen. \cite{Virvou1995}

\subsection{Aufbau der Arbeit}
In dieser Arbeit werden zu Beginn verschiedene auf dem Markt befindliche Virtual-Reality-Headsets anhand verschiedener Merkmale verglichen. Dabei werden sowohl günstige Einsteiger-Geräte, die sich auch für Schulen eignen könnten, als auch High-End-Produkte besprochen.

Anschließend wird auf bereits bestehende VR-Software-Systeme im Bildungsbereich eingegangen. Die dort aufgeführten Anwendungen sind teilweise für die Nutzung im Unterricht, aber auch für die individuelle Bildung ausgelegt. Insgesamt gibt es noch wenige Anwendungen, die speziell für die Nutzung im Klassenzimmer entworfen wurden.

Im folgenden Kapitel werden die Herausforderungen behandelt, die Virtual-Reality-Systeme aktuell noch haben. Außerdem werden Altersbeschränkungen zur Nutzung von VR-Systemen besprochen, da dies für die Nutzung im Schulunterricht eine Rolle spielen kann. Interessanterweise sehen Forscher Virtual Reality als eine vielseitige Möglichkeit besonders in der Forschung und Therapie von Kindern, während die Hersteller von kommerziellen VR-Headsets Kinder von der Nutzung ausschließen.

Das fünfte Kapitel beschäftigt sich mit den verschiedenen Inhaltstypen, die in VR-Systemen genutzt werden können. Dabei wird vor allem auf die Speicherung und Bereitstellung von 360°-Foto und -Videos, 3D-Modelle, 3D Audio und VR-Anwendungen eingegangen. Da im Rahmen des Softwareprojekt React360 genutzt wird, bespricht das Kapitel welche Medien in React360 genutzt werden können.

Anschließend werden die verschiedenen Möglichkeiten der Softwareentwicklung für VR-Anwendungen besprochen und verglichen. Es werden native und webbasierte VR-Applikationen unterschieden. Zudem werden die technischen Entscheidungen für das Softwareprojekt erläutert.

In Kapitel 6 wird auf die im Rahmen dieser Arbeit entwickelte Software ``VRClassroom'' eingegangen. Es wird das Anwendungsszenario und die zukünftige Nutzergruppe definiert und daraus Anforderungen abgeleitet. Daraufhin wird auf die Struktur des Systems, die einzelnen Funktionen und deren Implementierung eingegangen.

Anschließend an die Entwicklung wurde VRClassroom mittels einer Umfrage unter Lehrkräften evaluiert. Die Befragung beinhaltet sowohl die Bewertung von VRClassroom und seinen Funktionen, als auch die Einschätzung der Teilnehmer zum generellen Einsatz von Virtual Reality im Unterricht und die Bereitschaft der Lehrkräfte VR-Programme einzusetzen.
Die Mehrheit der befragten Lehrkräfte zeigte sich interessiert daran VR-Anwendungen im Unterricht zu nutzen und können sich auch vorstellen VRClassroom einzusetzen.

Abschließend wird ein Blick in die Zukunft von Virtual Reality im Allgemeinden und von VRClassroom im Speziellen geworfen. Dabei wird sowohl auf aktuell bestehende Probleme, als auf potenzielle Weiterentwicklungen der Software eingegangen.

%______________________________________________________________________

% Der Befehl \cleardoublepage erscheint nur vor \section, nicht vor
% den "kleineren" Gliederungsbefehlen wie \subsection!
\cleardoublepage % Neue rechte Seite anfangen
\section{Existierende VR-Hardware-Systeme}
Es gibt inzwischen eine große Zahl verschiedener VR-Hardware-Systeme, die sich in drei Gruppen mit unterschiedlichen Anwendungsszenarien unterteilen: Die Computer-gestützten VR-Systeme, die Stand-alone VR-Systeme und die Smartphone-gestützten VR-Systeme.

\subsection{Merkmale von VR-Systemen}
Um die verschiedenen VR-Hardware-Systeme einordnen zu können, gibt es einige technische Merkmale, die Headsets in unterschiedliche Kategorien unterteilen. Diese verschiedenen Kategorien von Headsets dienen unterschiedlichen Anwendungsszenarien und sollten entsprechend der Nutzung ausgewählt werden.

Folgende Merkmale können zur Differenzierung herangezogen werden: Freiheitsgrade (Degrees of Freedom, bzw. DoF), die Displaygröße und -auflösung, die Bildwiederholrate des Displays (Frequenz), das Sichtfeld (Field of View), die Rechenleistung und das Gewicht des Headsets, die verwendete Tracking-Methode, die mitgelieferten Controller beziehungsweise mögliche Eingabemethoden und der Verkaufspreis.

\subsubsection{Freiheitsgrade (Degrees of Freedom)}
Das Konzept der Freiheitsgrade oder Degrees of Freedom, kurz DoF, entspringt ursprünglich der Mechanik. Wie Gans in seiner Arbeit ``Engineering dynamics: From the lagrangian to simulation'' erläutert, hat ein Körper grundlegend sechs Freiheitsgrade. Die Bewegungen im Raum entlang der x-, y- und z-Achse. Hinzu kommen die Rotationen um die jeweiligen Achsen. Kurz beschreibt er Freiheitsgrade als die minimale Anzahl an Variablen um ein System zu spezifizieren. \cite{Gans2013}

Freiheitsgrade werden oft auch genutzt um die Bewegungen von maritimen Fahrzeugen zu benennen: Die Bewegung in x-Richtung wird ``surge'' genannt, in y-Richtung ``sway'' und in z-Richtung ``heave''. Die Rotation um die x-Achse heißt ``roll'', um die y-Achse ``pitch'' und um die z-Achse ``yaw''. Diese Bezeichnungen beziehen sich dabei auf ein Schiff, das in x-Richtung ausgerichtet ist. \cite{Fossen1995}

Wie in Abbildung \ref{fig:dof} zu erkennen, sind die Degrees of Freedom für VR-Headsets aus diesen Definitionen abgeleitet. Unterstützt ein System nur die Erkennung von Rotationsbewegungen um die drei Achsen wird es als ein 3 DoF-System bezeichnet. Kann es zusätzlich die Bewegungen im Raum messen ist es ein 6 DoF-System. Um ein 6 DoF VR-Gerät zu entwickeln, wird also ``positional tracking'' des Nutzers benötigt. 

\begin{figure}[ht]
  \centering
  \includegraphics[width=\textwidth]{images/dof.eps}
 \caption{Degrees of Freedom eine Virtual Reality Headsets.}
  \label{fig:dof}
\end{figure}

\subsubsection{Display, Frequenz und Field of View}
Bei Displays von VR-Headsets gibt es zwei grundsätzliche Ansätze: Ein durchgängiges Display, das das Bild für beide Augen darstellt oder zwei kleinere unabhängige Displays. 
Auf ein einzelnes großes Display setzt eigentlich nur Sony mit ihrer Playstation VR, alle anderen Hersteller von VR-Headsets, die fest integrierte Displays haben, arbeiten mit getrennten Displays für die beiden Augen.

Bei Smartphone-gestützten VR-Headsets hängt die Größe des Displays, sowie die Auflösung und damit auch das Sichtfeld (``Field of View'') vom eingelegten Smartphone ab. Da in aktuellen Smartphones sehr hochaufgelöste Displays eingebaut werden, stehen diese den festintegrierten Displays nicht oder nur minimal nach. Nutzt man beispielsweise ein Galaxy S9+ hat man ein Display mit 2960x1440 Pixel Auflösung und 529ppi (pixel per inch), was einer Auflösung von 1480x1440 pro Auge entspricht. \cite{SAMSUNGELECTRONICSCO} Es ist damit höher aufgelöst als alle fest eingebauten Displays (abgesehen von der Oculus Quest, die erst im Laufe des Jahres auf den Markt kommen soll).

In den Tabellen \ref{tab:headsets1} und \ref{tab:headsets2} sind die Angaben zur Auflösung der Displays als Auflösung pro Auge zu verstehen.

Neben der Auflösung ist auch der Abstand der einzelnen Pixel zueinander entscheidend für die Qualität der visuellen Darstellung. Der Screen Door Effect und der Pixel Fill Factor beziehen sich auf den nicht leuchtenden Abstand zwischen zweier Pixel. Der Screen Door Effect bezeichnet den Zustand, wenn diese Abstände deutlich als Gitter über dem Inhalt erkennbar werden. Der Pixel Fill Factor beschreibt die Dichte der Pixel zueinander. Ist der Pixel Fill Factor hoch, wird der Screen Door Effect schwächer oder verschwindet sogar komplett.
Da bei VR-Headsets das Display extrem nah am Auge ist, wurden bei den ersten Geräten von Oculus und HTC starke Screen Door Effekte wahrgenommen. Aber mit den Versionen, die dann letztendlich in den Handel kamen, sind diese Probleme großteils ausgeräumt worden.
 \cite{Desai2014}

Die Arbeit von Potter et al. schließt aus ihren Nachforschungen, dass das menschliche visuelle System schon ab 13ms komplette verschiedene Bilder wahrnehmen und damit unterscheiden kann. Das entspräche einer benötigten Bildwiederholrate (Framerate) von knapp 80Hz.  \cite{Potter2013}
Andere Forschungen zeigen allerdings auch, dass kleinere Veränderungen mit bis zu 500Hz noch deutlich schneller wahrgenommen werden können. 
Die aktuellen VR-Headsets sind mit Bildwiederholraten zwischen 60Hz und 120Hz noch deutlich davon entfernt. Allerdings ist zu vermuten, dass keine 500Hz notwendig werden, da die Studie mit hochfrequent flackernden Lichtern durchgeführt wurde, was in den meisten Situationen nicht der Realität entspricht.
 \cite{davis2015humans}

Wie Lin et al. in ``Effects of field of view on presence, enjoyment, memory, and simulator sickness in a virtual environment'' ausführen haben Menschen ein aus beiden Augen zusammengesetztes Sichtfeld von 180°. Ein weiteres Sichtfeld als 180° wäre also nicht sinnvoll, da es nicht mehr wahrnehmbar wäre.  \cite{Lin2002}
Aktuelle Headsets sind mit einem Sichtfeld von 100°-110° allerdings noch etwas von der Abbildung des vollen Sichtfelds entfernt. 
Wie aber Lin et al. ebenso in ihrer Studie bemerkten, hat ein weiteres Field of View als 140° kaum noch positivere Effekte im Bezug auf Immersionsgefühl und Spaß (Enjoyment). Zusätzlich fanden sie heraus, dass das Field of View  mit dem Auftreten von Cybersickness korreliert. Je weiter also das Field of View ist, desto mehr litten die Studienteilnehmer an den Symptomen. \cite{Lin2002}

\subsubsection{Rechenleistung}
Generell lässt sich sagen, das mit höherer Rechenleistung aufwändigere VR-Szenen in höherer Qualität dargestellt werden können. Bessere Display-Auflösung, Bildwiederholungsrate und Field of View benötigen eine höhere Rechenleistung um die Bildinhalte zu generieren. Deshalb können (zumindest momentan) stand-alone Geräte und Smartphone-gestützte Geräte nicht mit den Computer-gestützten Geräten in diesen Kategorien konkurrieren.
Da die Rechenleistung bei Computer- und Smartphone-gestützten VR-Headsets allerdings vom benutzen Rechner beziehungsweise Smartphone abhängt, ist die Rechenleistung nur bei den stand-alone Geräten ein Merkmal, das berücksichtigt werden kann.

Fast alle in dieser Arbeit untersuchten Geräte haben dabei den gleichen Prozessor verbaut. Während alle anderen Geräte einen Qualcomm Snapdragon 835 nutzen, hat lediglich die Oculus Go hat einen Qualcomm Snapdragon 821 verbaut. Damit sind die anderen Geräte nominell etwa 30\% schneller und verbrauchen dabei ungefähr 40\% weniger Energie als die Oculus Go. \cite{HTCCorporationViveFocus} \cite{Bastian2018} \cite{Bastian2018a}

\subsubsection{Eingabemethoden}
Es zwei grundsätzliche Eingabemethoden für Virtual Reality, die momentan verwendet werden: Eingaben über einen oder mehrere externe Controller oder aber die Eingabe am Headset.

Eine einfache Methode für die Eingabe am Headset selbst ist bei Smartphone-gestützten Systemen das Auslösen von Touch-Eingaben auf dem Smartphone-Display. Ein Beispiel dafür sind Google Cardboards, die einen Auslöser am Gehäuse haben, der eine Art ``Arm'' auf das Display drückt. Einige Nachbauten des ursprünglichen Google Cardboards verwenden Magnete um Touch-Eingaben auf kapazitiven Touchscreens auszulösen.

Egal mit welcher der beiden Techniken das Touch-Event ausgelöst wird, erfolgt das Touch-Event immer an der gleichen Stelle auf dem Display. Der Nutzer kann also keine bestimmte Stelle in der Szene auswählen. Deshalb wird bei dieser Eingabemethode meist der Mittelpunkt der aktuellen Blickrichtung als Position oder Richtung des ausgelösten Events interpretiert.

Ganz ohne Eingabe am Touchscreen können Gaze-Events (engl. ``starren'') zur Eingabe verwendet werden. Reagiert ein Bereich in einer Szene auf Gaze-Events, so löst das Ereignis aus, wenn der Bereich für eine bestimmte Zeit ``angestarrt'' wird. Üblicherweise wird der Mittelpunkt des Blickfelds als Zeiger für Gaze-Event verwendet.

Externe 3 DoF-Controller bieten meist mehrere Buttons zur Eingabe und stellen einen Zeiger da, mit dem Bereiche in der Szene ausgewählt werden können. Üblicherweise ist der Ursprung des Zeigers an einem Ort fixiert. Über Beschleunigungssensoren wird die Richtung des Zeigers bestimmt, in der ein Strahl ausgesendet wird (``Raycaster'').

Bei 6 DoF-Controllern wird deren Position im Raum durch Positional-Tracking-Systeme bestimmt. Anders als bei 3 DoF-Controllern ist damit die Position des Controllers nicht fixiert, sondern kann im Raum verändert werden.

Bisher setzen keine Systeme auf Spracheingabe. Eine Entwicklung in diese Richtung ist aber abzusehen, da die Eingabe sehr natürlich ist und dadurch die Zugänglichkeit zu VR-Systemen erhöht werden kann.

In ihrer Arbeit haben Geiselhart et al. ``EyeVR'' entwickelt. Dazu haben sie eine Kamera in ein bestehendes VR-Headset eingebaut, die die Bewegungen der Augen registriert. Sie haben dazu eine Software entwickelt, die sowohl die Pupillenbewegungen als auch die Bewegungen des Lids registriert und als Input verarbeiten kann. 
 \cite{Geiselhart2016}
 
 Außerdem gibt es Headsets, die einfache Handgesten aus dem Kamerabild als Eingabemethode nutzen. Ein Beispiel dafür sind die Headsets von ClassVR. \cite{AvantisSystemsLtd}

\subsubsection{Positional Tracking}
Um sechs Freiheitsgrade für VR-Headsets zu bestimmen, muss die Position im Raum oder die Veränderung der Position relativ zum Raum berechnet werden. Positional Tracking wird also generell nur von 6 DoF-Headsets genutzt.

Momentan gibt es dazu zwei verschiedene Ansätze: Die eine Möglichkeit ist es Tracker im Raum aufzustellen, die dann die Position des Headsets und gegebenenfalls der Controller im Raum bestimmen. Die Computer-gestützten VR-Headsets nutzen alle diese Technik, sie wird outside-in-Tracking genannt.  \cite{Stone2017}

Die zweite Möglichkeit ist das inside-out-Tracking. Für das inside-out-Tracking müssen keinerlei Sensoren im Raum platziert werden. Es werden nur Sensoren verwendet, die in das Headset integriert sind.  \cite{Stone2017}
Dafür nutzt die Lenovo Mirage Solo beispielsweise das WorldSense-System von Google. Es verwendet in das Headset integrierten Kameras zur Bestimmung der Position nutzt. \cite{GoogleWordlSense}

Die Oculus Quest dagegen soll mit dem von Facebook selbst entwickelten ``Insight''-Tracking kommen. Dafür sind vier Kameras an den vorderen vier Ecken des Displays angebracht. Diese bestimmen die Position im Raum anhand von Objekten im Raum, die von den Kameras erkannt werden. Das Insight-Tracking soll zusätzlich auch die Position der beiden Controller bestimmen, sodass auch diese sechs Degrees of Freedom unterstützen. \cite{Lang2018}

\subsubsection{Gewicht}
Die Spanne zwischen den Gewichten der einzelnen Virtual Reality-Headsets reicht von 645g im Maximum zu nur knapp 100g im Minimum. Die Entwicklung der Geräte geht zu leichteren Modellen. Allerdings ist bei den neu vorgestellten Modellen, die ohne Kabel funktionieren wiederum das Gewicht des Akkus ein neues Problem. Die Hersteller versuchen das doch noch recht hohe Gewicht durch gute Riemensysteme auszugleichen. Dennoch kann das Gewicht bei längerer Nutzung unangenehm sein.

Die Playstation VR hat ein kleines Gegengewicht im Bügel, sodass sich das Gewicht etwas besser verteilt.

Das leichteste aller VR-Headsets ist das Google Cardboard mit einem eingelegten Smartphone, es hat dann ein Gesamtgewicht von etwa 270g, wenn man die 96g des Cardboards mit einem durchschnittlichen aktuellen Smartphone addiert. Allerdings bietet das Gerät auch keine Polsterungen oder Riemen.
Bei allen Geräten, die in Tabelle \ref{tab:headsets3} aufgeführt sind, bezieht sich das Gewicht immer auf das Headset ohne Smartphone.

\subsubsection{Preis}
\label{subsec:price}
Wie in \ref{sec:challenges} erläutert sieht die Autorin des Artikels ``5 Major Challenges For The VR Industry'' den Preis als eines der Hautprobleme, dass sich VR noch nicht weiter durchgesetzt hat.
Da aber besonders die Smartphone-gestützten VR-Systeme sehr günstig zu bekommen sind, gilt dieses Argument eigentlich nicht wirklich.

Trotzdem wird erwartet, dass die Preise für VR-System weiter sinken werden, sodass auch hochwertige Lösungen erschwinglicher für alle Nutzergruppen werden. \cite{Hamilton2018}

Die Preise in den Tabellen \ref{tab:headsets1}, \ref{tab:headsets2} und \ref{tab:headsets3} sind in US-Dollar, wie sie auf dem amerikanischen Markt zu kaufen sind. Darin ist keine Umsatzsteuern enthalten.

Bei PlaystationVR handelt es sich um ein Bundle-Preis, bei dem 2 Spiele enthalten sind.

\subsection{Computer-gestützte VR-Hardware}
Die erste Gruppe sind die Computer-gestützten Hardware-Systeme: Sie sind mit einem Kabel mit dem Rechner verbunden, der die Rechenleistung für die eigentlich Brille übernimmt. Sie eignen sich besonders für rechenaufwändige Anwendungen, aber schränken durch ihre Kabel die Bewegungsfreiheit des Nutzers ein.

\begin{table}[]
\begin{tabular}{p{0.14\linewidth}|p{0.07\linewidth}|p{0.14\linewidth}|p{0.18\linewidth}|p{0.18\linewidth}|p{0.08\linewidth}|p{0.06\linewidth}}
\textbf{VR-Gerät} & \textbf{DoF} & \textbf{Display} & \textbf{Input} & \textbf{Tracking} & \textbf{Gewicht} & \textbf{Preis} \\ \hline
Oculus Rift & 6 DoF & 1080x1200 \newline 90Hz \newline 100° & Oculus Touch, Xbox One Controller & 2 optische Sensoren im Raum & 380-470g &  349\$ \\
Playstation VR & 6 DoF & 960x1080 \newline120Hz \newline 100° & PlayStation Move Controller
 & Playstation Camera & 610g & 349 \$ \\
HTC Vive & 6 DoF & 1080x1200  \newline 90Hz \newline 110° & SteamVR-Tracking Controller & 2 Laser-Sensoren im Raum & 555g & 499\$
\end{tabular}
  \caption{Vergleich der Computer-gestützten VR-Headsets.}
  ~\label{tab:headsets1}
\end{table}

\subsubsection{Oculus Rift}
Die Oculus Rift ist die erste von Oculus herausgebrachte VR-Brille. Sie wird durch einen Windows-Computer bespielt und wird durch zwei Kameras, die auf dem Schreibtisch um den Bildschirm des Computer platziert werden können, optisch getrackt. 

Mit der Oculus Rift kommen zwei Oculus Touch Controller, diese haben jeweils zwei Action-Buttons, einen Menü-Button und einen Thumbstick sowie einen rückseitigen Button, der für den Zeigefinger gedacht ist. Die Buttons können erkennen, ob ein Finger aufliegt, während der Button aber noch nicht gedrückt wurde. Dies erlaubt neben dem Klicken der Buttons auch die Position der Finger zu bestimmen, und so zu erkennen, ob der Nutzer beispielsweise ein Faust bildet. \cite{Davies2016}

Der Augenabstand (Inter-pupillary distance; IPD) ist verstellbar zwischen 58 und 72mm, der Linsendurchmesser liegt bei circa 50mm. 

Die Oculus Rift ist mit bis zu 470g deutlich leichter als die beiden anderen Kabel-gebundenen VR-Brillen. Auch die Controller sind mit 160g merkbar leichter als die der HTC Vive.

Für ein vollständiges Setup wird ein Windows-Rechner (mit Bildschirm, Mouse und Keyboard), die Oculus Rift Brille, die Oculus Touch Controller und die zwei Sensoren benötigt. \cite{VRBound} \cite{FacebookTechnologiesLLCRift}

\subsubsection{HTC Vive}
Die HTC Vive hat ein sehr ähnliches Display verbaut wie die Oculus Rift. Sie haben die gleiche Auflösung und Bildwiederholungsrate von 90Hz, allein das Field of View ist mit 110° etwas weiter als bei anderen VR-Headsets. Im Vive-Headset lässt sich die Pupillendistanz und der Objektivabstand (Focal length) einstellen.

Für ein vollständiges Setup wird ein Windows-Rechner (mit Bildschirm, Mouse und Keyboard), die HTC Vive Brille, die HTC Vive Controller und die zwei Laser-Sensoren, genannt ``Lighthouses'' benötigt. Die Lighthouses müssen an diagonalen Enden des Raums überhalb des Aktionsbereich aufgestellt werden, was die initiale Installation etwas aufwändiger macht. Durch die Lasertechnologie in den Lighthouses ist das mögliche Spielfeld der HTC Vive deutlich größer als das der anderen beiden VR-Systeme.

Um das Tracking einzustellen muss zu Beginn der ersten Nutzung das System kalibriert und der Spielbereich virtuell markiert werden.
Die Controller haben ein klickbares Trackpad, Menü- und System-Buttons auf der Vorderseite, einen Abzugsknopf für den Zeigefinger auf der Rückseite und Greifknöpfe an den Seiten des Controllers.
Die HTC Vive hat zusätzlich zu den Sensoren auch noch eine Kamera eingebaut, um gegebenenfalls im Weg stehende Gegenstände zu erkennen, um Unfälle zu verhindern.  \cite{Davies2016}

Preislich liegt die Vive mit 499\$ am oberen Ende des Spektrums der VR-Headsets. \cite{HTCCorporationVive}

\subsubsection{Playstation VR}
Die Playstation VR ist dafür entwickelt mit der Playstation 4 Konsole benutzt zu werden, sie wurde also spezielle für den Anwendungsfall des Gamings entwickelt. Sie unterstützt sogar das Spielen mit zwei Spielern, wobei ein Spieler die Playstation VR nutzt und der zweite Spieler das Bild auf dem Fernseher sieht. Beide nutzen die Playstation Move Controller.

Die Controller haben die üblichen Buttons eines Playstation-Controllers und ihre Position im Raum wird mit der Playstation Camera getrackt.

Wie in Tabelle \ref{tab:headsets1} zu sehen ist die Auflösung der Playstation VR deutlich geringer als die der anderen beiden Headsets. Dafür hat sie eine extrem hohe Bildwiederholrate von 120Hz, was gerade für schnelle Spiele einen Vorteil bietet. Allerdings ist sie mit 610g recht schwer, sodass ein langes Tragen nicht sehr angenehm ist.

Zum Tracking der Position im Raum nutzt die Playstation VR die bereits existierende Playstation Camera. Darin sind 2 Kameras verbaut, die beide Tiefenwahrnehmung haben. Sie tracken optisch sowohl die Position des Headsets als auch die der Playstation Move Controller.  \cite{Davies2016}

Für ein vollständiges Setup wird ein Playstation 4 Konsole, die Playstation VR Brille, die Playstation Move Controller und die Playstation Camera benötigt. \cite{VRBoundPlaystationVR} \cite{SonyInteractiveEntertainmentEuropeLimitedpsvr}

\subsection{Stand-alone VR-Hardware}
Die zweite Gruppe sind die Stand-alone Hardware-Systeme wie etwa die Oculus Go, Oculus Quest oder Google Daydream. Es handelt sich hierbei um vollumfängliche Systeme, die ohne weiteres Equipment auskommen und auch keinen Computer benötigen. Da sie mit einem Akku betrieben werden, ist die Betriebsdauer allerdings eingeschränkt und auch die Rechenleistung ist deutlich geringer als die der Computer-gestützten Systeme.

\begin{table}[]
\begin{tabular}{p{0.14\linewidth}|p{0.08\linewidth}|p{0.14\linewidth}|p{0.15\linewidth}|p{0.15\linewidth}|p{0.1\linewidth}|p{0.08\linewidth}}
\textbf{VR-Gerät} & \textbf{DoF} & \textbf{Display} & \textbf{Input} & \textbf{Tracking} & \textbf{Gewicht} & \textbf{Preis} \\ \hline
Oculus Go & 3 DoF & 1280x1440  \newline 60-72Hz \newline 100° & Oculus Go Controller & kein Tracking & 468g & 199\$ \\
Oculus Quest & 6 DoF & 1600x1440 \newline OLED \newline 72Hz \newline 100° & Oculus Touch Controller & Inside-out & 570g & 399\$ \\
Lenovo Mirage Solo & 6 DoF &1280x1440 \newline OLED \newline 75Hz \newline 110° & Daydream Motion Controller & Inside-out  & 645g & 349,99\$ \\
HTC Vive Focus & 6 DoF & 1440x1600 \newline 75Hz \newline 110° & Ultrasonic 6 DoF Controller & Inside-out & 670g & 599\$
\end{tabular}
  \caption{ \label{tab:headsets2}Vergleich der Stand-alone VR-Headsets. \cite{Tissler2018} }
\end{table}

\subsubsection{Oculus Go}
Mit ihrem Preis von knapp 200\$ ist die Oculus Go mit Abstand das günstigste VR-Headset. Sie kommt mit einem kleinen 3 DoF Controller, der ein klickbares Touchfeld, einen Button für den Zeigefinger, sowie einen kleinen ``Zurück''- und ``Oculus''-Button.

Die Oculus Go hat pro Auge ein Display von 1280x1440 Pixeln mit einer Refreshrate von 60 bis 72Hz und einem Field of View von 100°.

Sie erkennt durch einen kleinen Sensor am inneren der Brille, ob sie aufgesetzt ist oder nicht und schaltet sich nach längerer Zeit, in der sie abgesetzt ist, selbstständig ab. 
Sie bietet die Möglichkeit für Nutzer mit Sehschwächen an die Sehstärke angepasste Linsen auf die festeingebauten Linsen zu setzen, sodass die eigene Brille nicht benötigt wird. Diese müssen aber separat erworben werden.
Der Linsenabstand kann an der Oculus Go nicht individuell eingestellt werden und ist auf 64mm festgelegt.

Die Oculus Go kommt mit eingebauten Lautsprecher, die so positioniert sind, dass man trotzdem noch Umgebungsgeräusche hören kann und nicht abgeschottet wird.

Eine Akkuladung hält je nach Nutzung zwischen 2 und 2,5 Stunden, wobei die Ladezeit mit fast 3 Stunden relativ lange ist.

Da es sich bei der Oculus Go um ein 3 DoF-Headset handelt, unterstützt sie kein Positional Tracking, sondern nur orientational Tracking über die eingebauten Sensoren.  \cite{Tissler2018} \cite{VRBoundOculusGo}

\subsubsection{Oculus Quest}
Die Oculus Quest ist noch nicht auf dem Markt, sondern für Frühling 2019 angekündigt. Einige Informationen zu dem Headset sind jedoch schon bekannt.

Die Oculus Quest ist ein 6 DoF-VR-Headset. Die Position im Raum wird durch vier an den äußeren Ecken des Headsets eingebauten Kameras bestimmt. Sie übernehmen auch das Tracking der Oculus-Touch-Controller, die damit ebenfalls sechs Freiheitsgrade erhalten.
Sie hat auch die Möglichkeit, den Linsenabstand individuell einzustellen, sodass für jeden Nutzer das Virtual Reality Erlebnis perfekt ist und das Risiko von Cybersickness durch Überanstrengung der Augen sinkt.

Die Oculus Quest soll mit einem OLED-Display mit einer Auflösung von 1600x1440 pro Auge und einer Refreshrate von 72Hz ausgestattet sein und ist damit das VR-Headsets mit dem am besten aufgelösten Display. Nur bei der Refreshrate haben die Computer-gestützten Headsets hier noch etwas voraus. Die Quest kommt, genau wieder die Go, mit eingebauten Lautsprechern, die es ermöglichen sowohl den Sound der VR-Welt zu hören, als auch noch Geräusche aus dem Raum mitzubekommen.

Die mitgelieferten Oculus-Touch-Controller sind denen der Oculus Rift sehr ähnlich. Jedoch befindet sich der Ring mit dem die Controller getrackt werden nun oberhalb, damit er von den Kameras im Headset gesehen werden kann. Sie können alle Bewegungen im Raum erkennen und auch, ob die Buttons mit den Fingern berührt werden.

Das Gesamtgewicht der Oculus Quest ist noch nicht bekannt, soll aber etwa 100g schwerer als das Rift-Headset sein und läge damit dann bei 570g. Auch die Akkulaufzeit der Oculus Quest wurde noch nicht herausgegeben. \cite{Feltham2018}

\subsubsection{Lenovo Mirage Solo}
Die Lenovo Mirage Solo liegt bei einem Preis von 350\$. Sie ist ebenfalls ein 6 DoF-Headsets, der mitgelieferte Daydream Motion Controller kann allerdings nur drei Freiheitsgrade. Auch die Mirage Solo nutzt inside-out-Tracking für die Positionsbestimmung im Raum. Dafür sind in das Display vorne zwei Kameras integriert. Berichten zufolge ist der nutzbare Spielbereich, der mit dem von Google entwickelten WorldSense getrackt wird, nur etwa einen Quadratmeter groß, sodass nur wenig Bewegung möglich ist.

Die Controller sind die gleichen, die Google für das Daydream-View-Headset verwendet. Zudem sind keine Lautsprecher eingebaut. Mit 645g ist die Mirage Solo das schwerste VR-Headset, das momentan auf dem Markt ist. \cite{Lenovo}

Lenovo gibt die Akkulaufzeit mir 2,5 Stunden an, die bei Tests wohl sogar auf 3 Stunden ausgedehnt werden konnte.
Auf der Mirage Solo läuft Google Daydream, über das auch Apps geladen werden können. Im Vergleich zum Oculus Store sind dort Adi Robertson zufolge noch deutlich weniger und auch weniger elaborierte Titel zu finden. \cite{Robertson2018}

\subsubsection{HTC Vive Focus}
Bisher ist die HTC Vive Focus nur China auf dem Markt, allerdings wurde Ende 2018 angekündigt, dass sie demnächst auch in Europa und den USA verkauft werden wird. Mit ihrem Preis von 599\$ ist sie die mit deutlichem Abstand teuerste Stand-alone VR-Brille.
Wie die Oculus Quest hat sie eine Auflösung von 1440x1600 pro Auge und eine Bildfrequenz von 75Hz und einem Field of View von 110°. Sie ist sich also insgesamt sehr ähnlich zur Oculus Quest.

Die Controller der Vive Focus haben nur 3 DoF. Sie haben ein Trackpad, einen Trigger für den Zeigefinger und zwei Buttons auf der Oberseite. Sie erinnert formmäßig an eine Fernbedienung.

Wie die Mirage Solo nutzt die Vive Focus zwei ins Headset verbaute Kameras für ein Inside-Out Tracking mit einem Spielbereich von 1,5mx1,5m. 
Der Linsenabstand kann mechanisch eingestellt werden, bei größeren Brillen ist der Platz in der VR-Brille allerdings etwas knapp.

Sie hat eine Laufzeit von 3 Stunden. Die Anzahl der verfügbaren Anwendungen ist noch gering im Vergleich zu den Anwendungen der Oculus Geräte. \cite{HTCCorporationViveFocus} \cite{Langley2018}

\subsection{Smartphone gestützte VR-Hardware}
Die dritte Gruppe bilden die Smartphone-gestützten VR-Systeme. Damit sind alle Systeme gemeint, bei denen das Smartphone genutzt wird, um die VR-Inhalte zu zeigen.
Wie in Tabelle \ref{tab:headsets3} zu sehen sind damit die Merkmale für das Display, die Rechenleistung und das Tracking nicht beinhaltet, da diese abhängig vom eingelegten Smartphone sind. Diese liegen preislich auf einem sehr niedrigen bis mittlerem Niveau und bilden so eine gute Einstiegsmöglichkeit.

\begin{table}[]
\begin{tabular}{p{0.19\textwidth}|p{0.07\textwidth}|p{0.15\textwidth}|p{0.26\textwidth}|p{0.08\textwidth}|p{0.06\textwidth}}
\textbf{VR-Gerät} & \textbf{DoF} & \textbf{Field of View} & \textbf{Input} & \textbf{Gewicht} & \textbf{Preis} \\ \hline
Google Daydream View & 3 DoF & 100° &  Daydream controller & 261g & 99\$ \\
Samsung Gear VR & 3 DoF & 101° & Gear VR Controller & 345g & 130\$ \\
Google Cardboard & 3 DoF &  90° & Hebel, der Touch-Event auf Display auslöst & 96g & 15\$
\end{tabular}
  \caption{Vergleich der Smartphone-gestützten VR-Headsets.}
  ~\label{tab:headsets3}
\end{table}

\subsubsection{Google Cardboard}
In das Google Cardboard können Smartphones mit Displaygrößen zwischen 4 und 6 Zoll eingelegt werden. Um die Google Cardboard App zu nutzen, müssen die Geräten dann mindestens Android 4.1 oder iOS 8.0 haben.

Es kann kein Controller verbunden werden. Es kann lediglich mit einem Hebel ein Touch-Event auf dem Display ausgelöst werden. In der Google Cardboard App wieder die Interaktion beispielsweise so ausgelöst, dass der Mittelpunkt des Displays ``geklickt'' wird, wenn das Touch-Event ausgelöst wird. So sind einfache Eingaben möglich. 

Das Field of View der Google Cardboards kann je nach eingelegtem Smartphone bis zu 90° breit sein.

Durch die einfache Konstruktion und den günstigen Preis ist das Google Cardboard weit verbreitet. Zudem gibt es davon diverse Arten von Nachbauten, die teilweise statt dem Hebel, der das Touch-Event auslöst, schwache Magneten an der Seite des Cardboards bewegen um so Touch-Events auszulösen. \cite{GoogleLLCCardboard}

\subsubsection{Samsung Gear VR}
Die Samsung Gear VR Brille funktioniert nur mit den Galaxy Geräten ab Version 6 oder neuer und benötigt mindestens Android 5. Die Geräte werden auf dem USB-Stecker der Brille gesteckt, um so eine Verbindung herzustellen.
Wird ein Gerät zum ersten Mal verbunden werden Anweisungen angezeigt, die Oculus App herunterzuladen und einen Account zu erstellen beziehungsweise zu verbinden. Im Anschluss wird beim erneuten Verbinden mit dem Gerät die Oculus-App geöffnet um dort dann VR-Apps zu nutzen.

Da das Display vom Smartphone abhängig ist, kann hier nicht mit den anderen Geräten verglichen werden, aber das Field of View kann je nach eingelegtem Geräte bis zu 101° weit sein.

Im die Samsung Gear VR kommt mit einem Controller, der ein Touchpad und ``Zurück''-, ``Home''- und Lautstärke-Buttons hat. Auch den Controller der GearVR kann am Band des Headsets verstaut werden.

Die Gear VR ist nicht geeignet mit Brille genutzt zu werden. Dafür kann mit einem Rad an der Oberseite der Fokus eingestellt werden, sodass sie von Nutzern mit Sehschwächen trotzdem getragen werden kann.

Mit einem Preis von knapp 130\$ ist die GearVR die teuerste der drei Smartphone-gestützten VR-Brillen und schon nahe am Preis einer Oculus Go, die ohne ein extra Smartphone auskommt. Außerdem ist sie mit 345g ohne eingelegtem Smartphone eines der schwersten Headsets. \cite{SamsungElectronicsGmbHGearVR}

\subsubsection{Google Daydream View}
Das Google Daydream View Headset funktioniert ebenfalls nicht mit allen Smartphones, sondern nur Android-Smartphones: das Google Pixel 2 und 3, das Galaxy 8 und 9, das Zenfone AR von Asos und das LG V30 werden auf der Hersteller-Webseite als besonders geeignet aufgeführt. Grundsätzlich muss ein Gerät die Google Daydream App installiert haben innerhalb derer dann VR Apps gesucht und mit dem Controller verbunden und interagiert werden kann. 

Google Daydream View kann den aktuellen Bildschirminhalt auf einen Chromecast streamen, sodass andere sehen können, was gerade passiert.

Der Controller hat oben ein Touchpad, sowie einen Apps- und Home-Button. An der linken Seite sind zwei Lautstärketasten platziert. Da allerdings Daydream View selbst keine Lautsprecher hat werden damit die integrierten Smartphone Lautsprecher beziehungsweise die verbundenen Kopfhörer gesteuert.

Das Band mit dem die Brille am Kopf befestigt wird ist so gestaltet, dass man am hinteren Ende den Controller einstecken kann, sodass er nicht verloren gehen kann. \cite{GoogleLLCDaydream}

% ---------------------------------------------------------------------------------------------------------------------------
\cleardoublepage % Neue rechte Seite anfangen

\section{VR-Systeme im Bildungsbereich}
\label{sec:VReducation}
Virtual Reality wird in immer mehr verschiedenen Lebensbereichen eingesetzt und inzwischen gibt es neben den klassischen Simulatoren für Industriemaschinen und Flugobjekten auch verschiedene Ansätze von VR-Systemen im Bildungsbereich.

Merchant et al. schließen in ihrer Arbeit in 2014, dass Lernen mit Virtual Reality bessere Lernergebnisse produziert als Lernen ohne VR. Sie sehen damit die finanzielle und zeitliche Investition der Einführung von VR-Systemen als sinnvoll an. \cite{Merchant2014}

Im folgenden Kapitel werden drei dieser Systeme besprochen, die besonders interessant für den Einsatz im Unterricht sind.

In der Forschung wurden schon seit langer Zeit Systeme entwickelt, mit denen versucht wurde Behinderungen und psychische Störungen von Kindern und Jugendlichen zu verstehen und zu behandeln. Beispiele dafür sind:

Dazu zählt die Arbeit von Parsons et al \cite{Parsons2007}, in der die Aufmerksamkeitsperformance von Kindern mit ADHS in einem virtuellen Klassenzimmer mit standardisierten Methoden verglichen wird, die Entwicklung eines Lerntools für autistische Kinder von Strickland et al.  \cite{Strickland1996} und ``State-of-the-art of Virtual Reality technologies for children on the autism spectrum'' von Parsons und Cobb in 2011. \cite{Parsons2011}


\subsection{ClassVR}
Ein Beispiel für ein VR-System im Bildungsbereich, das einen ganz ähnlich Ansatz wie das im Rahmen dieser Arbeit entwickelte System verfolgt, ist ClassVR. 

ClassVR benötigt allerdings die ClassVR-Headsets und kann nicht mit anderen Geräten benutzt werden. Zudem wird ein Einrichtungsservice auf der Webseite angeboten.

Die ClassVR Headsets können sowohl für VR, als auch AR genutzt werden. Die Headsets nutzen das Android-Betriebssystem.
Die Geräte haben keinen Controller, sondern werden mit einfachen einhändigen Gesten und Kopfbewegungen gesteuert. Außerdem kann auch die Kontrolle durch die Lehrkraft übernommen werden, sodass alle verbundenen Geräte die Inhalte anzeigen, die die Lehrkraft auswählt.
Interessant an ClassVR ist, dass die Headsets in Boxen gekauft werden, in denen sie gelagert und gleichzeitig geladen werden können. Außerdem bietet die Firma auch fertige Inhaltssets an, die dann mit den Headsets genutzt werden können. Momentan sind die Inhalte aber ausschließlich für den britischen Markt ausgelegt, an dortige Lernpläne angepasst und nur in englischer Sprache erhältlich.

Ein Set aus acht Headsets in einem Koffer, in dem die Geräte geladen werden können, gibt es ab 2249£. Für einen Satz für eine Klasse mit 24 Kindern liegt der Preis also bei 6747£, was etwa einem Preis von 7650 Euro entspricht. Alternativ kann auch ein Schrank mit 30 Headsets erworben werden, dieser liegt preislich bei 7500£. \cite{LondonGridforLearning}
Hinzu kommt der Abo-Preis von 299£ für das ClassVR Portal, mit dem die Geräte gesteuert und Inhalte ausgewählt werden können.  \cite{InclusiveTechnologyLtd}

Die Lehrkraft kann eine Playlist mit den verfügbaren Inhalten anlegen, die dann auf die Schüler-Headsets geladen wird. Zudem können auch eigene 360°-Fotos und -Videos hochgeladen werden.
Es können 360°-Fotos und -Videos sowie 3D-Modelle angesehen werden. 3D-Modelle werden mit Karten aktiviert, auf denen ein Code ist, der dann das Anzeigen der Modelle aktiviert, sobald die Kamera ihn erkennt. Die Modelle werden dann in Augmented Reality dargestellt. Die 3D-Modelle sind relativ simple Modelle mit wenigen Polygonen, sodass sie schnell geladen werden können. Sie werden in einer festen Größe angezeigt und können nicht skaliert werden.
Sind die Inhalte auf die Geräte geladen, kann die Lehrkraft sehen, welche Geräte verbunden sind. Außerdem kann in Echtzeit verfolgt werden, was die Schüler in diesem Moment sehen und ein Punkt gesetzt werden, auf den die Aufmerksamkeit der Schüler gelenkt wird. Die Geräte senden zudem einen Livestream ihres Bildausschnitts an das ClassVR-System, das der Lehrkraft zusätzliche Kontrolle gibt. 
Die Kommunikation zwischen dem Rechner der Lehrkraft und den ClassVR-Headsets läuft über das Internet. Damit die großen Datenmengen der 360°-Inhalte übertragen werden können müssen sowohl das lokale WLAN als auch die Internetverbindung der Schule entsprechend ausgelegt sein. \cite{AvantisSystemsLtd}

\subsection{Google Expeditions}
Google Expeditions ist eine kostenfreie App, die es für die Smartphone gestützten VR-Headsets entwickelt wurde.
Sie kann einzeln genutzt werden oder im Unterricht. Wird sie im Unterricht genutzt, kann die Lehrkraft eine Guided Tour starten, in die sich dann die Schüler einklinken können. Dazu müssen sie im gleichen Netzwerk sein, in dem das Smartphone der Lehrkraft ist.

Es gibt VR-Touren, die aus 360°Foto-Strecken oder -Videos bestehen und AR-Touren, bei denen die Nutzer 3D-Modelle im Raum begutachten können. Um die Modelle zu zeigen müssen dafür spezielle Marker ausgedruckt werden, die im Kamerabild erkannt werden.

Wird eine Tour geleitet, kann der Tourleiter Markierungen setzen, auf die die Teilnehmer der Tour dann durch Pfeile in ihrem Bild hingewiesen werden. 
Momentan gibt es mehr als 900 verschiedene Expeditionen, die kostenlos genutzt werden können. Zudem können auch selbst Touren erstellt werden. \cite{GoogleLLCexpeditions}

\subsection{DiscoveryVR}
Die Discovery VR App ist eine Entwicklung des Discovery Channels, in der die Nutzer hochqualitative 360°-Videos ansehen können. Die App ist auf allen VR Plattformen kostenlos verfügbar.

Sie ist keine App, die speziell für die Nutzung in der Schule entwickelt wurde. Die verfügbaren Inhalte bieten sich aber für den Einsatz im Unterricht an. Die Videos haben dabei entweder eine englische Audiospur oder sind ohne Erzählerstimme. \cite{DiscoveryCommunications}


\subsection{Unimersiv}
Unimersiv ist eine App, die es für GearVR und Oculus Rift gibt. Innerhalb der App können unterschiedliche VR-Anwendungen genutzt werden. Beispielsweise kann die internationale Raumstation (ISS) erkundet werden. Im Moment sind neun verschiedene VR-Anwendungen auf der Plattform vorhanden. Schulen können für einmalig 49\$ pro Headset eine Lizenz erwerben, in der alle verfügbaren Inhalte enthalten sind und auch alle neuen Entwicklungen kostenlos geladen werden können.

Außerdem werden auch Simulations-Anwendungen für Unternehmen angeboten. Eine dieser Apps erlaubt es beispielsweise den Mitarbeitern einer Firma die Steuerung von Staplern in einem sicheren Rahmen zu erlernen. \cite{UNIMERSIV}

%_______________________________________________________________________________________________________________________
\cleardoublepage % Neue rechte Seite anfangen
\section[Herausforderungen]{Herausforderungen bei VR-Systemen}
\label{sec:challenges} 
In ihrem Artikel ``5 Major Challenges For The VR Industry'' aus dem März 2018 sieht Wolwort den Preis, fehlende Inhalte, fehlende Geschäftsmodelle, die unklaren Folgen für die Gesundheit der Nutzer und das Image als ``gimmick'' als Gründe dafür, dass VR bisher noch nicht den prophezeiten Durchbruch hat. \cite{Wolwort2018}

Wie bereits in \ref{subsec:price} diskutiert ist der Preis, seitdem es Smartphone-gestützte VR-Headsets gibt, ein deutlich kleineres Problem, wenn die Kosten für das Smartphone selbst nicht mit gerechnet werden. Aber auch stand-alone Einsteiger-Headsets wie die Oculus Go erlauben einen günstigeren Einstieg in die Thematik.

Der zweite Problempunkt der fehlenden Inhalte ist nach wie vor akut: Abseits von Spielen gibt es nach wie vor wenig Anwendungen, die für VR-Headsets entwickelt wurden. Besonders im Bildungsbereich sieht die Autorin eine Zukunft mit Virtual Reality-Apps, die eine komplett andere Wahrnehmung der Inhalte bieten. 
Besonders bei Simulator-Trainings wird bereits seit längerer Zeit auf VR gesetzt \cite{furness1978visually}, diese sind aber meist nicht auf konventionellen VR-Headsets nutzbar. Eine neuere Applikation für VR in der Bildung sind Google Expeditions, wie in \ref{sec:VReducation} genauer erläutert. 
Neben Inhalten können aber auch soziale Nutzungen von VR interessant sein. ``Facebook Spaces'' beispielsweise, erlaubt den Nutzern sich mit Freunden auszutauschen, Fotos zu teilen, Videotelefonate zu führen und zusammen in VR zu malen. Ebenso ist eine soziale Nutzung auch im professionellen Kontext denkbar, beispielsweise für virtuelle Meetingräume. \cite{FacebookTechnologiesLLCspaces}

Drittens sieht Wolfort das fehlen von Geschäftsmodellen, mit denen Unternehmen rentabel VR-Software anbieten können. Das Problem wird noch dadurch verschärft, dass es keinen technologischen Standard gibt, sodass native Anwendungen für mehrere Plattformen einzeln entwickelt werden müssen. Standards wie WebVR können hierfür eine wichtige Rolle spielen.
Für die im Rahmen dieser Arbeit entwickelte Software VRClassroom, könnte ein das anbieten von Inhalten für den Unterricht ein Geschäftsmodell sein, ähnlich zu Lehrfilmen oder -büchern.

Die für die Autorin wohl kritischste Herausforderung für Virtual Reality sind unerforschte mögliche Gesundheitsrisiken. Bekannt und erforscht ist bisher nur die Virtual Reality- oder Cybersickness, auf die in \ref{subsec:cybersickness} genauer eingegangen wird. Langfristige Risiken und Folgen starker Nutzung sind noch nicht erforscht worden.

Als letztes sagt Wolfort, VR werde momentan nur als ``gimmick'' wahrgenommen. Das ist zu Teilen auch verständlich, da bisher nur wenige Anwendungen abseits von Spielen verfügbar sind. Wie aber bereits oberhalb beschrieben und auch im Rahmen dieser Arbeit entwickelt, gibt es nun immer mehr Anwendungen, die eine Nutzung abseits von Spielen erlauben. Die Spiele-Industrie war jedoch für viele Entwicklungen auch früher schon eine treibende Entwicklungskraft  \cite{Wolwort2018}.

\subsection{Virtual Reality Sickness}
\label{subsec:cybersickness} 

Da die Begriffe oft vermischt werden und dadurch oft unklar definiert sind, werden im Folgenden die Begriffe ``motion sickness'', ``simulator sickness'', ``virtual reality sickness'' und ``cybersickness'' noch einmal differenziert und für diese Arbeit definiert.

\paragraph{Motion Sickness}
Motion Sickness beschreibt die Symptome wie sie zum Beispiel beim Fahren in einem Auto als Beifahrer entstehen können. Bekannt und erforscht ist Motion Sickness vor Allem bei Piloten und Astronauten. \cite{Stone2017}

\paragraph{Simulator Sickness}
Simulator Sickness bezeichnet man die negativen Effekte, die man nach der Nutzung von jeglichen Simulatoren verspüren kann. Oft handelt es sich dabei um die Differenz der simulierten Bewegungen zu denen, die der Körper wirklich erfährt, die dann zu Schwindel und Übelkeit führen können.  \cite{Stone2017}

\paragraph{Virtual-Reality-Sickness und Cybersickness}
Virtual-Reality- und Cybersickness sind Synonyme. Sie bezeichnen die Symptome, die Nutzer erleben können, wenn sie Virtual-Reality-Systeme verwenden. Sie sind verwandt mit der Simulator Sickness, allerdings beziehen sie sich ausschließlich auf die Folgen von Virtual Reality-Anwendungen. \cite{Stone2017}

\bigskip

Symptome von Cybersickness sind Schwindelgefühle, Orientierungslosigkeit, Augenschmerzen, Müdigkeit und Übelkeit. In extremen Fällen kann es auch zu Erbrechen führen. Physiologische Indikatoren von  Cybersickness sind Reaktionen des sympathischen Nervensystems wie elektrodermale Aktivität, die sich durch Hautrötungen und Schweiß äußern können, oder eine erhöhte Herzfrequenz. \cite{Stone2017}
 
Eine Umfrage von 2017, die auf reddit unter VR-Nutzern durchgeführt wurde zeigte sich, dass 60\% der insgesamt 862 Teilnehmer Symptome von Cybersickness erleben. 35\% hatten dabei leichte Symptome, 15\% mittel starke und 8\% stark ausgeprägte Symptome. 2\% der Teilnehmer gab an extrem starke Probleme mit Cybersickness zu haben. \cite{Unknown2017}
Diese Umfrage zeigt, dass Cybersickness nach wie vor ein großes Problem im Umgang mit VR-Headsets ist, an dessen Linderung weiter geforscht werden muss, damit Virtual Reality sich weiter durchsetzen kann.

Die Ergebnisse der Online-Befragung passen zu der Studie, die Stone in seiner Dissertation durchgeführt hat. Bei seiner Befragung mit 202 Teilnehmer erlebten 46\% keinerlei Symptome von Cybersickness, 35\% leichte Symptome, 13\% mittelstarke und 7\% sehr starke Symptome. 
Zudem ermittelten sie, dass Teilnehmer, die im VR-Headset noch eine Brille tragen, deutlich öfter Symptome von Cybersickness erlitten.
Trotz der vielen Personen, die Cybersickness-Symptome erlebten gaben 54\% im Anschluss an die Studie an, wieder VR-Headsets nutzen zu wollen, da sie Spaß an der Nutzung hatten.  \cite{Stone2017}

In seinem Artikel ``Everything You Wanted to Know About Simulator Sickness'' beschreibt der Autor, dass besonders Kinder zwischen 2 und 12 Jahren anfällig für Cybersickness. Außerdem sind laut ihm Frauen öfter von Symptomen betroffen als Männer. Zudem sind Personen, die eine Krankheit wie eine Ohrinfektion, Erkältung oder Grippe haben, sowie Personen die unter Alkohol- oder Drogeneinfluss stehen oder einen Kater haben öfter Cybersickness ausgesetzt. \cite{Burke2014}

Eine relativ einfache Maßnahme um das Auftreten von Cybersickness zu verhindern ist das Field of View der VR-Brillen zu verschmälern. Allerdings hängt der Grad der Immersion eng mit dem Field of View zusammen. Wird also das Field of View verkleinert, sinkt die Immersion und die Nutzer haben weniger Spaß an dem Virtual Reality Erlebnis. \cite{Lin2002}

In 2015 hat ein Team der Purdue University zudem herausgefunden, dass Cybersickness dadurch bekämpft werden kann, indem dem Nutzer eine virtuelle Nase angezeigt wird. Sie gibt dem Auge einen fixen Punkt im Bild, der dann hilft die Bewegungen im Rest des Bildes zu verarbeiten. Sie erklären, dass sie durch das Phänomen der Seekrankheit auf die Idee gekommen sind die Nase ins Bild zu integrieren, da es sich genau gegenteilig dazu verhält.\cite{whittinghill2015nasum}

Einen weiteren Ansatz zur Lösung des Problems der Cybersickness hat Eric Bear, der Gründer von MONKEYmedia. Er fand heraus, dass wenn die Bewegungen in der VR-Welt durch Neigen des Kopfes ausgelöst werden, als stünde der Nutzer auf einem Hoverboard, zeigen deutlich weniger Probanden Symptome der Cybersickness. Zusätzlich kommt diese Art der Bewegung gänzlich ohne die Nutzung von Controllern aus, sodass die Hände für andere Interaktionen frei bleiben.  \cite{Samit2018}

\subsection{Mindestalter zur Nutzung von VR-Systemen}
Fast alle Hersteller geben in ihren Nutzungsbedingungen oder Sicherheitsanweisungen ein Mindestalter für die Benutzung ihrer VR-Systeme an. Wie in Tabelle~\ref{tab:tableage} zu sehen, sind sich die Hersteller sehr einig, dass VR-Headsets nicht für kleine Kinder geeignet sind und frühestens für Jugendliche in Frage kommen.
\bigskip

Oculus weist konkret darauf hin, dass eine Nutzung ihrer Geräte unter 13 Jahren ihren Nutzungsbedingungen widerspricht. ``The Services are intended solely for users who are aged 13 or older. Any registration for, or use of, the Services by anyone under the age of 13 is unauthorised, unlicensed and in breach of these Terms.'' Es werden allerdings keine genauen Gründe für diese Altersrestriktion angegeben. \cite{FacebookTechnologiesLLC2018}

Samsung geht dabei noch einen Schritt weiter und warnt vor einer Nutzung unter 13 Jahren, da sich jüngere Kinder in einer ``critical period in visual development''  ~\cite{SAMSUNG} befinden. Zudem sollen auch Kinder über 13 Jahren nur unter Aufsicht einer erwachsenen Person die Gear VR benutzen und dabei darauf achten regelmäßig Pausen zu machen. Eine lange Nutzung soll generell vermieden werden und die Kinder sollen während und nach der Nutzung beobachtet werden, ob sich ihre Fähigkeiten in der Hand-Augen-Koordination, Balance oder Multi-Tasking verschlechtern.

Außerdem wird eine Liste an Symptomen aufgeführt bei deren Anzeichen eine Nutzung sofort unterbrochen werden soll. Das sind: ``seizures, loss of awareness, eye strain, eye or muscle twitching, involuntary movements, altered, blurred, or double vision or other visual abnormalities, dizziness, disorientation, impaired balance, impaired hand-eye coordination, excessive sweating, increased salivation, nausea, lightheadedness, discomfort or pain in the head or eyes, drowsiness, fatigue, or any symptoms similar to motion sickness'', also verschiedenste Probleme beim Sehen und an den Augen sowie Probleme der Konzentration, Koordination und Balance.   ~\cite{SAMSUNG}

HTC dagegen gibt für die Nutzung der HTC Vive beziehungsweise Vive Solo kein genaues Mindestalter an. Sie geben allerdings an, dass das Gerät nicht dafür ausgelegt ist von kleinen Kindern genutzt zu werden. Sie warnen davor, dass Kinder Kleinteile verschlucken könnten oder sich und Andere auf anderem Wege damit verletzen können. Für ältere Kinder empfehlen sie die Aufsicht einer erwachsenen Person und dass die Nutzungszeit nicht zu lang ist. \cite{HTC2016}

Zudem wird für die Nutzung der HTC Vive ein HTC Account benötigt, der laut HTC erst ab 14 Jahren erlaubt ist. \cite{HTCCorporation}


In ihren FAQs gibt Sony an, dass man zur Nutzung ihrer Playstation VR Konsole mindestens zwölf Jahre oder älter sein sollte. Weitere Angaben oder Gründe dieses Mindestalter sind auch hier nicht zu finden. \cite{SonyEntertainmentLLC2017}
\bigskip

Gegenüber all der Warnungen der Geräte-Hersteller gibt Martin Banks, Professor of Optometry, Vision Science, Psychologie, and Neuroscience an der University of California in Berkeley in einem Interview im Frühling 2016 an, dass er ``no concrete evidence that a child of a certain age was somehow adversely affected by wearing a VR headset,'' [keine konkreten Beweise, dass ein Kind in einem gewissen Alter durch das Tragen von VR-Brillen negativ beeinflusst wurde] gefunden hat. Er ist überzeugt, dass die Hersteller der VR-Headsets die Nutzung durch Kinder ausschließen, um sicher sein zu können, dass nicht später bekannt werdende Probleme bei Kindern, die VR-Headsets nutzen, ihnen angelastet werden können. 

Weiter gibt er an, dass die Angst, dass die Entwicklung des Auges negativ beeinflusst wird im Gegensatz zur Nutzung von Büchern oder Smartphones viel unproblematischer ist, das durch die in die VR-Brillen eingebauten Optiken das Auge gar nicht auf eine so nahe Sache fokussiert, sondern auf weiter entfernte und somit keine Schäden der Augen nach sich zieht. 

Banks sieht als Gefahren lediglich die gleichen, die auch für Erwachsene bestehen: Das sind hauptsächlich Virtual Reality Sickness, auch bekannt als Cybersickness, und die Gefahr mit Personen oder Gegenständen im Raum zu kollidieren, während das VR-Headset getragen wird.  Ansonsten bewertet er die Nutzung der VR-Brillen von Kindern unproblematisch. \cite{Hill2016}

\bigskip

Momentan gibt es keine veröffentlichten Forschungsarbeiten zu den Gefahren für Kinder bei der Nutzung von VR-Brillen, dagegen sind viele Arbeiten zu finden, die VR-Systeme in der Therapie von verhaltensauffälligen, lernverzögerten oder behinderten Kindern erfolgreich einsetzen. 
Beispiele für solche Systeme wurden bereits in \ref{sec:VReducation} besprochen.


\begin{table}[]
\begin{tabular}{p{0.2\linewidth}|p{0.2\linewidth}|p{0.5\linewidth}}
\textbf{VR-Gerät} & \textbf{Mindestalter} & \textbf{Weitere Angaben} \\ \hline
Oculus Rift \newline Oculus Go \newline  Oculus Quest & 13 Jahre & keine  \\
HTC Vive \newline HTC Vive Solo & keine genaue Altersangabe & HTC Account erst ab 14 Jahren  \\
Google Daydream & 13 Jahre & keine  \\
Samsung Gear VR & 13 Jahre & nur unter Aufsicht eines Erwachsenen  \newline regelmäßig Pausen machen \newline Warnung vor einer Vielzahl an Symptomen aus dem Bereich Koordination, Balance und Sehen \\
Playstation VR & 12 Jahre & keine  \\
Google Cardboard & keine Angabe & nur unter Aufsicht eines Erwachsenen
\end{tabular}
  \caption{Übersicht der verschiedenen VR-Headsets mit ihren jeweiligen Nutzungsmindestaltern}~\label{tab:tableage}
\end{table}



%_______________________________________________________________________________________________________________________

\cleardoublepage % Neue rechte Seite anfangen
\section{360°-Inhalte}
Neben der verwendeten Hardware spielen die verfügbaren Inhalte für das Virtual Reality Erlebnis eine erhebliche Rolle. Nur wenn die gezeigten Inhalte richtig in der 360°-Umgebung dargestellt werden bringt die Darstellung Nutzung von VR-Headsets die gewünschte Immersion.

Für 360°-Inhalte meist keine eigenen Dateiformate entwickelt, sondern gewohnte Formate für zweidimensionale Inhalte verwendet gespeichert. In dieser Arbeit werden vier verschiedene Medientypen von 360°-Inhalten: 360°-Fotos, -Videos, 3D-Modelle und spatial Audio, auch bekannt als 3D-Audio behandelt.

\subsection{Projektionen von 360°-Fotos und -Videos}
360°-Panoramen sind kugelförmig und der Betrachter steht im Zentrum der Kugel. Um das kugelförmige Bildmaterial in 2D Bildformaten zu speichern, wird es auf verschiedene Arten ``zerschnitten'' oder verzerrt, um dann zur richtigen Darstellung wieder zusammengesetzt werden zu können. Das Panorama wird dazu auf ein zwei-dimensionales Bild projiziert, um in den bekannten 2D-Bildformaten speicherbar zu sein. Die Information, wie das Panorama projiziert wurde, wird dann in den Metadaten der Datei gespeichert. 360°-fähige Software kann mit diesen Informationen das zweidimensionale Bildmaterial wieder in eine kugelförmige Darstellung umrechnen.

Viele der verwendeten Projektionen wurden ursprünglich dazu entwickelt Weltkarten drucken zu können, auf denen die gesamte Erdkugel zu erkennen ist. Diese Technik wird auch für die Projektion von 360°-Inhalten verwendet.

Im Folgenden werden die bekanntesten und am weitesten in der 360°-Szene verbreiteten Projektionen erläutert und verglichen:

Die Abbildungen zeigen das Ergebnis der Projektion einer Kugel, bei der die Längen- und Breitengrade alle 10° durch eine Linie dargestellt werden.

\subsubsection{Equirectangulare Projektion}
\begin{figure}[ht]
  \centering
  \includegraphics[width=\textwidth]{images/projection-equirectengular.eps}
 \caption{Die equirectangulare Projektion einer Kugel.}
  \label{fig:projection-equirectengular}
\end{figure}

Verbildlichen kann man die Projektion etwa damit, dass eine Kugel in einem Zylinder sitzt, der den gleichen Durchmesser wie Kugel hat und dann die Pole nach außen zieht, bis die Kugel die Form des Zylinders angenommen hat.

Bei der equirectangularen Projektion werden die Gradzahlen von Längen- und Breitengrade auf die x und y-Werte in einem Gitternetz projiziert. Ein Bildpunkt an (53°/ 90°) wird dann im projizierten Bild an (53,100) dargestellt. Es wird keine zusätzliche Skalierung oder Transformation vorgenommen, wie in Abbildung \ref{fig:projection-equirectengular} zu sehen ist.
Dadurch werden Äquator-nahe Bildbereiche wenig verzerrt und die Pol-nahen Regionen extrem in horizontaler Richtung verzerrt, sodass die Pole der Kugel auf die Breite des Äquators ausgedehnt werden. 
Vertikale Linien bleiben unverzerrt erhalten, horizontale Linien werden bis auf den Äquator immer verzerrt dargestellt. Dadurch bleiben zudem keine Winkel erhalten.

Die equirectangulare Projektion hat dadurch den Nachteil, dass die Bildelemente am Äquator eine deutlich geringere Auflösung hat als die Bildbereiche an den Polen, was meist diejenigen sind, die interessanter für den Betrachter sind. Außerdem ist durch die starke Verzerrung das Bild an den Rändern schwer zu erkennen.

Trotz diesen Nachteilen ist die equirectangulare Projektion die am weitesten verbreitete Projektion in der 360°-Szene, was vermutlich auf die einfache Berechnung der Projektion zurückzuführen ist.

Die Online-Video-Plattform Vimeo nutzt für 360°-Videos die equirectangular Projektion \cite{VimeoInc} und auch Youtube hat bis 2017 diese Projektion benutzt bis sie auf ihren eigens entwickelten Standard EAC gewechselt haben. \cite{Wheeler2017}


\subsubsection{Zylindrische Projektion}
Die zylindrische Projektion hat einen ähnlichen Ansatz wie die Equirectangulare. Auch hier kann man sich die Kugel in einem stehen Zylinder vorstellen, der die Kugel am Äquator berührt. Dann wird die Oberfläche der Kugel aus dem Mittelpunkt auf den Zylinder projiziert. Dadurch bleiben vertikale Linien und der Äquator unverzerrt, alle anderen Linien werden allerdings verzerrt. (Abbildung \ref{fig:projection-cylindric})

Durch diese Projektion werden Bildbereiche die Nahe den Polen extrem in vertikaler Richtung verzerrt, sodass sie viel größer erscheinen, als sie eigentlich sind. Daher ist ein vertikales Field of View, das höher als 120° ist, für zylindrische Projektion nicht zu empfehlen. Die Pole werden ins unendliche projiziert, deshalb kann diese Projektion keine komplette Kugel abbilden.

\begin{figure}[ht]
  \centering
  \includegraphics[width=\textwidth]{images/projection-cylindric.eps}
 \caption{Die zylindrische Projektion einer Kugel.}
  \label{fig:projection-cylindric}
\end{figure}

\subsubsection{Mercator-Projektion}
\begin{figure}[ht]
  \centering
  \includegraphics[width=\textwidth]{images/projection-mercartor.eps}
 \caption{Die Mercator-Projektion einer Kugel.}
  \label{fig:projection-mercator}
\end{figure}

Die Mercator-Projektion ist die wohl berühmteste Projektion für Landkarten, die seit ihrer Entwicklung 1569 für Weltkarten genutzt wird und auch heute noch besonders in der Nautik eine große Rolle spielt. 

Wie in Abbildung \ref{fig:projection-mercator} zu sehen sieht eine Mercator-Projektion ähnlich wie eine zylindrische Projektion aus, die Berechnung ist aber ungleich viel komplexer. Der große Vorteil einer Mercator-Projektion ist, dass sie winkelttreu ist. So kann eine Route zwischen zwei Punkten gezeichnet werden und die Gradzahl, die sich auf der Karte abläsen lässt stimmt mit dem tatsächlichen Winkel überein. Dadurch bewirkte die Mercator-Projektion einen Durchbruch in der Kartografie und Seefahrt. Auch heute sind viele Karten noch in der Mercator-Projektion abgebildet.

Problematisch an dieser Projektion ist wie auch bei der zylindrischen Projektion die extreme Verzerrung von Bereichen, die weit vom Äquator entfernt sind. Sie kann auch keine komplette 180° Vertikal-Projektion produzieren, sondern nur bis etwa 85° nördlich und südlich, und deckt damit einen Bereich von 170° an. Diese extreme Verzerrung führt beispielsweise dazu, dass auf Weltkarten Grönland oft größer als der ganze afrikanische Kontinent wirkt, obwohl es in der Realität nur etwa einem Vierzehntel der Fläche entspricht.

Varianten der Mercator-Projektion sind bis heute der de-facto Standard für digitale Karten. Auch Google Maps hat lange eine Abwandlung der Mercator Projektion, die so genannte ``Web Mercator Projection'' verwendet. Im August 2018 haben sie jedoch ihren Wechsel zu einer Kugeldarstellung angekündigt. Bisher ist diese neue 3D Globe Ansicht allerdings nur auf dem Desktop ausgerollt und nicht in den mobilen Apps zu sehen. \cite{GoogleLLC2018maps}

\subsubsection{Rectilinear-Projektion}
Die Rectilinear-Projektion ist anders als die vorigen Projektionen keine zylindrische Projektion. Sie lässt sich verbildlichen indem man eine Kugel auf eine Fläche legt und einen Teil der Kugel vom Mittelpunkt der Kugel auf die Ebene projiziert. Ein einzelnes rectilineares Bild stellt also kein komplettes 360°-Panorama dar. Um ein komplettes Panorama zu erhalten werden dann einfach mehrere rectilineare Projektionen zusammen gesetzt.

Ein großer Vorteil dieser Projektion liegt darin, dass gerade Linien erhalten bleiben. Eine starke Verzerrung in dieser Projektion ist in den Ecken des Bildes zu finden und wird umso stärker, je weiter das Field of View ist. Will man ein möglichst unverzerrtes Panorama, sollten mehrere rectilineare Bilder mit einem kleineren Field of View zusammengesetzt werden.

Bekannt unter dem Begriff Cubemap-Projektion oder Skybox aus der Gaming-Szene ist die Kombination von sechs rectilinearer Bilder, die wie die Seiten eines Würfels angeordnet sind. Eine solche Cubemap wird in Abbildung \ref{fig:projection-cubemap} dargestellt. Ein bekanntes Beispiel, bei dem Cubemap-Bilder verwendet werden, sind die Google StreetView-Panoramas. 

\begin{figure}[ht]
  \centering
  \includegraphics[width=\textwidth]{images/projection-cubemap.eps}
 \caption{Die Cupemap einer Kugel zusammengesetzt aus sechs rectilinearen Teilprojektionen der Kugel.}
  \label{fig:projection-cubemap}
\end{figure}

\subsubsection{Stereographische Projektion}
Die Stereographische Projektion hat den gleichen Ansatz wie die rectilineare Projektion. Der Unterschied ist, dass nicht aus dem Mittelpunkt der Kugel projiziert wird, sondern aus dem Punkt an der Oberfläche der Kugel, der exakt gegenüber dem Punkt liegt, der die Ebene berührt, auf die projiziert wird. 

Auch mit der stereographischen Projektion ist keine volles 360°-Panorama möglich, sondern nur bis etwa 330°. Empfehlenswert ist es allerdings ein Panorama aus drei 120°-Projektionen zusammenzusetzen. 

Stereographische Projektionen erzeugen immer ein Rundes Bild, das einer Fish-Eye Projektion ähnlich sieht, wie Abbildung \ref{fig:projection-stereographic} zeigt.

\begin{figure}[ht]
  \centering
  \includegraphics[width=\textwidth]{images/projection-stereographic.eps}
 \caption{Die stereographische Projektion einer Kugel zeigt bis zu 330° der Kugel.}
  \label{fig:projection-stereographic}
\end{figure}

\subsubsection{Pannini/ Vedutismo-Projektion}
Die Vedutismo- oder Pannini-Projektion ist ebenfalls eine zylindrische Projektion und hat damit dem Vorteil, dass alle vertikalen Linien unverzerrt bleiben. Mit dieser Projektion werden jedoch zusätzlich auch noch alle Linien, die durch das Projektionszentrum gehen ebenfalls als gerade Linien dargestellt. Die Perspektive mit einem zentralen Fluchtpunkt wird dadurch also für einen weiten Blickwinkel möglich.

Die Pannini-Projektion lässt sich als eine rectilineare Projektion einer Kugel auf einen Zylinder vorstellen. Das Projektionszentrum kann dabei überall auf der Sichtachse liegen mit der Distanz d zur Zylinder-Achse. Ist d also d=0, wird eine normale rectilineare Projektion erzielt, geht d gegen unendlich entsteht eine zylindrische Projektion. Durch die Variation von d werden jeweils unterschiedliche Projektion erreicht. Die Konstruktion einer Pannini-Projektion wird in \ref{fig:projection-pannini} abgebildet.
Die Pannini-Projektion entspricht dabei d=1.

\begin{figure}[ht]
  \centering
  \includegraphics[width=\textwidth]{images/projection-pannini.eps}
 \caption{Die Konstruktion einer Pannini-Projektion.}
  \label{fig:projection-pannini}
\end{figure}

Eine einzelne Pannini-Projektion ist allerdings für maximal 150° in beiden Richtungen möglich, sodass ein volles Panorama wiederum durch mehrere Bilder zusammengesetzt werden muss, ähnlich zur rectilinearen Projektion.

\subsubsection{Equi-Angular Cubemap (EAC)}
Wie bereits erwähnt hat YouTube vor einigen Jahren ihren eigenen Standard für 360°-Bildprojektionen Equi-Angular Cubemap entwickelt. Der Name Cubemap verrät bereits, dass auch dieses Bild wieder aus mehreren Projektionen zusammengesetzt wird. Hierbei werden allerdings die einzelnen Projektionen nicht aus rectilinearen Projektionen zusammengesetzt.

Die Teile der EAC-Cubemap sind sehr ähnlich wir rectilineare Projektionen mit dem Unterschied, dass Bildabschnitte in Äquatornähe genauso hoch aufgelöst werden wie Bildbereiche nahe den Polen. Die Pixeldichte bleibt also an allen Bildbereichen gleich, sodass alle Bereiche des 360°-Bildes eine gleiche Auflösung haben. Letztendlich führt das dann zu einer besseren Videoqualität mit gleicher Auflösung. Abbildung \ref{fig:projection-eac} zeigt die Unterschiede in der Auflösung bei einer normalen Cubemap und einer Equi-Angularen Cubemap. \cite{Brown2017}

\begin{figure}[ht]
  \centering
  \includegraphics[width=\textwidth]{images/projection-eac.eps}
 \caption{Vergleich einer rectilinearen und einer equi-angularen Cubemap (EAC).}
  \label{fig:projection-eac}
\end{figure}


\subsubsection{Fish-Eye Projektion}
Die Fish-Eye Projektion lässt sich wie die Spiegelung in einer Silberkugel vorstellen. Sie kann eine Szene mit bis zu 180° in beiden Dimensionen darstellen und erzeugt damit ein rundes Bild. Bildbereiche nahe dem Zentrum werden vergrößert dargestellt, während Bildbereiche nahe den Rändern stark verzerrt werden. Die Fish-Eye Projektion ist meist nicht als End-Projektion sondern eher als Speicherformat von 360°-Kameras zu finden, das dann noch weiter verarbeitet wird. \cite{CambridgeinColour}

Viele 360° Kameras, wie zum Beispiel auch die Rico Theta haben zwei Linsen, die jeweils ein 180°-Fish-Eye-Panorama erzeugen. Diese können dann in der Nachbearbeitung zu vollen 360°-Panoramas in einer anderen Projektion umgerechnet werden. Eine volles Panorama ist mit einer Fish-Eye Projektion nicht darstellbar und auch das zusammensetzen mehrerer Fish-Eye Panoramen wie bei rectilinearen Projektionen ist nicht möglich. \cite{RicohCompanyLtd}

\subsection{360°-Fotos}
Bei 360°-Fotos handelt es sich um Bilddateien, die wenn sie mit Programmen angesehen werden, die 360°-Inhalte korrekt anzeigen können, ein volles Panorama um den Nutzer bilden, in dem er sich dann umsehen kann. 

Die wahrscheinlich bekanntesten Anwendungen, die 360°-Bilder anzeigen sind Google StreetView \cite{GoogleLLCphotos} und 360°-Fotos auf Facebook \cite{FacebookTechnologiesLLCphotos}, die der Nutzer einfach durch bewegen des Smartphones erkunden kann.

\subsubsection{Quellen für 360°-Fotos}
Es gibt bisher keine Plattform, die sich darauf spezialisiert hat 360°-Fotos, die in der Lehre genutzt werden können, anzubieten. 

Die verbreiteten Anbieter für Archivbilder wie zum Beispiel Shutterstock oder iStockPhoto bieten zwar bereits 360°-Fotos an, aber die meisten dieser Archivbilder zeigen keine relevanten Inhalte, sondern nur eine schöne Landschaft oder einen Raum. Sie sind also für zur Nutzung im Unterricht nicht geeignet. Zudem sind diese Bilder oft teuer oder mit einem Abo beim jeweiligen Anbieter verbunden.

Die aktuell beste Quelle für 360°-Fotos ist die Foto-Sharing-Plattform Flickr, auf der die Urheber das Herunterladen ihrer Bilder erlauben und auch die Nutzungslizenz dafür angeben können. Eine Suche nach ``equirectangular'' und der Creative Commons Lizenz ist dort einfach möglich und man wird schnell fündig.\cite{Flickr}
Flickr hat zudem die Funktionalität die equirectingularen 360°-Fotos richtig darzustellen, sodass die Nutzer sich im Bild umsehen können, indem sie den Viewport verschieben.

Ein engagierter Lehrer hat sogar eine Gruppe gegründet, die speziell dafür gedacht ist, dass Lehrkräfte aus aller Welt ihre 360°-Fotos teilen können, damit Andere sie auch in ihrem Unterricht nutzen können. \cite{Flickrgroup}

Eine weitere Möglichkeit an 360°-Fotos zu kommen, die größtenteils ebenfalls von Hobby-Fotografen stammen, ist die Google Photo Sphere Community, eine Gruppe auf Google Plus. Allerdings ist hier der Download nicht immer erlaubt und die Rechte für die Bilder auch nicht immer angegeben. \cite{GoogleLLCphotosphere}

Außerdem gibt es noch die Möglichkeit 360°-Fotos bei 360cities oder Airpano zu lizensieren. 
360cities ist eine Plattform auf der professionelle 360°-Fotographen ihre Panoramen hochladen und zum Verkauf anbieten können. Dabei handelt es sich oft um sehr gute Bilder, es ist aber relativ teuer, sodass sich das für die Lizenzierung für eine einzelne Lehrkraft nicht wirklich anbietet. \cite{360Citiess.r.o}

Airpano bietet speziell Bilder von Orten auf der Welt an wie zum Beispiel Städten oder geschichtlichen Orten aus der Luftperspektive an. Die Lizenzierung dieser Bilder ist mit ca. 1000 Euro pro Jahr teuer, sodass die Nutzung im Unterricht schwierig ist. \cite{AirPano}

\subsubsection{Erstellung und Bearbeitung von 360°-Fotos}
Es gibt grundlegend zwei verschiedene Arten 360°-Bilder zu erstellen: Eine einfache Möglichkeit, ist die Verwendung einer 360°-Kamera wie zum Beispiel der Vuze 360, 360 Rize, Rico Theta, Samsung Gear 360, oder Panono 360 Kamera. Diese Kameras haben mehrere Linsen und Sensoren eingebaut und generieren aus den Einzelbildern ein fertiges 360°-Foto, das dann direkt weiterverwendet werden kann.


Die zweite Möglichkeit ist mehrere normale Kameras auf ein Rig, also ein Stativ für mehrere Kameras, zu installieren. Jede Kamera deckt dabei einen anderen Bildabschnitt ab und die Bilder werden nachträglich manuell zusammengesetzt. 
Den Vorgang des Zusammensetzens der Bilder nennt man Stitching. 

Das Stitching kann entweder mit einer speziellen Software wie zum Beispiel PTGui oder mit bekannten Bildbearbeitungsprogrammen Photoshop oder Lightroom durchgeführt werden.

Erstellt man 360°-Fotos durch zusammenstitchen mehrerer ``normaler'' Kamerabilder ist es wichtig, dass dann die richtigen Metadaten zum Bild hinzugefügt werden, damit anzeigende Software weiß, dass es sich um ein 360°-Bild handelt und welche Projektion verwendet wurde.
360°-Bilder, die direkt von einer 360°-Kamera kommen, haben bereits die passenden Metadaten und sie müssen nicht manuell hinzugefügt werden.
Die Metadaten können auf verschiedene Art und Weisen zu den Fotos hinzugefügt werden, unter anderem mit Photoshop oder mit dem ExifMetaLrPlugin für Lightroom. 
In den Metadaten sind ist gespeichert, welche Software zum Stitching verwendet wurde, welche Dimensionen das Bild hat, was die Ausgangsblickrichtung ist und natürlich welche Projektion das Panorama hat. 
Zusätzlich können dort auch noch die Koordinaten, an denen das Bild aufgenommen wurde, der Urheber, das Copyright und einige andere Informationen gespeichert sein, die jedoch nicht spezifisch für 360°-Bilder sind. \cite{Panotwin2014}

\subsection{360°-Videos}
In seinem Artikel ``360 Videos und ihre Zukunft: eine Prognose'' im filmpuls-Magazin, einem Fachmagazin für Filmemacher, erklärt der Autor Kristian Widmer, dass die größte Herausforderung für die Produktion von guten 360°-Filmen ist, dass komplett eigene Erzählkonzepte entwickelt werden müssen, da die aus dem klassischen Filmformat nicht anwendbar sind. Außerdem können viele Tricks und Praktiken nicht angewandt werden, weil es keine unsichtbaren Bereiche um die Kamera gibt. 

Seiner Meinung nach sollten 360°-Filme so angelegt werden, dass der Zuschauer dazu ermuntert wird die Szene zu erkunden und dazu auch 3D-Sound integriert werden sollte, um zu suggerieren wo im Panorama gerade interessanter Inhalt zu sehen ist.

Die großen Videoplattformen Vimeo und YouTube haben seit einigen Jahren die Funktion 360°-Videos abzuspielen und bieten auch Möglichkeiten die Filme in VR-Headsets zu sehen. Widmer kritisiert hier allerdings die Qualität der Filme und die Tatsache, dass oft nur Stereo-Sound in dem Filme verwendet wird. \cite{Widmer}

Allerdings sind erst durch die Angebote von YouTube und Vimeo 360°-Filme und ihre Möglichkeiten weiter in das Bewusstsein der breiten Masse gekommen.

\subsubsection{Quellen für 360°-Videos}
Eine gute Quelle für 360°-Videos is genau wie bei 360°-Fotos Vimeo. Wie bereits oben beschrieben, können die Eigentümer der Videos angeben unter welcher Lizenz sie das Video veröffentlichen und es gegebenenfalls zum Download freigeben. In den meisten Fällen sind sie frei zur privaten Nutzung und nur die kommerzielle Weiterverwendung untersagt.

Auch bei YouTube gibt es viele interessante 360°-Videos. Allerdings lässt YouTube den Download der Videos nicht zu. Es stehen aber auf fast allen Geräten und Plattformen Apps zur Verfügung: die YouTube VR-App gibt es im Oculus und SteamVR-Store, die iOS und Android YouTube-App kann die 360° Videos ebenfalls für das Abspielen in VR-Headsets anzeigen.
Da bei YouTube immer ersichtlich ist, wer der Urheber des Videos ist, besteht hier allerdings die Möglichkeit in Kontakt zu treten, um zu Erfragen, ob das Video für die Nutzung im Unterricht bereitgestellt werden könnte.

Auch bei Facebook gibt es 360°-Videos. Wie bei fast allen anderen Videoplattformen sind sie auch hier nicht zum Download freigegeben. Sie sind mit der Facebook App auf den mobilen Geräten, der Facebook 360 App im Oculus Store und im Browser anzusehen. Im VR-Modus mit Bildern für jedes Auge einzeln kann man die Videos allerdings nur in der Facebook 360 App ansehen.

Der Guardian hat eine App veröffentlicht, die exklusiv VR-Inhalte für ihre Leser bereitstellt und speziell für die Nutzung mit dem Google Cardboard entwickelt wurde. Darin zeigen sie von gefilmten 360°-Reportage bis hin zu 360°-Animationsfilmen Geschichten, die speziell für das 360°-Medium entwickelt wurden. \cite{TheGuardian}

Auch die New York Times hat einige Beiträge seit 2016 in 360°. Die Videos können im Browser oder in der New York Times-App angesehen werden. Der Player unterstützt allerdings keine VR-Headsets, die Smartphones können also nur genutzt werden um sich in der Szene umzusehen indem sie in der Hand gehalten und bewegt werden.
Ein Download ist auch hier nicht vorgesehen. \cite{TheTimesNewYorkCompany}

Eine schöne Quelle deutschsprachiger 360°-Videos gibt es vom ZDF, leider sind diese allerdings ebenfalls nicht zum Herunterladen, sondern nur über die ZDF VR-App, die es für Android, iOS und GearVR gibt, oder im Browser betrachtbar ist. \cite{ZDF}

Lizenzierbare 360°-Videos gibt es wie auch Fotos bei 360cities und Airpano. Diese sind aber eher gedacht um in Filme eingebaut zu werden, als einzeln verwendet zu werden. Sie sind vergleichbar zu Archivaufnahmen und damit meist nicht im Bildungsbereich sinnvoll verwendbar. \cite{360Citiess.r.o}\cite{AirPano}

In die gleiche Kategorie fallen dabei die Videos von Videoblocks.com, die ebenfalls einige schöne 360°-Videos haben, die aber keinen wirklichen Inhalt haben, sondern eher als Bildmaterial für die Weiterverarbeitung gedacht sind. Alle drei Anbieter sind zudem nicht besonders günstig. \cite{Storyblocks}

\subsubsection{Erstellung und Bearbeitung von 360°-Videos}
360°-Videos können wie 360°-Fotos entweder mit 360°-Kameras gefilmt werden oder aus Filmen von mehreren Kameras zusammengesetzt werden.
Zum Stitchen von 360°-Filmen gibt es viele Programme. Beispiele dafür sind pixvana oder Stereostitch. 

Um aus den rohen 360°-Video kann dann in Videobearbeitungsprogrammen fertige 360°-Videos produziert werden. Unter Anderem haben die Videoschnitt-Programme Final Cut Pro und Adobe Premiere Pro Werkzeuge zur Bearbeitung von 360°-Videos integriert.

\subsection{3D-Modelle}
3D-Modelle können einen Blick auf Dinge gewähren, die so nicht so einfach angesehen werden. Beispielsweise können damit Molekülstrukturen oder menschliche Organe gezeigt und erforscht werden. Da es bereits eine große Szene für 3D-Modelle gibt, ist es nicht schwierig für die verschiedensten Themengebiete passende Modelle zu finden. Auch für den Bildungsbereich gibt es dort einiges zu finden, was man im Unterricht verwenden kann.

\subsubsection{Quellen für 3D-Modellen}
3D-Modelle sind die einzige Art von Inhalten für 360°, für die es Webseiten gibt, über die man Modelle suchen und herunterladen kann. Dabei kann man nach Formaten, Auflösung, Lizenzen und Preisen filtern, sodass man für alle Anwendungsbereiche passende Modelle finden kann.

Anbieter gibt es dafür viele. Im Laufe der Entstehung dieser Arbeit sind unter Anderen folgende Webseiten genutzt worden:

\begin{itemize}
      \item Turbosquid.com
      \item cgtrader.com
      \item Sketchfab.com
      \item free3d.com
   \end{itemize}

Da für Computerspiele, neuere Animationsfilme und für 3D-Druck bereits seit Jahren 3D-Modelle eingesetzt werden, hat sich hier anders als bei den anderen Inhalten bereits eine Szene gebildet, in der Modelle erstellt und dann verkauft oder verschenkt werden.
   
\subsubsection{Erstellung und Bearbeitung von 3D-Modellen}
Das Erstellen von 3D-Modellen ist anders als bei den anderen Inhalten deutlich aufwändiger. Man muss sich dafür zuerst in eines der 3D-Modellierungsprogramme hineinarbeiten und auch das Erstellen der Modelle ist sehr zeitaufwändig. 

Es gibt viele verschiedene 3D-Modellierungsprogramme, die auch alle etwas unterschiedlich zu benutzen sind:
Beliebte Programme sind Autodesk 3ds Max, Blender, Cinema 4D, Paint3D. Paint3D für Windows 10 bietet einen einfachen Einstieg und begrenzten Funktionsumfang. Blender ist als kostenlos Open-Source-Software weit verbreitet.

Da es viele verschiedene Modellierungsprogramme gibt, gibt es auch viele verschiedene Datenformate für die Speicherung der Modelle. Oft können Modelle aber auch zwischen unterschiedlichen Formaten konvertiert werden. Den Export zu standardisierten OBJ-Dateien, unterstützen viele Programme und es ist damit eine gute Speicherform, um 3D-Modelle zu veröffentlichen.

\subsection{360° Sound/ Spatial Audio}
360° Sound, 3D audio oder auch spatial audio bezeichnet Audioinhalte, die Töne die an verschiedenen Orten um den Kopf platzieren und so eine reale Situation erschaffen. 
Das menschliche Gehör ist in der Lage die Position von Geräuschquellen im Raum zu bestimmen. Ausgehend vom Kopf als Bezugspunkt im Zentrum wird Position einer Quelle mit drei Werten beschrieben. Diese Werde sind die Entfernung vom Kopf, der Azimuth-Winkel und der Elevationswinkel.
Der Azimuth-Winkel beschreibt dabei die horizontale Rotation und der Elevationswinkel die vertikale Rotation. 

Es gibt drei verschiedene Verfahren für spatial audio: Channel-based, object-based und transform domain-based spatial audio.

Momentan ist das Channel-basierte Audio-Format am weitesten verbreitet. Channel-basierte Audio-Inhalte sind playback-orientiert, brauchen als nur minimales Processing beim Abspielen und können direkt an die Lautsprecher gegeben werden, da das Mixing bereits im Vorfeld von einem Sound Engineer vorgenommen wurde. Dabei muss die Standard-Konfiguration der Lautsprecher beachtet werden, damit das spatial audio dann auch wie gewünscht funktioniert. Zudem muss die Anzahl der Kanäle mit dem des Ausgabesystems, also der Anzahl der Lautsprecher, übereinstimmen.
Beispiele für Channel-basierte spatial Audio sind Dolby Atmos und Auro 3D.

Objektbasiertes spatial Audio besteht aus einer Soundszene und einfach Soundobjekten. Diese Soundobjekte haben alle eigene Metadaten in denen unter anderem ihre Position, Lautstärke, Richtung und Gedämpftheit gespeichert sind. Die Soundszene kann dann mit den verschiedensten Abspiel-Setups genutzt werden, da sie keine festgelegte Konfiguration braucht. Dafür ist es deutlich aufwändiger objektbasiertes spatial audio in Realtime zu berechnen. Inzwischen wird objektbasiertes spatial audio immer beliebter, da die aufwändige Berechnung ein zunehmend geringeres Problem darstellt.

Das transform domain-based spatial audio wird auch als Ambisonics bezeichnet und beruht auf den zuvor beschriebenen Lage der Tonquellen. Da die Richtungen nicht direkt in Kanälen abgespeichert werden wie beim Channel-basierten spatial audio, kann auch hier ein variables Abspielsystem verwendet werden. \cite{He2017}

\subsubsection{Quellen für Spatial-Audio-Dateien}
360° Videos auf YouTube können bereits mit spatial Audio hochgeladen werden. Hier könnten die Audiospuren als 3D-Sounddateien heruntergeladen werden. Eine Plattform für den Vertrieb von Spatial-Audio-Dateien gibt es allerdings nicht.

\subsubsection{Erstellung und Bearbeitung von Spatial Audio}
Um Channel-basierten spatial audio content zu erstellen werden acht Kanäle benötigt. Pro Achse (Vorne/Hinten, Links/Rechts, Oben/Unten) wird ein Stereo-Signal erzeugt. Hinzu kommt noch ein Stereo-Signal für den Teil der Töne, die aus keiner Richtung kommen, sondern direkt auf dem Zentrum sitzen. Das sind dann meistens Inhalte wie eine Hintergrundmusik.

Es gibt verschiedene Werkzeuge, um Spatial Audio Dateien zu erzeugen. Viele Audiosoftwareprodukte haben zudem Plugins für 3D Audio. Beispiele für kostenfreie Programme sind die Spatial Workstation von Facebook oder das Ambisonic Toolkit für Reaper.

In der Entwicklung von VR-Anwendungen wird der 3D-Sound meist dadurch erzeugt, dass Sound-Quellen direkt als Objekte in den virtuellen 3D-Raum platziert werden. Die eingebundenen Audiodateien sind dabei dann nur normale Mono- oder Stereodateien. \cite{GoogleLLCspatialaudio} \cite{React360audio}

\subsection{Virtual Reality Spiele und Anwendungen}
Die Kombination aus den vorigen Inhalten ergibt VR-Anwendungen und Spiele. Inzwischen ist die Auswahl an verschiedenen Anwendungen stark gewachsen und tut das auch weiterhin. Der größte Anteil an VR-Programmen sind weiterhin Spiele.

\subsubsection{Quellen für Virtual Reality Anwendungen}
Auf den Plattformen der VR-Geräte gibt es eigene Stores, um Virtual Reality-Anwendungen für die jeweilige Plattform zu finden. 
Bei HTC Vive und HTC Focus ist das der SteamVR Store \cite{ValveCorporation}, bei den Oculus Geräten der Oculus Store \cite{FacebookTechnologiesLLC} und Anwendungen für die PlayStation VR sind im PlayStation Store zu finden. \cite{SonyInteractiveEntertainmentEuropeLimited}

Für webbasierte Inhalte ist es relativ schwierig, Apps zu finden. Eine Möglichkeit, mit sehr begrenzter Auswahl ist auf itch.io nach ``WebVR'' zu filtern. \cite{Itchcorp}

Außerdem gibt es noch wearvr.com, einen unabhängigen Virtual Reality App Store, der Sammlungen für Anwendungen für alle Plattformen hat. \cite{WEARVR}


\subsubsection{Erstellung von Virtual Reality Anwendungen}
Um Virtual Reality Anwendungen zu erstellen, gibt es zwei grundlegend verschiedene Ansätze: 

Zum einen können Softwareentwickler native Anwendungen für die verschiedenen Plattformen oder webbasierte programmieren. Dazu muss die jeweilige Programmiersprache beherrscht werden. Dafür ist der Gestaltungsfreiraum am größten.

Für die webbasierte Entwicklung gibt es Frameworks wie A-Frame, React360 oder Three.js die die Entwicklung erleichtern sollen. Genauere Erläuterungen dazu finden sich in \ref{subsec:webvr}.

Native Applikationen verwenden oft Game Engine zur Entwicklung von VR-Anwendungen, da diese bereits gute Abstraktionen für dreidimensionales Rendering bieten. Beispiele der verbreitetsten Game Engines sind Unity3D und die Unreal Engine. Diese Game Engines können native Applikationen für verschiedene Plattform kompilieren.
Genauere Ausführungen dazu sind wiederum in \ref{subsec:unity} beziehungsweise \ref{subsec:unreal} zu finden.

\bigskip

Die zweite Möglichkeit ist die Nutzung von Tools wie zum Beispiel InstaVR oder Vizor360. Mit diesen Programmen können auch Personen, ohne Programmierkenntnisse einfache 360°-Anwendungen erstellen. Dazu werden einfach 360°-Fotos oder -Videos hochgeladen, mit dem Tool zu einer Tour verbunden. \cite{Vizor2019} \cite{InstaVRInc}
Zusätzlich können auch Textelemente eingefügt werden, um dem Betrachter weitere Informationen zu geben.

\subsection{3D 360°}
Um ein 3D-Bild zu erzeugen müssen zwei leicht verschobene Bilder angezeigt werden, die um den Augenabstand eines Menschen horizontal verschoben sind. Dies erzeug im Gehirn die Tiefenwahrnehmung. Soll also ein 3D-Bild oder Video produziert werden, werden zwei Kameras horizontal versetzt auf ein Stativ montiert und und dadurch ein Bild pro Auge produziert.

Wird ein 360°-Bild erstellt nimmt eine Kamera oder ein Rig mit mehreren Kameras wie in Abbildung \ref{fig:3d-360} links zu sehen ein Bild der kompletten Umgebung auf, in dem sich dann der Betrachter umsehen kann.
Würde man nun also einfach zwei 360°-Bilder um den Augenabstand versetzt aufnehmen hätte man das Problem, dass nur in einem Blickwinkel die Position der Kameras zueinander korrekt ist, sodass ein 3D Bild entsteht. Ist der Betrachter um 90° gedreht, befinden sich die Kameras nicht mehr nebeneinander sondern hintereinander, sodass es keine Parallaxenverschiebung mehr gibt. Deshalb ist dieses Verfahren nicht für 3D-360°-Aufnahmen geeignet.

Für 360°-Fotos können die zwei Kameras mit gleicher Blickrichtung auf ein drehbares Rig montiert werden, sodass jede Kamera aus den Fotos, die sie gemacht hat ein 360° Bild errechnet. Da allerdings durch die Drehung nicht durchgehend aufgenommen werden kann ist diese Technik nur für die Produktion von Fotos geeignet. \cite{bourke2006synthetic}

Grundsätzlich gibt es zwei Ansätze, wie 3D-360°-Videos produziert werden können. Für beide braucht man einen Aufbau von mindestens acht Kameras, um ein brauchbares Bild zu erzeugen.
Die erste Möglichkeit ist ein Rig-Aufbau wie in \ref{fig:3d-360} mittig zu sehen. Es besteht aus acht Kameras, von denen jeweils zwei parallel zueinander ausgerichtet sind und die Paare wiederum jeweils 90° zueinander ausgerichtet sind.
Werden nun jeweils die Bilder der linken Kameras und die der rechten Kameras zusammengestitcht, entsteht ein 360°-Bild pro Auge, das beim Drehen den Abstand der Augen beibehält.
Dieser Aufbau funktioniert gut, wenn der Betrachter in eine der vier Richtungen schaut, in die die Kameras zeigen. Dazwischen verschiebt sich der Augenabstand jedoch, sodass hier Bildbereiche entstehen, die wiederum nicht korrekt in 3D dargestellt werden.
Das Verändern des Augenabstands kann dazu führen, dass die Nutzer schneller Symptome von Cybersickness erleben.

Eine zweite Möglichkeit die acht Kameras zu nutzen ist in \ref{fig:3d-360} rechts dargestellt. Die acht Kameras werden sternförmig mit jeweils einer Rotation von 45° zueinander auf das Rig montiert. Um daraus das korrekte 3D-360°-Bild zu bekommen, werden die Kamerabilder jeweils in einen rechten und einen linken Bildbereich aufgeteilt. Dann werden alle linken Bildbereiche zum Bild für das linke Auge zusammengestitcht und alle rechten Bildbereiche zum Bild für das rechte Auge. Dies minimiert die Veränderung des Augenabstands innerhalb des Bilds. \cite{Peleg2001}

\begin{figure}[h]
  \includegraphics[width=\linewidth]{images/3d-360.eps}
  \caption{Aufbau von 360° Rigs.}
  \label{fig:3d-360}
\end{figure}


\subsection{360°-Inhalte in React360}
Die VR-App innerhalb von VRClassroom wurde mit React360 entwickelt. Nachfolgend wird darauf eingegangen, wie 360°-Inhalte in React360 eingebunden werden können.

\subsubsection{Fotos und Videos}
React360 ermöglicht sowohl die Nutzung von 180°- also auch 360°-Panoramas für Videos und für Fotos. Ebenfalls für beide Medientypen gilt, dass React360 2D- und 3D-Bilder verarbeiten kann.

Die Fotopanoramas in 360° müssen entweder in equirectangularer Projektion oder im 1x6 Cubemap-Format vorliegen. Dabei besteht das Gesamtbild aus sechs unverzerrten Einzelbildern, die in der Reihenfolge rechts, links, oben, unten, vorne, hinten angeordnet sind.
In der equirectangularen Projektion kann das Bild auch 3D sein. Dazu müssen die zwei Bilder übereinander angeordnet sein. Diese Anordnung ist als ``top/bottom''-Layout bekannt. Cubemap-Panoramen können nur als 2D-Bild verarbeitet werden.

180°-Panoramen sind nur equirectangular möglich. Das Stereobild kann hierbei neben dem ``top/bottom''-Layout auch in ``left/right'', also nebeneinander vorliegen.

Bei den möglichen Formaten für Videos verhält es sich vergleichbar zu den Fotos: Equirectangulare Panoramen können 2D oder 3D sein. Sind sie 360°-Panoramen im ``top/bottom''-Layout, sind sie 180° im ``left/right''-Layout. 360°-Panoramen können wiederum auch im Cubemap-Format vorliegen.

Wenn ein 3D-Foto (Stereobild) eingebunden werden soll, muss der Parameter ``format'' mit dem das Bild geladen wird auf einen der folgenden Werte gesetzt werden:

\begin{itemize}
\item \textbf{2D}: [Defaultwert] Für Monobilder
\item \textbf{3DTB}: Stereobilder, bei denen das linke Bild oben ist und das rechte unten
\item \textbf{3DBT}: Stereobilder, bei denen das linke Bild unten ist und das rechte oben
\item \textbf{3DLR}: Stereobilder, bei denen das linke Bild links ist und das rechte rechts
\end{itemize}

Fotos und Videos können in die React360-Applikation eingebettet sein oder dynamisch über das Internet geladen werden. Sie werden dann als Hintergrundbild beziehungsweise -video der Szene gesetzt.

Um Videos abzuspielen muss zuerst ein Player erstellt werden und der Pfad zur Video-Datei übergeben werden. Danach können auf dem Player die Steuerungsfunktionen zu den Videos aufgerufen werden. \cite{React360video}

\subsubsection{Sounds}
In React360 gibt es zwei grundlegende Arten Sounds einzubinden:

Zum Einen können Audiodateien als Hintergrund-Audio oder Soundeffekt eingebunden werden. Hier wird auf dem AudioModule die Funktion playEnvironmental() mit dem Pfad und der gewünschten Lautstärke übergeben. Die Audiodateien werden dann als Loop abgespielt und erst beendet, wenn stopEnvironmental() aufgerufen wird. Soundeffekte werde mit .playOnShot() einmalig abgespielt. Diese Sounds haben keine räumliche Position.

Zum Anderen können Sounds in der 360°-Szene an einem bestimmten Ort installiert werden. Dazu muss die .createAudio()-Funktion aufgerufen werden. Sie erwartet den Pfad zur Audiodatei und ``is3d'' muss auf true gesetzt werden. Im Aufruf der play-Funktion wird dann die gewünschte Position im dreidimensionalen Raum übergeben, an der der Sound abgespielt werden soll. \cite{React360audio}

\subsubsection{3D-Modelle}
3D-Modelle, in React360 als ``Entities'' bezeichnet, können an eine beliebige 3D-Position in der Szene gerendert werden. Mit einem transform-Attribut können sie dann noch skaliert, rotiert und verschoben werden.

3D-Modelle können als gltf2-Dateien (Dateiendung .gltf oder .glb) oder als OBJ-Dateien vorliegen. Bei OBJ-Modellen muss auch noch die zugehörige MTL-Datei übergeben werden, die die Materialdefinitionen enthält. \cite{React360models}

%\_____________________________________________________________________

\cleardoublepage
\section{Software Entwicklung von VR-Systemen}
Wie bereits erwähnt gibt es verschiedene Software-Produkte, die es auch dem Laien ermöglichen simple VR-Anwendungen wie eine 360°-Fotostrecke zu erstellen. Sollen allerdings komplexere VR-Anwendungen entstehen, müssen VR-Apps programmiert werden. Dafür gibt es zwei grundlegend verschiedene Möglichkeiten. Entweder können native VR-Apps für die verschiedenen VR-Plattformen entwickelt werden oder auf WebVR gesetzt werden, das im Browser läuft und somit mit allen Plattformen kompatibel ist.

\subsection{Native VR-Applikationen}
Um VR-Anwendungen, die nativ auf den VR-Headsets laufen, zu entwickeln gibt es momentan nur zwei verschiedene Möglichkeiten. Das sind die Game-Engines Unity und Unreal. Wie aus dem Namen schon erkennbar, wurden sie ursprünglich entworfen, um normale Computer und Konsolenspiele zu programmieren. Sie wurden aber weiterentwickelt, sodass sie sich inzwischen auch sehr gut zur Entwicklung von VR-Anwendungen eignen.

Ingesamt sind sich die beiden Engines relativ ähnlich, verwendet man Unity, muss man C\# programmieren und die Unreal Engine verwendet so genannte Blueprints, um einfachere Sachen visuell programmieren zu können mit einem Editor, in dem Nodes mit Links verbunden werden können um Funktionen zu nutzen. Darunterliegend wird in der Unreal Engine mit C++ gearbeitet.

\subsubsection{Unity}
\label{subsec:unity}
Unity ist eine Game-Engine mit der zwei- und dreidimensionale Anwendungen entwickelt werden können. Den Unity-Editor gibt es für Windows und MacOS. Die Programmiersprache von Unity ist C\#.
Die in Unity entwickelten Anwendungen können für viele verschiedene Plattformen kompiliert werden: Zu den Zielplattformen gehören Playstation, Xbox, Wii, Nintendo Switch, tvOS, Hololens, iOS, Android, sowie Oculus und SteamVR (für die HTC-VR-Headsets).

Unity ist für private und selbstständige Entwickler kostenlos, wenn sie unter 100.000\$ pro Jahr mit ihren Anwendungen verdienen kostenlos und für Unternehmen liegt der Preis bei 125\$ pro Entwickler.

Neben der Anwendungsentwicklung können mit Unity auch 360°-Filme produziert werden. Dazu können 360°-Clips hineingeladen werden, Overlays, Objekte oder Animationen eingefügt werden und sogar Logik für Nutzerinteraktion eingebunden werden. Unity unterstützt dafür auch die Nutzung von Ambisonic Audioquellen. \cite{UnityTechnologies}

\subsubsection{Unreal Engine}
\label{subsec:unreal}
Ebenso wie Unity ist die Unreal Engine für die Entwicklung von Computer und Konsolenspielen entwickelt worden. Der Unreal Editor ist für Linux, MacOS und Windows verfügbar und die entwickelten Anwendungen können zu Windows PC, PlayStation 4, Xbox One, Mac OS X, iOS, Android, AR, VR, Linux, SteamOS, Nintendo Switch und HTML5 deployed werden.

Die Unreal Engine ist erstmal komplett kostenfrei zu nutzen. Wird mit einer Anwendung Geld verdient, verlangt Epic Games allerdings einen Anteil von 5\% des Gewinnes. Ziel dieser Preisgestaltung ist es, möglichst vielen Leuten Zugang zur Engine zu geben und besonders auch für Schüler und Studenten die Möglichkeit zu bieten die Engine kennen zu lernen und Erfahrungen zu sammeln.

Der Code wird in in der Unreal Engine in C++ geschrieben, einige Teile können aber über die Nutzung der Blueprints generiert werden ohne direkt Code schreiben zu müssen. Blueprints sind ein Tool, um visuell Skripte zu erzeugen und Design anzupassen. Die Blueprints sind in einem Node-Link Editor realisiert.

Für die Entwicklung von VR Anwendungen bietet die Unreal Engine die Möglichkeit in VR an den VR-Szenen zu arbeiten. Dafür ist der komplette Unreal Editor in VR für die Oculus Rift und HTC Vive umgesetzt und macht vollen Nutzen der Interaktionsmöglichkeiten der Controller der VR-Headsets.

\subsection{WebVR - VR im Browser}
\label{subsec:webvr}

WebVR ist eine API, die es ermöglicht VR-Geräte und deren Sensoren über Webapps zu verwenden.
Die ursprüngliche WebVR API wird zukünftig durch die WebXR API ersetzt werden, die neben Virtual Reality-Geräten auch Augmented Reality- und Mixed Reality-Geräte unterstützt. \cite{WebVRimmersive}
Da sich die WebXR-API noch in der Entwicklung befindet, wird in dieser Arbeit die WebVR-API verwendet. 

Die WebVR-API erlaubt Zugriff auf mit dem Browser verbundene VR-Displays. Eine Webseite oder Webapp kann so Inhalte auf VR-Headsets darstellen. Die WebVR-API übernimmt dann die korrekte, auf das entsprechende Headset abgestimmte Darstellung der Inhalte. Das ermöglicht Webanwendungen unabhängig vom einzelnen Headset des Nutzers zu entwickeln und Darstellungsdetails dem Browser und Betriebssystem zu überlassen.

Die WebVR-API ist in folgenden Browsern unterstützt: \cite{Deveria}
\begin{itemize}
\item Microsoft Edge: ab Version 15
\item Google Chrome Desktop: ab Version 57, hinter einem Flag
\item Google Chrome für Android: ab Version 71
\item Samsung Internet: für GearVR-Geräte
\item Mozilla Firefox: ab Version 55
\item Oculus Browser: ja
\end{itemize}

In Browsern, die die WebVR-API nicht implementieren, kann die Unterstützung mit einem sogenannten Polyfill nachgerüstet werden. Ein Polyfill ist ein JavaScript, das von einer Webseite ausgeführt wird um fehlende APIs in Browsern dynamisch nachzurüsten. Dabei ahmen Polyfills die Funktion der API nach, damit sich der Browser so verhält, als wäre die API verfügbar. Oft haben Polyfills dadurch eine schlechtere Performance als die native Implementierung der Browser-Hersteller. Für WebVR gibt es ein offizielles Polyfill der Immersive Web Working Group des W3Cs, die auch für die Entwicklung des Standards zuständig ist. In Browsern, die die WebVR-API bereits unterstützen, wird das Polyfill ignoriert und die native Implementierung verwendet. \cite{WebVRPolyfill}

\subsubsection{Verwendung der WebVR-API} 
Per JavaScript kann von der Webseite mit navigator.getVRDisplays() die Liste aller verbundenen VR-Displays abgefragt werden. Im Regelfall kann davon ausgegangen werden, das an einem Gerät nur ein VR-Display verbunden ist. Mit display.requestPresent() kann dann angefragt werden Inhalte auf diesem Display darzustellen. Die Inhalte werden dann in display.requestAnimationFrame gerendert. Diese Funktion wird mit der Bildwiederholrate des verbundenen VR-Displays aufgerufen.

Die WebVR-API liefert Informationen über das Display, etwa ob dieses positional tracking unterstützt. Ebenso können Informationen über verbundene VR-Controller abgefragt werden und die Eingaben dieser Controller verarbeitet werden. Dazu wurde die schon existierende Gamepad-API in Browsern verwendet. Hier können Button-, Touch- und Bewegungs-Eingaben behandelt werden.

Die direkte Verwendung der WebVR-API ist allerdings für die Entwicklung von VR-Anwendungen umständlich. Deshalb werden üblicherweise Frameworks verwendet, die die Darstellung mittels WebVR-API übernehmen, sodass der Entwickler sich nicht um das eigentlich Rendern der Szene kümmern muss. Zwei verbreitete VR-Frameworks werden im Nachfolgenden behandelt.

\subsubsection{A-Frame}
A-Frame ist ein Framework, das ursprünglich von Mozilla entwickelt wurde, mit dem Virtual Reality und Augmented Reality Apps entwickelt werden können.

A-Frame basiert auf HTML, ist aber trotzdem ein Komponenten-basiertes Framework, das eine erweiterbare deklarative Struktur zur Nutzung von Three.js bietet. Es kommt bereits mit vielen Core Komponenten, diese können aber selbstverständlich durch eigene Komponenten erweitert werden.
Es kann VR-Apps mit bis zu 90 FPS bauen und kommt mit einem visuellen 3D Inspector, der es einfach macht die VR-Szene zu überblicken.

Mit A-Frame können auch Apps entwickelt werden, die positional Tracking nutzen, also sechs Freiheitsgrade haben. Zudem werden die Controller der VR-Headsets unterstützt. \cite{Mozilla}

\subsubsection{React360}
React360 nutzt ebenso wie A-Frame Three.js für einen Teil des Renderings. React360 ist wie A-Frame komponenten-basiert und kommt mit einigen Core-Components, die durch eigene Komponenten erweitert werden können.
Um eine fertige React360-App zu bundeln wird Node.js genutzt.

React360 bietet mit der React 360 CLI Die Möglichkeit mit dem ``react-360 init ProjectName'' Kommando im Terminal den initialen Code für das VR-Projekt zu generieren. Es wird das ProjectName-Projekt erzeugt, das direkt in der nötigen Software-Struktur mit allen Libraries erstellt wird. Auch die Development Server werden hier direkt mitgeliefert.

Eine App besteht grundsätzlich erstmal aus drei Dateien. Das sind die index.html-Datei, in der der Javascript-Code eingebunden wird, die index.js-Datei, in der der eigentliche React360-Code ist, und die client.js, die die Runtime für die React360 App bildet. 
Die HTML-Datei wird nur dazu genutzt, den JavaScript-Code einzubinden. 
In der client.js wird der React360 Code in den DOM eingebunden. Hier werden auch die Komponenten auf Locations oder Surfaces gemounted.

React360 baut auf React Native auf. Es werden zum Beispiel das Konzept der Stylesheets übernommen, die ein einfaches Styling der Komponenten erlauben, das dem Styling bei HTML sehr ähnlich sieht, und nutzt zum App Bundling ebenfalls den Metro Bundler. \cite{FacebookTechnologiesLLCreact360}


\subsubsection{Three.js}
Three.js ist ein JavaScript Framework mit dem 3D Szene erstellt werden können. Um Objekte in die Szene zu setzen müssen sie dem Document Object Tree hinzugefügt werden. Auch die Kamera muss als Objekt manuell hinzugefügt werden und ein Field of View, das Seitenverhältnis und die ``near'' und ``far''-Werte mit übergeben werden. Diese Werte bezeichnen den Bereich der z-Werte, der gerendert wird. Liegt ein Objekt außerhalb dieses Intervalls, ist es nicht in der Szene zu sehen.

Three.js wird standardmäßig mit dem WebGL-Renderer gerendert, bietet aber auch andere Renderer als Fallback für inkompatible Browser.

Um VR-Apps mit Three.js zu bauen, muss zuerst WebVR eingebunden werden. Dann kann ein WebVR Button hinzugefügt werden, der dann die VR-Ansicht startet. Zudem muss der Animations-Loop angepasst werden. Für VR-Apps muss statt der ``requestAnimationFrame''-Funktion die ``setAnimationLoop''-Funktion genutzt werden.

Soll ein Objekt wie zum Beispiel ein Würfel in die Szene gesetzt werden, sind mehrere Schritte erforderlich. Zuerst muss mit THREE.BoxGeometry(1,1,1) eine Rechteckige Form und mit THREE.MeshBasicMaterial({color: 0x00ff00}) ein Material erzeugt werden. Danach wird der Würfel erzeugt in dem das geometrische Objekt und das Material mit THREE.Mesh(geometry, material) zusammengefügt werden. Zuletzt muss der erzeugte Würfel der Szene hinzugefügt werden. Damit ist Three.js eine sehr low-levelige Möglichkeit VR-Apps zu entwickeln. Dadurch stehen viele Möglichkeiten offen, es ist jedoch deutlich aufwändiger einfache VR-Szenen zu bauen. Andere VR-Frameworks wie A-Frame oder React360 nutzen im Hintergrund Three.js und bieten den Entwicklern einfache Funktionen, um Objekte in die VR-Szene zu setzen. \cite{Three.js}

\subsubsection{PlayCanvas}

Eine spannende Möglichkeit VR-Applikationen zu entwickeln ist die Nutzung von PlayCanvas. PlayCanvas ist eine Web-basierte Game-Engine, in der mit HTML und WebGL entwickelt werden kann. Das besondere ist, dass in Real-Time kollaborativ gearbeitet werden kann und da es keine Kompilierzeit gibt die Veränderungen auch direkt auf den Geräten angezeigt werden. PlayCanvas ist WebVR kompatibel und ist damit sehr gut geeignet VR-Anwendungen für Smartphone-gestützte VR-Headsets zu entwickeln. \cite{PlayCanvasLtd}

\subsection{Vergleich und Auswahl}
Der große Vorteil von WebVR gegenüber nativer Apps ist, dass keine App vor der Nutzung installiert werden muss. Außerdem muss eine webbasierte Anwendung nicht für einzelne Zielplattformen erzeugt werden sondern funktioniert plattformübergreifend. Die bessere Performance nativer Apps spielt bei VRClassroom eine untergeordnete Rolle, da es sich nicht um ein grafisch aufwändiges Spiel handelt, sondern die Anzeige einzelner 3D-Modelle oder 360°-Fotos und -Videos.
Daher wurde die Entscheidung getroffen die Schüler-App von VRClassroom webbasiert umzusetzen.

React360 und A-Frame sind beides Frameworks, die die Ansprüche für die Schüler-App vom VRClassroom erfüllen. 
Die Entscheidung für React360 wurde getroffen da React als View-Framework sowohl für die VR-Anwendung (React360) als auch für die Lehrer-App (React.js) verwendet werden kann. Die beiden Varianten des React-Frameworks unterscheiden sich für die beiden Plattformen nur gering. Während A-Frame nur die eigentlich Darstellung der Szene übernimmt, bietet React auch ein Daten- und State-Management, das hilft komplexe Anwendungen sauber in Komponenten zu zerlegen.

%\_____________________________________________________________________



\cleardoublepage
\section[VRClassroom]{Software Projekt: VRClassroom}
Aufbauend auf den Erkenntnissen dieser Arbeit wurde im Rahmen dieser Arbeit VRClassroom entwickelt. VRClassroom ist ein System, das es Lehrkräften ermöglichen soll 360°-Fotos, -Videos und 3D-Modelle im Unterricht zeigen zu können. Die Schüler können die Inhalte synchronisiert auf VR-Headsets ansehen. Die Lehrkraft kann die Anzeige vom Computer aus auf allen Headsets steuern und trotzdem den Überblick in der Klasse behalten. 

VRClassroom bietet dafür die Lehrer-App, die auf einem Rechner läuft und die Schüler-App, die über den Browser der mobilen Geräte beziehungsweise VR-Headsets erreicht werden kann.

Die Lehrer-App ist eine Electron-App und damit für Windows, macOS und Linux verwendbar. Sie verwendet React.js für das Interface und startet den Server von dem aus die Schüler-App geladen wird. Die Schüler-App ist eine React360-Applikation. Der Code von VRClassroom ist also komplett in JavaScript geschrieben, nur unterschieden durch die verschiedenen Frameworks Node.js, React.js, React360. Im Anschluss an diese Arbeit wird VRClassroom unter einer open-source-Lizenz veröffentlicht. Entwickler können dadurch alle Teile der Codebasis verstehen ohne sich in mehrere Programmiersprachen einarbeiten zu müssen.

Abbildung \ref{fig:electron} zeigt die Struktur von Electron-Apps.

\begin{figure}[ht]
  \centering
  \includegraphics[width=\textwidth]{images/electron.eps}
 \caption{Aufbau von Electron-Applikationen.}
  \label{fig:electron}
\end{figure}

\subsection{Nutzungsszenario und Anwendungsfokus}
VRClassroom wurde speziell für die Nutzung in der Schule entwickelt.
Dabei kann die Lehrkraft das VR-Erlebnis für die SchülerInnen führen und ihnen so komplexe 3-dimensionale Inhalte besser vermitteln und eine spannende neue Art des Lernens zu erleben. 
Außerdem wurde Funktionen wie etwa die Seitenleiste mit den verbundenen Geräten entwickelt, die es der Lehrkraft erleichtern sollen VRClassroom in einer Schulklasse einzusetzen und sicher zu stellen, dass alle Geräte verbunden sind und die Inhalte angezeigt werden.

Da viele Schulen noch überhaupt keine Ausrüstung an VR-Headsets haben, lag der Fokus besonders auf der Nutzung des VRClassroom-Systems mit einem Smartphone in einem Google Cardboard. Allerdings wurde es bewusst so entwickelt, dass auch mit professionelleren VR-Headsets das System problemlos weiter genutzt werden kann. So wird den Schulen ermöglicht mit einer sehr geringen Investition zu testen, ob es für sie in Frage kommt. Zu einem späteren Zeitpunkt kann dann auf ein elaborierteres Hardware-System gewechselt werden ohne auf neue Software umsteigen zu müssen.

Weitere Einsatzszenarien für VRClassroom könnten Besprechungen im Arbeitsumfeld sein, bei denen es sich um plastische Inhalte handelt wie zum Beispiel Architekturbüros, Designagenturen oder Landschaftsgärtner. Mit einem 360°-Foto oder -Video könnte der Ist-Zustand besprochen werden und anschließend die Entwürfe in 3D-Modellen vorgeführt werden. Dadurch könnten sich Kunden besser in die Entwürfe hineinversetzen und bewusster entscheiden, was sie letztendlich haben möchten.

\subsection{Anwender}
Wie bereits weiter oben beschrieben wurde VRClassroom speziell dafür entwickelt im Schulunterricht verwendet zu werden. Damit sind die Hauptanwendergruppe Lehrkräfte und Schülerinnen und Schüler.
Die Schülerinnen und Schüler als Anwender der Schüler-App können dabei im Alter von Grundschulkindern bis hin zu Erwachsenen reichen, die Spanne ist damit also sehr weit und somit ist auch die Vorerfahrung mit VR-Headsets und die individuelle Technikaffinität äußerst unterschiedlich.
Den Schülern soll es also möglichst leicht gemacht werden die Schüler-App von VRClassroom zu verwenden. Da wenige Schulen über VR-Headsets verfügen, die für eine gesamte Klasse ausreichen, muss also dafür Sorge getragen werden, dass die Schüler-App auf allen Geräten, aber besonders auf Smartphone-gestützten VR-Headsets läuft.

Bei den Lehrkräfte verhält es sich ähnlich: Sie kommen aus verschiedensten Fachrichtungen, mit unterschiedlicher Technikaffinität und Erfahrung im VR-Bereich, außerdem hat die Umfrage gezeigt, dass mehr als die Hälfte teilnehmenden Lehrkräfte selbst noch nie ein VR-Headset ausprobiert haben. 

Dementsprechend lag der Fokus bei der Entwicklung besonders darauf, dass die Bedienung intuitiv und einfach ist. Außerdem soll den Lehrkräften mit der Anzeige der verbundenen Geräte eine Möglichkeit an die Hand gegeben werden, zu sehen, ob die Schüler-Geräte verbunden sind und alle Inhalte dargestellt werden. Bewusst wurde das System so entworfen, dass die Lehrkräfte keine VR-Brille tragen, damit sie einen Überblick im Klassenzimmer behalten können und trotzdem sehen können, was die Schüler angezeigt bekommen.

\subsection{Lehrer-Applikation}
Die Lehrer-Applikation besteht aus einer Electron-App, die in zwei logische Teile zerlegt ist. Das ist zum einen der main-Prozess, dieser startet die Applikation und läuft in node.js ohne sichtbares UI. Hier können weitere Hintergrundprozess gestartet werden. Zum Anderen gibt es den renderer-Prozess. Dieser verwendet Chromium und ist für die Darstellung des UIs verantwortlich, das die Lehrkraft letztendlich auf ihrem Bildschirm sieht.

Die Lehrer-Applikation startet aus dem main-Prozess einen HTTP-Server. Über diesen wird die student-App und die Inhalte im lokalen Netzwerk zugänglich gemacht. Aus dem main-Prozess wird der renderer-Prozess gestartet, der das UI der teacher-App anzeigt. In diesem UI wird die student-App, die als iFrame eingebunden. So kann die Lehrkraft sehen was auf den Geräten der Schülern läuft.

Die teacher-App enthält außerdem noch den QR-Code Generator, der in einem zweiten renderer-Prozess geladen wird.

Um eine Liste der verbundenen Geräte zu halten und Veränderungen der Inhalte auf die Schüler-Geräte zu synchronisieren, wird ein WebSocket-Server gestartet, mit dem sich alle Schüler-Geräte verbinden. 

\subsubsection{Verbundene Geräte}
Wie in der Abbildung zu sehen hält die teacher-App eine Liste mit allen verbundenen Geräte dieser Session. Sind die Geräte gerade aktiv, werden sie mit einem grünen Icon dargestellt, sind sie inaktiv, mit einem roten. Da das Laden der Inhalte je nach Größe auch über das lokale Netzwerk etwas dauern kann, wird der Indikator in gelb dargestellt, währen das Schüler-Gerät die Inhalte noch herunterlädt. Sobald der Inhalt bereit zum abspielen ist, wird der Indikator grün.

Das soll der Lehrkraft erleichtern zu überprüfen, ob die Schüler den gezeigten Stoff verfolgen oder sich anderweitig beschäftigen.

Haben die Schüler bereits einen Namen eingegeben, wird dieser in der Liste angezeigt. Ist dies nicht der Fall wird aus dem user-agent der WebSocket-Verbindung versucht möglichst genau zu schließen, um welches Gerät es sich handelt, sodass der Lehrer zumindest einschränken kann, um welchen Schüler bzw. welche Schülerin es sich handeln könnte. 

\bigskip

Ein User-Agent kann zum Beispiel wie folgt aussehen: 
\begin{framed}
Mozilla/5.0 (iPhone; CPU iPhone OS 5\_0 like Mac OS X) AppleWebKit/534.46 (KHTML, like Gecko) Version/5.1 Mobile/9A334 Safari/7534.48.3
\end{framed}

Daraus lässt sich schließen, dass es sich um eine iPhone handelt, welches Safari benutzt, um die Schüler-App zu laden. Die Oculus Geräte hingegen geben in ihrem User-Agent an, den Oculus Browser zu verwenden und sind so auch gut von den anderen verbundenen Geräten zu unterscheiden. Da aber hier nicht ersichtlich wird welches Oculus-Gerät es genau ist wird nur ``Oculus device'' angegeben und nicht genauer spezifiziert, ob es sich dabei um eine Go, Quest oder Rift handelt.

\subsubsection{Hineinladen von Inhalten}
Um Inhalte in VRClassroom hineinzuladen kann in der Menüleiste über ``Datei'' und ``Öffnen...'' die gewünschte Datei ausgewählt werden. Wird eine Datei geöffnet, wird sie sofort in die App geladen und auf allen verbundenen Geräten angezeigt.
Wurde die VRClassroom App zum ersten Mal installiert wird ein Ordner ``.vrclassroom'' im Home-verzeichnis angelegt, in den die verwendeten Dateien hineingeladen werden. Von dort aus werden sie dann auch für den Zugriff durch die verbundenen Schüler-Geräte über den HTTP-Server freigegeben.

Bereits früher verwendete Dateien sind unter ``Datei'' und ``Zuletzt geöffnet'' zu finden und können so bequem wieder verwendet werden ohne sie lange im Dateisystem suchen zu müssen. Neu geöffnete Dateien werden beim Öffnen in den ``.vrclassroom''-Ordner im Dateisystem kopiert und sind ab diesem Moment auch in der Liste der bereits geöffneten Dateien zu finden.

Drittens können auch noch die URLs von Google StreeView-Panoramen eingegeben werden, sodass dann allen verbundenen Geräten das verlinke Panorama angezeigt wird. Dazu muss ``Datei'' und ``StreetView...'' ausgewählt werden und in das Pop-Up die URL zum gewünschten Panorama eingegeben werden. Handelt es sich um eine korrekte URL wird dann das StreetView Bild geladen und angezeigt wie ein normales 360°-Foto. Für die Nutzer der Schüler-App ist kein Unterschied zu normalen 360°-Fotos erkennbar. Das StreetView-Bild wird dabei aus dem Internet auf den Lehrer-Computer heruntergeladen und vor dort and die Schüler-Geräte verteilt.
 
\subsubsection{Bedienelemente}
Wie in Abbildung \ref{fig:controls} zu sehen sind die Bedienelemente, mit denen Aktionen des aktuell geladenen Inhalts ausgelöst werden können durch einen Overlay über dem iFrame der React360-App dargestellt. 

Für jeden der drei Typen an Inhalten, 360°-Fotos, -Videos und 3D-Modelle unterscheiden sich die Bedienelemente und Funktionen, das Setzen einer Markierung ist allerdings bei allen Medientypen möglich.

\begin{figure}
  \includegraphics[width=0.9\linewidth]{images/controls-overlay.png}
  \caption{Die Bedienelemente als Overlay über den iFrame der React360-App.}
  \label{fig:controls}
\end{figure}

\paragraph{Marker}
\label{subsec:marker}



\begin{figure}[h]
\centering
  \includegraphics[width=\linewidth]{images/marker.png}
  \caption{Die leuchtende Farbe und das Drehen der Marker erhöht die Erkennbarkeit.}
  \label{fig:marker}
\end{figure}

Soll ein Marker gesetzt werden, muss zuerst in den Marker-Modus gewechselt werden. Dazu klickt man auf ``Marker setzen''. Danach kann an beliebiger Stelle in der Szene geklickt werden um einen Marker zu setzen.

Um die 3D-Koordinaten des Markers zu bestimmen wird mit Hilfe der 2D-Klick-Koordinaten, der Fenstergröße, der Kameraposition und -richtung ein Strahl berechnet, der durch die Kameraposition und den Cursors geht. Der Marker muss auf diesem Strahl positioniert werden. Jedoch fehlt die Tiefeninformation.
 
Bei 360°-Fotos und -Videos wird die Entfernung des Markers zur Kamera auf einen festen Wert gesetzt, der so gewählt wurde, dass er gerade noch im Zylinder der Welt liegt.

Um Markierungen auf 3D-Modellen setzen zu können muss ein Schnittpunkt des 3D-Modells mit dem virtuellen Strahl berechnet werden. Dazu wird getestet mit welchen (Teil-)Objekten der Szene sich der Strahl schneidet. Ist der Schnittpunkt des Strahls mit dem 3D-Modell berechnet, wird derjenige Schnittpunkt als Koordinate verwendet, der am nächsten zur Kamera ist. An dieser Stelle wird dann der Marker platziert. Zudem werden die Marker invers zur Nähe zur Kameraposition skaliert, sodass sie immer in der gleichen Größe dargestellt werden, egal wo am Modell sie gesetzt sind.

Das verwendete Modell erinnert wie in Abbildung \ref{fig:marker} zu sehen an Stecknadeln, sodass Nutzern die Bedeutung direkt verständlich ist. Um die Markierungen leichter erkennbar zu machen sind sie in einer leuchtenden Farbe und drehen sich, sodass die Aufmerksamkeit direkt auf die markierte Stelle gelenkt wird.

\paragraph{Fotos}
Ist ein Foto geladen beinhaltet die Kontrollzeile nur den Dateinamen des 360°-Fotos und die Buttons zum Setzen von Markierungen, die wie bereits beschrieben bei allen Medientypen gegeben sind. Abbildung \ref{fig:photocontrols} zeigt die Kontrollleiste für 360°-Fotos.

\begin{figure}[h]
  \includegraphics[width=\linewidth]{images/photocontrols.png}
  \caption{Die Kontrollleiste für 360°-Photos.}
  \label{fig:photocontrols}
\end{figure}


\paragraph{Videos}
Wird ein Video gezeigt, hat die Lehrkraft mehrere unterschiedliche Funktionen in der Kontrollleiste: 
Wie in Abbildung \ref{fig:videocontrols} zu sehen ist links ein kombinierter Play/Pause-Button, der für alle verbundenen Geräte synchronisiert das Abspielen beziehungsweise Pausieren des Videos auslöst, daneben die aktuelle Abspielzeit, gefolgt von einem Slider, der grafisch die aktuelle Position im Video darstellt. Außerdem kann mit dem Slider zu anderen Zeitpunkten im Video gesprungen werden.
Rechts daneben wird die Gesamtdauer des Videos angezeigt. Der letzte Video-spezifische Button ist der Sound-Button, der im aktiven Zustand auf allen Geräten den Ton des Videos aktiviert. Der Sound-Button ist standardmäßig deaktiviert. 

Zudem sind wiederum die Buttons für das Setzen von Markierungen vorhanden. Sie sind während dem Abspielen des Videos deaktiviert und können nur genutzt werden, wenn das Video pausiert ist. Wird das Video dann wieder weiter abgespielt, werden alle gesetzten Markierungen automatisch entfernt.

\begin{figure}[h]
  \includegraphics[width=\linewidth]{images/videocontrols.png}
  \caption{Die Kontrollleiste für 360°-Videos.}
  \label{fig:videocontrols}
\end{figure}

\paragraph{3D-Modelle}
Wie in Abbildung \ref{fig:modelcontrols} zu sehen ist auch in der Kontrollleiste bei 3D-Modellen ein Slider vorhanden. Dieser kann sowohl zum Drehen des Modells als auch zum Skalieren benutzt werden. Dazu sind links vom Slider die Buttons ``Drehen'' und ``Skalieren'' mit denen zwischen den zwei Funktionalitäten des Sliders gewechselt werden kann. Der aktive Modus wird durch die blaue Farbe dargestellt.

Auch auf 3D-Modellen können Markierungen gesetzt werden. Dabei ist zu beachten, dass nur auf dem Modell eine Markierung gesetzt werden kann. Wird außerhalb des 3D-Modells geklickt, wird keine Markierung gesetzt.

\begin{figure}[h]
  \includegraphics[width=\linewidth]{images/modelcontrols.png}
  \caption{Die Kontrollleiste für 3D-Modelle.}
  \label{fig:modelcontrols}
\end{figure}
 
\subsection{Kommunikation zwischen Lehrer-App und Schüler-App}
\begin{figure}[ht]
  \includegraphics[width=\textwidth]{images/architecture1.eps}
 \caption{Aufbau des VRClassroom Systems.}
  \label{fig:architecture}
\end{figure}

Wie in Abbildung \ref{fig:architecture} zu erkennen kommunizieren die Lehrer-App und die Schüler-App über eine WebSocket-Verbindung. Beim Start der VRClassroom-App auf dem Computer der Lehrkraft werden sowohl ein WebSocket-Server als auch ein HTTP-Server (Asset-Server) gestartet. Der HTTP-Server liefert die Inhalte so wie die Schüler-App selbst aus. Der WebSocket-Server wird zur Echtzeit-Kommunikation zwischen Lehrer- und Schüler-App verwendet.

Wenn die Schüler-App auf einem Gerät geladen wird, stellt sie die Verbindung zum WebSocket-Server her und meldet sich damit an. Als Antwort schickt die Lehrer-App den aktuellen Zustand. So wird sichergestellt, dass die Schüler-App alle Informationen hat, die verpasst hat, solange sie noch nicht verbunden war.

Bei jeder inhaltlichen Veränderung in der Lehrer-App wird eine Nachricht an alle verbundenen Geräte geschickt, sodass diese dann die Anzeige aktualisieren können.
Damit alle Geräte zu jedem Zeitpunkt über die gleichen Informationen verfügen und auch Geräte, die nach Beginn der Session hinzustoßen alle Information zur Anzeige haben, enthält jede Nachricht alle Informationen zum aktuellen Zustand der App.

Eine Nachricht ist immer wie folgt aufgebaut:

\begin{lstlisting}
{ 
mediatype: 'video',
  url: 'myUrl',
  markers: [(3,5,3)],
  playing: false,
  playbackPosition: 0,
  rotation: 0,
  scaleFactor: 1,
  muted: true, 
  }
\end{lstlisting}

Im Beispiel ist ein Videofile in die VRClassroom-App geladen worden, dass momentan pausiert ist und der Ton deaktiviert ist. Die aktuelle Abspielposition ist 00:00 und es ist ein Marker an der Position (3,5,3) gesetzt.

Das Feld ``mediatype'' kann die Werte ``photo'', ``video'' und ``model'' enthalten und ist dafür zuständig, dass die Schüler-App weiß, welcher Medientyp angezeigt wird. 
Im URL-Feld enthält die URL des aktuell anzuzeigenden Inhalts. Dies sind lokale HTTP-URLs, die vom Asset-Server auf dem Lehrer-Computer gehostet werden. Dieses Feld wird nur verändert, wenn ein neuer Medientyp hineingeladen wird. Geschieht dies wird auch das Array der ``marker''-Positionen zurückgesetzt. Das Array enthält die Positionen der Markierungen in Form von Tripeln innerhalb des Arrays.

Die Felder ``playing'', ``playbackPosition'' und ``muted'' werden nur beim Anzeigen von 360°-Videos verwendet. ``playing'' gibt an, ob das betreffende Video in diesem Moment abgespielt oder pausiert werden soll, ``playbackPosition'' gibt die Stelle im Video an, an der es abgespielt werden soll. Werden Videos abgespielt schickt die Lehrer-App jede Sekunde eine Nachricht mit der aktuellen Abspielposition an alle Geräte. Falls die Abspielposition auf einem Gerät um mehr als eine Sekunde versetzt ist, springen dann an die mitgeschickte Position, um von dort weiter abzuspielen. So wird das Abspielen über die Geräte hinweg synchronisiert.
Das ``muted''-Feld gibt an, ob der Ton des Videos an den Geräten abgespielt werden soll oder nicht.

Nachrichten von Seite der Schüler-Geräte sind viel seltener als anders herum. Gibt der Schüler seinen Namen ein wird dieser als Nachricht an die Lehrer-App geschickt. Außerdem senden die Geräte eine Nachricht, wenn sich der visibility-Status ändern. Zum Beispiel, wenn der Browser geschlossen wird. Außerdem geben die Schüler-Geräte Rückmeldung an die Lehrer-App, solange sie Inhalte laden und wiederum, wenn sie die Inhalte fertig geladen beziehungsweise Videos genug vorgeladen haben. Das wird dann mit dem gelben Status-Indikator neben dem Geräte-Namen in der Lehrer-App angezeigt. Wechselt hier die Farbe wieder auf grün, ist das Gerät bereit die Inhalte anzuzeigen.

\subsection{QR-Code Fenster}
Das QR-Code Fenster ist ein zweites Browserfenster, das aus dem main-Prozess der Electron-App auf dem Lehrer-Computer gestartet wird. Es zeigt, wie in \ref{fig:qrcode} zu sehen, einen QR-Code, den die SchülerInnen mit ihren Smartphones scannen können, um bequem die URL zu laden, auf der sie die Schüler-Applikation erreichen können. Der QR-Code wird dynamisch beim Öffnen der App aus der lokalen IP-Adresse des Computers generiert.

Für Geräte, die keine Kamera haben oder wenn das Scannen des QR-Codes fehlschlägt, wird zudem unterhalb des QR-Codes die URL angezeigt, unter der die Schüler-Applikation zu erreichen ist. 

Der QR-Code wird in diesem extra Fenster generiert und angezeigt, damit die Lehrkraft dieses Fenster auf einem Beamer anzeigen kann, um den SchülerInnen den Zugang zur Schüler-App ohne umständliches URL-Abtippen zu ermöglichen.

\begin{figure}
\centering
  \includegraphics[width=0.6\linewidth]{images/QR-Code.png}
  \caption{Qr-Code Fenster mit Link zur Schüler-App.}
  \label{fig:qrcode}
\end{figure}

\subsection{Schüler-App}
In der VRClassroom App nehmen die Schüler eine passive Rolle ein und können selbst nicht in der 3D-Welt navigieren. 

Die WebApp für die Schüler ist eine React360-App, sie stellt eine 3D-Welt da, in deren Mitte sich die Kamera, also der Viewport der Geräte, befindet. Sie stellt die in der Lehrer-App hineingeladenen 360°-Fotos und Videos in einer Kugel um den Viewport da, sodass die Nutzer sich in alle Richtungen umsehen können. 3D-Modelle werden in kurzer Entfernung vor dem Viewport angezeigt, als befänden sich sich im Raum vor der Person.

Abbildung \ref{fig:react360} zeigt die Software-Struktur der React360 App von VRClassroom.

\begin{figure}[ht]
  \includegraphics[width=\textwidth]{images/react360.eps}
 \caption{Struktur der React360 App von VRClassroom.}
  \label{fig:react360}
\end{figure}

\subsubsection{Eingabe des Namens}
Lädt ein Gerät zum ersten Mal die VRClassroom-App wird dem Nutzer neben der Begrüßung, die immer angezeigt wird bevor Inhalte hineingeladen werden, eine Tastatur im VR-Raum angezeigt. Damit können die Schüler ihren Namen eingeben, der dann in der Liste der verbundenen Geräte angezeigt wird. Wird ein Smartphone als VR-Headset genutzt, muss die Tastatur mittels der Touch-Eingabe auf dem Display gemacht werden, bevor die WebVR Ansicht geladen wird. VR-Headsets, die einen Controller haben, können den Namen mit dem Controller auf der Tastatur tippen. 

Damit die Schüler nicht jedes Mal wieder ihren Namen eingeben müssen, wird der eingegebene Name im Browser des Schüler-Gerätes gespeichert und beim erneuten Laden der VRClassroom Applikation direkt wieder an die Lehrer-App übermittelt.

\subsubsection{Anzeigen von 360°-Fotos}
Um 360°-Fotos in der 3D-Szene anzuzeigen lädt die React360-App das Foto von dem Link, den die Teacher-App geschickt hat und setzt sobald das gesamte Foto geladen wurde das Bild als Hintergrund der Szene. Gleichzeitig wird bei erfolgreichem Ladens des Fotos eine Nachricht an die Teacher-App geschickt, um dem Status des Geräts von ``loading'' wieder auf ``active'' zu setzen.
Das 360°-Bild wird dazu von innen an die Kugel in der die React360-Szene ist projiziert und ergibt so durch seine equirectangulare Projektion ein unverzerrtes 360°-Bild.

Das Bild wird angezeigt bis VRClassroom von der Lehrkraft beendet wird oder etwas anderes in die App hineingeladen wird.

\subsubsection{Abspielen von 360°-Videos}
Sendet die Teacher-App die Nachricht, dass ein Video angezeigt werden soll, startet die React360-App sofort mit dem Laden des Videos und lädt soviel von dem Video wie es geht. 
Ist eine Zeit des Videos vorgeladen, sodass React360 davon ausgeht, dass das Video ruckelfrei abgespielt werden kann, sendet wird wiederum die Nachricht, dass das Gerät aus dem ``loading''-Status wieder auf ``active'' gesetzt wird.
Ähnlich wie bei 360°-Fotos wird das 360°-Video dann als Hintergrund-Video der Szene gesetzt, um als 360°-Video abgespielt werden zu können

Die React360-App spielt das Video nicht automatisch ab, sobald genug geladen ist, sondern wartet auf das Signal der Teacher-App, um das Abspielen zu starten. Alle Funktionen werden erst ausgeführt wenn das Signal der Teacher-App kommt, die Funktion auszuführen. Zu den Funktionen zählen: Play, Pause, Springen zu einer anderen Stelle im Video und den Ton des Videos abspielen.

Falls ein Schüler-Gerät sich verspätet verbindet oder aus anderen gründen die Synchronisierung nicht mehr gegeben ist, sendet die Teacher-App während dem Abspielen von Videos jede Sekunde eine neue Nachricht in der die aktuelle Abspielposition enthalten ist. Unterscheidet diese sich um mehr als eine Sekunde von der Abspielposition in der React360-App wird zu der Abspielposition, die in der Nachricht steht gesprungen. So ist auch Vor- und Zurückspulen durch die Lehrkraft möglich.

\subsubsection{Abspielen von Ton in Videos}
Da die meisten Browser das automatische Abspielen von Videos mit Ton verbieten, werden 360°-Videos von React360 standardmäßig beim erstellen der Video-Player-Komponente stumm geschalten. Die Browser-Hersteller wollen dadurch die Ablenkungen, die beim surfen auf den Nutzer zukommen, abmildern. Erst wenn der Nutzer ein ``user gesture click'' also einen Klick auf der Website gemacht hat, darf die Tonspur aktiviert werden. Wird versucht ein Video mit Ton abzuspielen, ohne dass ein Klick gemacht wurde, wird der Video-Player blockiert und kann nicht mehr abspielen. \cite{Decker2017}

Damit React360 in einer single-threaded Umgebung wie einem Webbrowser flüssig ablaufen kann und nicht durch ``blocking behavior'' irgendeiner Art das Rendern unterbrochen wird, ist eine React360-App in zwei Teile aufgeteilt: Die React-Applikation läuft in einem WebWorker-Prozess, das Rendering und die Darstellung erfolgt dann im Prozess des eigentlichen Browser-Fensters. \cite{FacebookInc.2018}

Das führt dazu, das Interaktionen mit React360-Elemente aber nicht als Interaktion mit dem Browserfenster gewertet wird. Deshalb können Videos zunächst nur ohne Ton abgespielt werden..

Um dieses Hindernis zu umgehen ist nun ein durchsichtiger, Bildschirm-füllender Button über die React360-App gelegt, der bei einem Klick verschwindet. Dieser unsichtbare Button aktiviert den Ton des Video-Players. Somit hat die Webseite dann die Erlaubnis Ton in Videos abzuspielen. Ob tatsächlich Ton bei Videos abgespielt wird, kann der Lehrer in der Teacher-App einstellen. Auch dort ist das Abspielen von Ton an den Schüler-Geräten standardmäßig erst einmal deaktiviert.

\subsubsection{Anzeigen von 3D-Modellen}
Wird die Nachricht empfangen, dass der ``mediatype'' auf ``model'' gesetzt wurde, wird als Hintergrund ein Standardhintergrund gesetzt, wie er auch beim Start der App angezeigt wird. Das Laden und Darstellen der 3D-Modellen passiert in der ModelView. Hier wird unterschieden, ob es sich um eine .gltf-Datei oder eine .obj-Datei handelt. Valide gltf-Dateien haben die Endung .gltf oder .glb, ein Container-Dateiformat, das alle Texturen und Modell-Informationen enthält. Handelt es sich um eine .obj-Datei wird versucht eine gleichnamige .mtl-Datei zu laden. Diese enthält alle Informationen über Texturen und Materialien, die .obj-Datei ist alleine das Modell.

Im Anschluss wird die Datei geladen und angezeigt. Damit die Modelle gut zu erkennen sind, wird zudem eine Punktlichtquelle installiert, die das Modell von oben rechts beleuchtet. Außerdem wird das Modell mit einem leichten Ambientlight versehen, damit auch Teile des Modells, die sonst im Schatten liegen, erkennbar werden.

Auf ähnliche Weise werden die Marker gerendert, die von der Lehrkraft gesetzt werden können, wie bereits in ref{subsec:marker} beschrieben. Die Marker haben keine Punktlichtquelle, sondern nur ein Ambientlight. Sie werden dynamisch an die von der Lehrkraft ausgewählte Stelle in der VR-Szene gerendert und ihre Größe in Abhängigkeit ihrer Entfernung zur Kamera skaliert.

\subsubsection{Cardboard VRDisplay}

Die Nutzung der Schüler-App soll darauf optimiert werden auf Smartphones verwendet zu werden. Wie in \ref{subsec:webvr} beschrieben muss für die meisten Smartphones die WebVR-API per Polyfill nachgerüstet werden. Das Polyfill stellt benötigten APIs zur Darstellung der Inhalte bereit. Dedizierte VR-Geräte wissen, wie die Inhalte für das jeweilige Auge dargestellt werden müssen. Auf einem Smartphone ist aber selbst, wenn die WebVR-API per Polyfill nachgerüstet wurde noch kein ``VRDisplay'' verfügbar.

Ist im Anzeigenden Browser die WebVR-API nicht verfügbar oder kein ``VRDisplay'' verbunden, kann der Inhalt trotzdem direkt im Browser gesehen werden. Er wird dann in ein HTML-Canvas gerendert. Auch wenn ein Smartphone die WebVR-API unterstützt, hat es normalerweise kein verbundenes VRDisplay. \cite{WebVR} 

Von der Immersive Web Working Group des W3Cs gibt es eine Implementierung eines virtuellen VRDisplays: cardboard-vr-display meldet an der WebVR-API verbundenes VRDisplay an. Wenn in einem Browser ein VRDisplay verbunden ist, wird wie in Abbildung \ref{fig:WebVRfig}a links zu sehen ein Button in der unteren rechten Ecke des Fensters angezeigt, mit dem dann die VR-Ansicht geladen werden kann.\cite{Jsntell}

Wird der Button gedrückt übernimmt WebVR die Darstellung und sendet die Inhalte an der virtuell erzeugte cardboard-vr-display. Wie Abbildung \ref{fig:WebVRfig}b zeigt wird dafür der Bildschirm in zwei Bilder für die Linsen in einem VR-Headset wie zum Beispiel dem Google Cardboard aufgeteilt. Die zwei Bilder sind dabei nicht Bildschirm-füllend, sondern in einer annähernd ovalen Form, die von einem schwarzen Rand umgeben wird, sodass sie gut auf die Linsen des genutzten Headsets passen. 


\begin{figure}[h!]
  \centering
  \begin{subfigure}[b]{0.4\linewidth}
    \includegraphics[width=\linewidth]{images/WebVR1.png}
    \subcaption{``View in VR'' Button in VRClassroom.}
  \end{subfigure}
  \begin{subfigure}[b]{0.4\linewidth}
    \includegraphics[width=\linewidth]{images/WebVR2.png}
    \subcaption{VR-Szene mit WebVR Ansicht.}
  \end{subfigure}
  \caption{WebVR Anwendung im nicht aktiven und aktiven VR-Modus.}
  \label{fig:WebVRfig}
\end{figure}

Damit ist VRClassroom auf allen gängigen Smartphones im VR-Modus über die WebVR-API nutzbar.

%______________________________________________________________________
\cleardoublepage

\section{Nutzerstudie und Evaluation}
Um das entwickelte VRClassroom-System zu evaluieren wurden zwei verschiedene Umfragen geplant. Eine quantitative Online-Befragung, bei der die Teilnehmer ein Video ansehen und anschließend Fragen beantworten, und eine qualitative Feldstudie, bei der Lehrkräfte VRClassroom mit ihrer Klasse ausprobieren und im Anschluss Feedback geben.

\subsection{Online-Umfrage}
\label{subsec:evaluation}
Um die Meinung möglichst vieler verschiedener Lehrkräfte zum entwickelten VRClassroom System zu erfahren, wurde eine Online-Umfrage durchgeführt, bei der die Teilnehmer ihre Meinung zu VRClassroom anzugeben, nachdem sie ein erklärendes Video gesehen haben.
Dafür wurde ein Video produziert, welches die Idee und Nutzung von VRClassroom erklärt und die einzelnen Funktionen zeigt, die die App anbietet. Im Anschluss haben die teilnehmenden Lehrkräfte den Fragebogen ausgefüllt.
Das Video ist unter https://youtu.be/b60PQ3bqjk0 zu erreichen.
Der Fragebogen ist dem Appendix dieser Arbeit angehängt.

Die Fragen wurden als Aussagen formuliert, zu denen mit Hilfe einer unzentrierten Likert-Skala die Zustimmung zur jeweiligen Aussage angegeben werden konnte. Es wurde eine unzentrierte Skala verwendet um zu aussagekräftigeren Antworten zu ermuntern.

Insgesamt haben 59 Personen an der Umfrage teilgenommen, allerdings wurden nicht alle Fragen von allen Teilnehmer beantwortet, da die Beantwortung der einzelnen Fragen optional war. Die Prozentangaben aus den Abbildungen beziehen sich dabei immer auf die abgegebenen Antworten für die jeweilige Frage.

Die Umfrage war an Lehrkräfte aller Schulen und Altersgruppen gerichtet, um eine möglichst diverse Meinungsbasis zu erhalten.

\subsubsection{Gesamteindruck VRClassroom}
Allgemein waren Antworten der Studienteilnehmer positiv. 83\% der Teilnehmer gaben an, dass sie VRClassroom als intuitiv und leicht bedienbar bewerten würden. 
Besonders überzeugt waren sie von der Wirkung auf die Schüler. Abbildung \ref{fig:survey1} zeigt, dass eine große Mehrheit von 92\% der Meinung ist, dass die Schüler am Einsatz von VRClassroom im Unterricht Spaß hätte. Auch für sich selbst erwarten 78\%, dass sie Spaß bei der Nutzung hätten. 66\% sind zudem der Meinung, dass VRClassroom dazu beitragen würde, dass ihre Schüler drei dimensionale Inhalte besser verstehen würden als ohne.

\begin{figure}[ht]
  \includegraphics[width=\textwidth]{images/survey1.eps}
 \caption{Ich brauche noch einen sinnvollen Text.}
  \label{fig:survey1}
\end{figure}

In Freitext-Feldern der Umfrage konnte qualitatives Feedback der Teilnehmer gesammelt werden.

Der Hauptkritikpunkt an VRClassroom war die technische Ausstattung, die zur Nutzung benötigt wird. Dabei ging es zum Einen darum, dass nicht alle Schüler, besonders in den unteren Klassenstufen und der Grundschule, ein Smartphone besitzen. Andererseits befürchten Viele, dass die Schule nicht die nötigen Vorraussetzungen bietet, um VRClassroom einsetzen zu können.
Dabei sehen sie vor Allem das fehlende oder schlecht verfügbare WLAN in ihrer Schule, aber auch, dass die Schule keine VR-Headsets besitzt und dafür auch kein Budget zur Verfügung steht.


VRClassroom wurde entwickelt um die Problematik fehlende Infrastruktur in den Schulen und das fehlende Budget für die Ausrüstung zu minimieren. Wird davon ausgegangen, dass ein Rechner im Klassenzimmer zur Verfügung steht sind die Anschaffungskosten für einen Satz einfacher Cardboard-Headsets für eine gesamte Klasse mit circa 30 Euro zu veranschlagen. 
Falls kein Internetzugang im Klassenzimmer zur Verfügung stehen kann mit einem WLAN-Access-Point ein lokales Netzwerk für VRClassroom eröffnet werden. Eine Internetverbindung ist nicht notwendig, da alle Daten im lokalen Netzwerk übertragen werden. Für einen einfachen WLAN-Access-Point müssen noch einmal etwa 30 Euro eingerechnet werden müssen. Das ganze System könnte also mit einem finanziellen Aufwand von etwa 60 Euro eingesetzt werden.

Ein weiterer Kritikpunkt an VRClassroom ist, dass es keine fest integrierten Inhalte hat. Einige Lehrkräfte wünschen sich ein System, das bereits passende Inhalte mitbringt, die dann speziell dafür angepasst sind und auch den Ansprüchen für den Unterricht entsprechen. 
Da VRClassroom als offenes System entwickelt wurde, können beliebige Inhalte verwendet werden. Sowohl selbsterstellte oder Inhalte von anderen Produzenten. Sollte VRClassroom in der Zukunft weiterentwickelt werden, könnte ein Geschäftsmodell darin bestehen, passende Inhalte für den Schulunterricht zu erstellen.

Zudem wurde kritisiert, dass Google Cardboards nicht mit einer Brille genutzt werden können. Einige standalone-Headsets ermöglichen die Nutzung für Brillenträger, wie oben beschrieben. Diese Limitierung ist eine Hardware-Limitierung und lässt sich softwareseitig in VRClassroom nicht lösen.

\bigskip

Als besonders vorteilhaft schätzen die Teilnehmer an VRClassroom ein, dass es die Schülerinnen und Schüler motiviert. Außerdem finden sie es sehr gut, dass die Aufmerksamkeit auf die gezeigten Inhalte gelenkt wird und die Schüler kaum Ablenkungen preisgegeben sind. 
Einige Teilnehmer äußerten zudem, dass sie es sehr schätzen das dass System so gestaltet ist, dass nur die geringen Anschaffungskosten von Google Cardboards anfallen würden und das ganze System keinen hohen finanziellen Anspruch hat.

Zudem lobten sie die einfach wirkende Handhabung und die Kompatibilität verschiedenster VR-Plattformen.
Besonders gefiel zudem die Möglichkeit Orte virtuell zu Erkunden und die korrekte Darstellung räumlicher Inhalte sowie die Darstellung von Inhalten, die sonst schwer darzustellen sind.

\bigskip

Auf die Frage nach Funktionen, die sich die Teilnehmer für VRClassroom wünschen würden, wurde besonders oft angemerkt, dass die Lehrkräfte die Marker mit einem Text oder sogar Links versehen können. 
Einen einfachen Text an die Marker zu binden, könnte für manche Unterrichtsinhalte sicherlich eine Bereicherung sein, die in der Zukunft bei einer Weiterentwicklung von VRClassroom realisiert werden könnte.
Das öffnen von Links dagegen würde die VR-Welt verlassen und dazu führen, dass die URL in einem neuen Browserfenster geöffnet werden würde. Um dann weiter zu agieren, müsste das Smartphone aus dem VR-Headset entfernt.

Zudem wünschten sich die Teilnehmer eine Funktion, um überwachen zu können, dass die Schüler die gezeigten Inhalte auch ansehen. Der Aktivitätsindikator zeigt in der aktuellen Implementierung bereits an, ob ein Schüler gerade den Inhalt im Vordergrund hat.

Ein weiterer Funktionswunsch war die schnelle Skalierung der 3D-Modelle auf Realgröße, um den Schülern einen guten Eindruck zu vermitteln, wie das Objekt in echt aussieht. Darauf wird im folgenden in \ref{subsec:modelscaling} eingegangen.

Einige Teilnehmer forderten eine Schulung für Lehrkräfte zum Umgang mit Virtual Reality und mehr Betreuungspersonal für die Technik in der Schule. Die Lehrkräfte wünschen sind für den Unterricht nutzbare Inhalte. Am besten wäre eine Plattform für Lehrinhalte für Virtual Reality oder die Möglichkeit Themenpakete zu lizensieren.

Themengebiete, die sich die befragten Lehrkräfte für Virtual Reality Anwendungen wünschen würden umfassten viele naturwissenschaftliche Inhalte. Besonders 3D-Modelle wurden vorgeschlagen: Die Teilnehmer nannten: Inhalte, die Vektoren, Ebenen und Schnitte von mathematischen Körpern zeigen, Touren und Bilder von Kernkraftwerken oder anderen Orten, die nicht im Rahmen einer Exkursion besucht werden können, Körper, Organe und biologische sowie chemische Prozesse, Moleküle und architektonische Modelle.

\subsubsection{Medien in VRClassroom}
Abbildung \ref{fig:survey2} zeigt die Meinung der Befragten zu den verschiedenen Medientypen die in VRClassroom unterstützt werden. 360°-Videos wurden dabei deutlich schlechter bewertet als Fotos und 3D-Modelle. Ein Drittel der Befragten schätzten dabei die Nutzung von 360°-Videos in VRClassroom als nicht hilfreich zur Vermittlung von Lehrstoff ein. Mit 87\% Zustimmung schnitten die 3D-Modelle nach Einschätzung der befragten Lehrkräfte am Besten ab, um Lehrinhalte zu vermitteln.

\begin{figure}[ht]
  \includegraphics[width=\textwidth]{images/survey2.eps}
 \caption{Einschätzung zu den verschiedenen Medientypen in VRClassroom.}
  \label{fig:survey2}
\end{figure}

Wie Abbildung \ref{fig:survey3} zu zeigt, korrelieren die Einschätzungen zu Markern auf den unterschiedlichen Medientypen mit den Meinungen zu den Medientypen selbst: 82\% stuften Marker auf 360°-Fotos als hilfreich ein, auf 360°-Videos waren es nur 61\% und bei den 3D-Modellen waren es 83\%.

\begin{figure}[ht]
  \includegraphics[width=\textwidth]{images/survey3.eps}
 \caption{Einschätzung zu Markern auf den verschiedenen Medientypen in VRClassroom.}
  \label{fig:survey3}
\end{figure}


\subsubsection{Alters- und Fächergruppen für VRClassroom}
Anschließend wurden die Lehrkräfte dazu befragt für welche Fächergruppen VRClassroom einen Mehrwert bringen könnte.
Die Fächer wurden zu Gruppen zusammengefasst, um die Studie möglichst unabhängig von Schultypen zu machen.

Für die Fächergruppe der naturwissenschaftlichen Fächer wurde VRClassroom mit 89\% Zustimmung als sinnvoll erachtet. Ebenfalls hohe Zustimmungswerte gab es für die gesellschaftswissenschaftlichen Fächer und Kunst/Musik (siehe \ref{fig:survey4}).

\begin{figure}[ht]
  \includegraphics[width=\textwidth]{images/survey4.eps}
 \caption{Mehrwert von VRClassroom für verschiedene Fächergruppen.}
  \label{fig:survey4}
\end{figure}

Viele Teilnehmer antworteten in der Folgefrage nach Lehrstoff, den sie sich gut vorstellen können, dass sie besonders geschichtliche Themen sowie Plastiken und architektonische Modelle neben den naturwissenschaftlichen Inhalten sähen. Zu diesen zählen: Chemikalische Experimente, physikalische Kräfte, Moleküle und geometrische Körper.

Interessant war zudem der Vorschlag Bewegungsabläufe aus dem Sportunterricht im Vorfeld in VR zu besprechen, damit die Schüler besser verstehen, wie der optimale Ablauf ist.

Viele Teilnehmer glauben, dass im Sprachunterricht VRClassroom keinen Mehrwert bringt.

\bigskip

In Frage 18 wurde abgefragt, welche Klassenstufen für die Nutzung von VRClassroom geeignet sind. 59\% der Studienteilnehmer halten Grundschüler für noch nicht geeignet. Erst an Schulen der Sekundarstufe wird die Software als geeignet eingestuft. Dabei steigen die Zustimmungszahlen mit dem Alter der Schüler von 73\% bei der Unterstufe, über 82\% bei der Mittelstufe auf 89\% in der Oberstufe. Lediglich für Erwachsene sinkt sie wieder leicht auf 88\%.

Abbildung \ref{fig:survey5} zeigt die genauen Prozentwerte zu diesem gut erkennbaren Trend.

\begin{figure}[ht]
  \includegraphics[width=\textwidth]{images/survey5.eps}
 \caption{Eignung der Klassenstufen für VRClassroom.}
  \label{fig:survey5}
\end{figure}


\subsubsection{Net Promotor Score}

Der Net Promotor Score, kurz NPS, ist ein verbreitetes Erhebungssystem um die Zufriedenheit und Loyalität der Nutzer eines Produkts zu erheben.

Dabei sollen die Befragten auf einer Skala von 0 bis 10 angeben, wie wahrscheinlich sie das Produkt an einen Kollegen weiterempfehlen würden.
0 steht dabei für sehr unwahrscheinlich und 10 steht für extrem wahrscheinlich.
Aus den Zahlenwerten kann dann ein Wert berechnen, der die Loyalität der Nutzer wiederspiegeln soll.

Personen, die mit einem Wert von 0 bis 6 angeben gelten als Detraktoren, also Personen, die ihren Kollegen davon abraten würden das Produkt zu verwenden. Bei Werten von 7-8 gelten die Personen als Indifferente, die das Produkt nutzen, allerdings weder positiv noch negativ Anderen gegenüber davon berichten würden. Die dritte Gruppe sind die Promotoren, die auf die Frage mit 9 oder 10 geantwortet haben. Promotoren werden ihren Kollegen positiv von ihren Erfahrungen mit dem Produkt berichten und dadurch dafür sorgen, dass das Produkt neue Kunden gewinnt und bekannter wird. Der NPS wird berechnet, indem die Prozentzahl der Detraktoren von der der Promotoren abgezogen wird. Daraus ergibt sich ein möglicher Wertebereich des Net Promoters Scores von {-100,+100}.\cite{White2008}

\begin{figure}[ht]
  \includegraphics[width=\textwidth]{images/survey6.eps}
 \caption{Net Promotor Score von VRClassrooms.}
  \label{fig:survey6}
\end{figure}

In der Studie zeigte sich, dass 58\% der Teilnehmer Detraktoren wären, 29\% sind indifferent und nur 13\% sind von VRClassroom überzeugt und würden auch ihre Kollegen ermuntern, die Software auszuprobieren. Abbildung \ref{fig:survey6} veranschaulicht den NPS der Online-Evaluation von VRClassroom. Der berechnete Net Promoter Score von VRClassroom läge somit bei -45. 

\subsubsection{Einschätzung der Zukunft von VR und VRClassroom}
\begin{figure}[ht]
  \includegraphics[width=\textwidth]{images/survey7.eps}
 \caption{Einschätzungen zur Zukunft von VR und VRClassroom.}
  \label{fig:survey7}
\end{figure}

Trotz der insgesamt positiven Bewertungen von VRClassroom in der Studie zeigt Abbildung \ref{fig:survey7}, dass nur 42\% glauben, dass Virtual Reality zukünftig eine große Rolle im Schulunterricht spielen wird.
Trotzdem können sich 65\% vorstellen VRClassroom in ihrem Unterricht zu verwenden.

Abbildung \ref{fig:survey8} zeigt, dass die meisten Lehrkräfte VRClassroom nur sporadisch einsetzen würden und nur wenige monatlich oder öfter darauf zurückgreifen würden. 

\begin{figure}[ht]
  \includegraphics[width=\textwidth]{images/survey8.eps}
 \caption{Voraussichtliche Einsätze von VRClassroom im eigenen Unterricht.}
  \label{fig:survey8}
\end{figure}

Zuletzt wurde nach Szenarien (auch außerhalb des Unterrichts) gefragt, in denen die Teilnehmer den Einsatz von VRClassroom sehen könnten. Die Teilnehmer gaben hauptsächlich Szenarien an, die im Zusammenhang mit Orten stehen. Von virtuellen Museumsbesuchen, Erkundung von interessanten Orten  über Tourismus und zur Vorbereitung von Exkursionen.

Besonders interessante Ideen war der Einsatz in der Fahrschule um die Situation im Auto möglichst realistisch nachzuempfinden und die Begleitung von Personen an ihrem ersten Arbeitstag. So könnte beispielsweise der Arbeitsalltag einer Person und das Arbeitsumfeld an der Arbeitsstelle im Rahmen einer Orientierungsfindung besser eingeschätzt werden.

Ein drittes Szenario war die Anwendung in Architekturbüros, die 360°-Fotos des Ist-Zustands und 3D-Modelle der geplanten Bauten präsentieren können.


\subsubsection{Demografie und persönliche Angaben}
Am Ende der Befragung wurden noch einige demographische Fragen gestellt. Es wurde nach dem Geschlecht, dem Alter (innerhalb von Altersgruppen) und der Schulart, an der die Person unterrichtet, gefragt.
Zudem wurden die Teilnehmer gefragt, ob sie sich selbst als technikaffin bezeichnen würden und ob sie schon einmal eine VR-Brille benutzt haben.
Als Folgefrage sollte dann noch angegeben werden, welche VR-Brille schon einmal ausprobiert wurde. 
Insgesamt war die Teilnehmermenge in allen Aspekten sehr ausgewogen: Beim Geschlecht, dem Alter und den Schulen, an denen die Personen unterrichten, waren die Teilnehmer sehr divers verteilt, sodass davon auszugehen ist, dass die Antworten der Studie ein realistisches Abbild der Berufsgruppe liefern.

\begin{figure}[ht]
  \includegraphics[width=\textwidth]{images/survey9.eps}
 \caption{Demografische und persönliche Angaben.}
  \label{fig:survey9}
\end{figure}

Die überwiegende Mehrheit mit 86\% bezeichnet sich selbst als technikaffin, wie Abbildung \ref{fig:survey9} zeigt. Allerdings haben nur 46\% der Teilnehmer schon einmal eine VR-Brille ausprobiert.

Die Frage nach der Schulart zeigt, dass Lehrkräfte für alle Altersgruppen und Schularten von Grundschulkindern bis Erwachsenen Schülern an Berufs- oder Fachoberschulen teilgenommen haben. 

\subsubsection{Ergebnis und Schlüsse aus der Evaluation}
Die Umfrage hat gezeigt, dass viele Lehrkräfte noch keine Erfahrung mit Virtual Reality generell und auch im Unterricht haben. Das Interesse dazu ist jedoch durchaus vorhanden. Wie bereits beschrieben fürchten jedoch viele der Teilnehmer, dass ein System wie VRClassroom aufgrund von schlechter, veralteter oder gar komplett fehlender Infrastruktur scheitern würde.

Das bestätigt die Idee hinter der Struktur von VRClassroom im aktuellen Zustand: Denn das System wurde so entwickelt, dass es bis auf die Funktion Google StreetView Panoramen zu laden komplett ohne Verbindung zum Internet funktioniert.
Auch die Entwicklung der Software als WebVR-App im Browser wird durch die Umfrage bestätigt. So kann durch eine sehr geringe Investition in einen Satz Google Cardboards mit den eigenen Geräten der Schülerinnen und Schüler VRClassroom benutzt werden. Wird dann zu einem späteren Zeitpunkt in Standalone-VR-Headsets investiert, kann die bestehende Software weiter genutzt werden und es wird keine neue Eingewöhnung benötigt.

\subsection{VRClassroom Studie in Schulen}
Da das System speziell für die Nutzung im Schulunterricht entwickelt wurde, sollte auch die Nutzerstudie in diesem Szenario durchgeführt werden. Dafür wurden Lehrkräfte angefragt, die einen Teil ihrer Unterrichtszeit für einen Nutzertest der Software verwenden wollten und im Anschluss bereit waren einen kurzen Fragebogen dazu zu beantworten.

Insgesamt hatten sich 15 Lehrkräfte bereit erklärt mit ihrer Klasse das VRClassroom-System zu evaluieren. 


Für den Test sollte allen SchülerInnen ein Cardboard zur Verfügung gestellt werden, das die Kinder mir ihrem eigenen Smartphone ausstatten. 
Der Rechner, auf dem das Programm installiert war, sollte den Lehrkräften bereitgestellt und den Lehrkräften im Vorfeld eine kurze Einführung in die Software gegeben werden.


Für die Durchführung der Studie war ein Zeitrahmen von 15 bis 20 Minuten pro Klasse veranschlagt. In dieser Zeit sollte die Lehrkraft die App starten, sich alle Schüler-Geräte damit verbinden und die Lehrkraft die eigentlichen Inhalte präsentieren.

Damit die Studienteilnehmer ein Bild zu allen Funktionen des Systems machen können, hatte jede Lehrkraft ein vollständiges Set an Inhalten mit jeweils mindestens einem 360°-Fotos, einem 360°-Video und einem 3D-Modell erhalten.

\subsubsection{Freigabe Kulturministerium}
Da alle Studien, die an bayerischen Schulen gemacht werden, einer Freigabe des Kultusministeriums bedürfen, wurde im Vorfeld der Studie beim Kultusministerium eine solche Freigabe beantragt. Für den Antrag werden alle Fragebögen und der gesamte Ablauf der Studie an das Kultusministerium zur Prüfung vorgelegt. Das Kultusministerium hat die Durchführung von Studien beschränkt, um den Datenschutz der Schülerinnen und Schüler zu sichern. Da allerdings in der Studie nur die Lehrkräfte befragt werden und die Fragebögen anonymisiert sind, ergäbe sich für die Daten der Schüler keine Gefahr.

Leider wurde der Antrag vom Kultusministeriums nie beantwortet, sodass die gesamte Studie kurzfristig abgesagt werden musste. Da das VRClassroom-System speziell für den Einsatz im Unterricht entwickelt wurde, wäre eine praktische Nutzerstudie im Unterricht sehr sinnvoll gewesen, um abschätzen zu können, ob es im aktuellen Zustand einen guten Mehrwert bietet oder noch Anpassungen braucht um gut im Unterrichtsablauf zu funktionieren.


%______________________________________________________________________
\cleardoublepage

\section{Ausblick}
Vorraussichtlich wird die Rolle von VR in den kommenden Jahren weiter wachsen, von Ausbildung, Beruf bis zur Freizeit, wird es immer mehr VR-Geräte und auch Anwendungen geben. 
Im Bildungsbereich zeigt VRClassroom jetzt eine Möglichkeit für Lehrkräfte ohne großen finanziellen Aufwand und ohne viel Vorwissen zu benötigen, VR-Inhalte in ihrem Unterricht zu nutzen. Im Folgenden wird darauf eingegangen, wie die Zukunft von Virtual Reality aussehen kann, beziehungsweise was an nächsten Entwicklungsstufen passieren muss, damit VR den prognostizierten Durchbruch auch haben wird.
Außerdem werden bestehende Probleme von VRClassroom besprochen und wie das System in der Zukunft weiterentwickelt werden könnte.

\subsection{Zukunft von VR}
Trotz dieser steigenden Zahlen und positiven Prognosen ist davon auszugehen, dass es noch eine Weile dauern wird, bis Virtual Reality den Weg in die Schulen finden wird. Wie in den Ergebnissen der Studie zu sehen sind, glauben nur 42\% der Befragten, dass Virtual Reality - in welcher Form auch immer - eine große Rolle im Unterricht spielen wird. Außerdem haben selbst von den teilnehmenden Lehrkräften bis dato nur 46\% jemals selbst eine VR-Brille genutzt. Es ist eine Voraussetzung, dass die Lehrkräften mit dem Umgang mit VR vertraut sind und genug Unterrichts-geeignetes Material zur Verfügung steht, bevor Virtual Reality im großen Umfang im Unterrichtsgeschehen einbezogen werden kann. VRClassroom soll dazu einen Beitrag leisten, damit sowohl Schülerinnen und Schüler, als auch Lehrkräfte erste Erfahrungen mit Virtual Reality als Lernmedium sammeln können.

Die Studie zeigt allerdings auch, das bei vielen Lehrkräften Interesse an der Thematik vorhanden ist, dies jedoch oft an der fehlenden Infrastruktur scheitert. Damit die Schulen VR-Systeme einsetzten können müssen die Schulnetzwerke verbessert werden und finanzielle Mittel für VR-Headsets bereitgestellt werden. 

Zukünftig ist damit zu rechnen, dass die VR-Headsets weiter im Preis fallen und gleichzeitig mehr Rechenleistung bekommen. Zudem zeigen die Ankündigungen der Hersteller, dass weitere Stand-alone Headsets auf den Markt kommen werden, die sich für die Nutzung im Unterricht eignen können.

Weiterhin könnte es sein, dass die Entwicklung der Headsets mit der Zeit wieder weg von klassischen Controllern und hin zu anderen Interaktionskonzepten gehen wird. Dabei kämen zum Beispiel das Erfassen der Augenbewegungen wie es bei EyeVR passiert \cite{Geiselhart2016} oder die Nutzung von Freihandgesten wie es die ClassVR-Headsets tun in Frage. \cite{AvantisSystemsLtd}

Eine weitere Veränderung ist die Wahrnehmung von Virtual Reality. Wie die Autorin von ``5 Major Challenges For The VR Industry'' beschreibt, wird momentan VR von viele Menschen noch als Spielerei wahrgenommen. Diese Wahrnehmung wird sich aber zusehends ändern, wenn mehr Anwendungen abseits von Spielen auf den Markt kommen. \cite{Wolwort2018}

\subsection{Probleme von VRClassroom}
In den Tests haben sich ein paar Probleme gezeigt, die das VRClassroom-System in seinem aktuellen Zustand noch hat, die beim produktiven Einsatz im Schulunterricht das Erlebnis stören können.
Diese Problem konnten nicht mehr im Rahmen dieser Arbeit gelöst werden. Auch die technischen Grenzen von Geräten spielen dabei aktuell noch eine Rolle, die sich in der Zukunft durch leistungsstärkere Hardware lösen lassen.

\subsubsection{Anzeigen großer Dateien}
\label{subsec:problems}
Eines der größten Probleme, die VRClassroom momentan hat, ist die Anzeige von komplexen 3D-Modellen oder 360°-Videos mit einer großen Dateigröße. 360°-Videos sind besonders problematisch, wenn sie eine extrem hohe Auflösung, von mehr als 4096 Pixeln und eine hohe Framerate haben. Bei der Datenübertragung per WLAN müssen die Inhalte an alle verbundenen Geräte gesendet werden. In einer Schulklasse können das bis zu etwa 30 Geräte sein. Die verfügbare Bandbreite des WLANs muss dann zwischen allen Geräten geteilt werden, so dass die Übertragung zu den einzelnen Geräten nicht schnell genug ist. 

Bei 4k-Videos sind Datenraten von 20 MBit/s und mehr nicht unüblich. Bei älteren WLAN-Standards mit einer Übertragungsrate von nur 54 MBit/s, können damit maximal zwei Clients parallel beliefert werden. Werden mehr Geräte bespielt, muss entsprechend lange gewartet werden, bis die Inhalte überall entsprechend vorgeladen sind, damit sie flüssig abgespielt werden.

Zudem war bei Tests vereinzelt zu sehen, dass bei sehr detailreichen Modellen die Anzeige abstürzt und ein Neuladen der Schüler-App herbeigeführt wird, sodass das Gerät aus dem Cardboard genommen und die VR-Ansicht neu aktiviert werden muss. Hier ist der Arbeitsspeicher der Geräte der Engpass.

\subsubsection{Synchronisierung}
Ein der Herausforderungen bei VRClassroom war es, alle Darstellung auf allen Geräten zu synchronisieren. Besonders beim Abspielen von 360°-Videos fallen hier auch schon geringe Verschiebungen auf. 

Der WebSocket-Server sendet die Nachricht über den veränderten Inhalt gleichzeitig an alle verbundenen Geräte, als eine Broadcast-Nachricht. Trotzdem kann es passieren, dass die Geräte unterschiedlich lange brauchen um auf diese Nachricht zu reagieren. Dadurch ist die Anzeige der Geräte , bei Fotos und 3D-Modellen ist das im Normalfall kein Problem. Der Ton der Videos kann dadurch aber bei einzelnen Geräten um einen Bruchteil einer Sekunde verschoben sein, was zu unangenehmen Vermischungen mit anderen Geräten im Raum und Halleffekten führen kann. 

Eine nicht technische Lösung hierfür kann es sein, dass nur der Sound ausschließlich vom Lehrer-PC abgespielt wird. Dadurch würden aber Bild- und Tonspur gegebenen Falls leicht gegeneinander verschoben. Die Schülerinnen und Schüler könnten ebenso Kopfhörer verwenden oder die Lautstärke am Gerät sehr leise einstellen. Technische Lösungen zu Millisekunden-genauen Synchronisierung von Videowiedergaben über das Netzwerk sind sehr aufwändig zu implementieren.

\subsection{Mögliche Weiterentwicklungen an VRClassroom}
VRClassroom ist ein vollfunktionsfähiges System, dass laut Umfrageergebnissen bereits bei einigen Lehrkräften das Interesse geweckt hat, es in ihrem Unterricht einzusetzen. Trotzdem sind in der Entwicklung und bei der Online-Befragung noch mögliche Weiterentwicklungen von VRClassroom aufgefallen, die zukünftig integriert werden könnten.

\subsubsection{Online-Verbindung statt lokalem Netzwerk}
Eine mögliche Weiterentwicklung von VRClassroom könnte sein das ganze VRClassroom System nicht lokal auf dem Rechner der Lehrkraft laufen zu lassen, mit dem sich dann alle VR-Geräte verbinden, sondern eine Version anzubieten, die über das Internet bereit gestellt wird.

Aktuell müssen alle Geräte der Schülerinnen und Schüler im selben Netzwerk wie der Rechner der Lehrkraft sein und Kommunikation zwischen den Geräten im Netzwerk erlaubt werden. Dies ist bei betreuten Netzwerken in Schulen nicht immer der Fall. Wenn VRClassroom über das Internet genutzt werden kann, müssten sich die Geräte nicht mehr in einem gemeinsamen Netzwerk befinden. Das kann die Einrichtung und das Verbinden der Geräte erleichtern.

Zudem würde eine internetbasierte Lösung erlauben, das sich nicht alle Teilnehmer am selben Ort befinden müssen. So könnten e-Learning-Konzepte auf Basis von VRClassroom entwickelt werden.

 Durch Vorgespräche mit den Lehrkräften, die an der Studie teilnehmen wollten und aus eigener Erfahrung wurde schnell klar, dass jede Schule unterschiedlich gut an das Internet angebunden sind. Es gibt Schulen, die für Schülerinnen und Schüler gar kein Internetzugang anbieten oder dieses stark beschränken. Um diese Probleme zu umgehen, wurde dann die Entscheidung gefällt, VRClassroom so zu entwickeln, dass es unabhängig vom Internet funktionieren kann.

Ein weiteres Argument für die aktuelle Implementierung ist das schon zuvor in \ref{subsec:problems} besprochene Problem des Ladens von komplexen Modellen oder hochaufgelösten Videos, also großen Datenmengen. Dies ist über lokale Netze im Normalfall schneller aus über das Internet. Gerade bei Mobilgeräten kann das Herunterladen von großen Inhalten viel Datenvolumen verbrauchen.  

Ist kein lokales WLAN in der Schule verfügbar, oder das Schul-WLAN lässt keine Verbindungen zwischen einzelnen Geräten zu, könnte für die Verwendung von VRClassroom ein eigener WLAN-Access-Point aufgestellt werden, der für die Anwendung vorkonfiguriert ist. Die Kosten hierfür sind gering. Einzig die StreetView-Funktion fällt weg, wenn der Lehrer-PC nicht auf das Internet zugreifen kann.

\subsubsection{Skalierung von 3D-Modellen auf echte Größe}
\label{subsec:modelscaling} 
Um den Schülern die tatsächliche Größe von betrachteten Modellen zu vermitteln, könnte eine Funktion in der Lehrer-App eingefügt werden, die die 3D-Modelle auf Realgröße skaliert. Dazu müsste für die verwendeten 3D-Modelle diese Information vorliegen, was nur sehr selten der Fall ist, da diese Funktion in den meisten Szenarien nicht benötigt wird.

Dazu ist auch eine Anpassung an VRClassroom notwendig. Verkleinern der Modelle ist jetzt schon problemlos möglich. Problematisch wird die Skalierung aber wenn das Modell deutlich größer dargestellt wird als der Betrachter. Dabei kann es passieren das Teile des Modells nicht mehr vollständig angezeigt und abgeschnitten werden, da sie aus der  Szene herausragen. Außerdem bräuchten Modelle, damit eine große Darstellung gut aussieht auch einen entsprechen hohen Detailgrad, der wie oben beschrieben Probleme bereiten kann.

\subsubsection{Navigation von 3D-Modellen aus Schüler-App}
Momentan können Schüler keinen Einfluss darauf nehmen, welchen Teil eines 3D-Modells sie sehen können und welchen nicht, da nur aus der Lehrer-App gesteuert werden kann wie das Modell skaliert und gedreht wird. 
Eine mögliche Erweiterung von VRClassroom könnte sein, dass die Schüler die 3D-Modelle selbstständig drehen und skalieren können beziehungsweise zu anderen Punkten am Modell springen können. 


\subsubsection{Ausfragemodus: Ausgewählter Schüler setzt Markierung}
Eine mögliche Weiterentwicklung von VRClassroom wäre eine Art ``Ausfragemodus'', bei dem die Lehrkraft aus der Liste der verbundenen Geräte eins auswählen kann, das dann eine Markierung setzen kann. Das ausgewählte Gerät hätte dann die Möglichkeit einmalig eine Markierungen zu setzen.

Dafür wäre es notwendig für die Schüler-Geräte eine Möglichkeit zu geben einen Punkt auszuwählen, an dem die Markierung gesetzt werden soll. Bei der aktuellen Implementierung mit einem Smartphone in einem Google Cardboard ist das noch nicht möglich. Dazu müsste mit dem Touch-Event vom Cardboard an der aktuellen Gaze-Position des Nutzers an dieser Stelle ein Marker gesetzt werden. Mit einem Headset, das bereits einen zugehörigen Controller hat, wäre das einfacher zu lösen. 

Da die Markierungen auch in der Lehrer-App über das iFrame mit der Schüler-App gesetzt werden ist die Kommunikation der Markerposition an die Lehrer-App bereits implementiert und müsste nur leicht abgeändert werden. Möglicherweise wäre es gut die von Schülern gesetzten Marker noch in einer anderen Farbe darzustellen wie die der Lehrkraft, um für andere Schüler leichter erkennbar zu machen, welche Marker von welcher Person kommen. 

Momentan können Schüler nicht beeinflussen wie sie 3D-Modelle sehen, sondern nur den von der Lehrkraft ausgewählten Blickwinkel. Um auch auf 3D-Modellen sinnvoll Markierungen setzen zu können wäre es dann auch sehr sinnvoll den Schülern die Möglichkeit zu geben den Blickwinkel des 3D-Modells zu verändern. 

\subsection{Fazit}
Virtual Reality wird nicht nur die Informatik in den nächsten Jahren beschäftigen. Neben den technischen Herausforderungen, die es zu lösen gilt, kann diese Technologie einen großen gesellschaftlichen Einfluss haben. Allerdings stehen wir hier noch ganz am Anfang und es bleibt abzusehen, wie die neuen technischen Möglichkeiten genutzt werden können, um die Gesellschaft voranzubringen und neue Wege der Wahrnehmung und Kommunikation zu ermöglichen.

VRClassroom ist angetreten um Virtual Reality in den Bildungsbereich zu bringen. Hier hat diese Technologie das Potenzial neue Wege des Lernens zu ermöglichen. Wie auch in den Gesprächen mit den Lehrkräften deutlich wurde, ist die Rolle der Lehrkraft aber essentiell für eine gute Ausbildung. Die Technik soll hierbei nur ein neues Werkzeug bieten. Für einen effektiven Einsatz der Technologie werden Lehrer benötigt, die die Kapazitäten haben sind sich mit innovativen Techniken auseinander zu setzen und dafür offen sind.

Neben einer guten technischen Lösung braucht es vor Allem gut aufbereitete Inhalte, deren Lizenz die Nutzung für den Unterricht ermöglicht. Die Produktion solcher Inhalte ist aufwändig und teuer, weshalb es im Moment noch wenige Inhalte gibt. Neue Hard- und Software-Systeme machen hier aber große Fortschritte die Produktion von Inhalten zu vereinfachen.

Es ist mit Sicherheit davon auszugehen, dass der Fortschritt auf Seiten der Headsets durch die Hersteller weiter schnell voranschreitet, so dass technische Limitierungen zunehmend weniger werden. Dadurch kann die Technik letztendlich billiger einem großen Publikum zur Verfügung stehen. Wichtig ist hier aber, dass die Schulen die finanziellen Mittel haben, ihre Infrastruktur zeitgemäß auszubauen und damit die Grundlage für eine digitale Bildungszukunft zu legen. Nur wenn die notwendige Infrastruktur in den Schulen zur Verfügung steht, können Zukunftstechnologien eingesetzt werden.

Als kostenlose Software bietet VRClassroom einen Lösungsansatz, der darauf ausgelegt ist, Schulen einen einfachen Einstieg zu ermöglichen, auch wenn keine gute digitale Infrastruktur vorhanden ist. VRClassroom ist sicherlich nur ein kleiner Anfang. Es ist aber als open-source-Software veröffentlicht, damit weitere Forschung und Entwicklung darauf aufbauen kann.

%______________________________________________________________________

\cleardoublepage
\fancyhead[LE,RO,LO,RE]{} % Keine Kopfzeile mehr oben auf jeder Seite

\section*{Anhang}
\subsection*{Fragebogen der Online-Umfrage}
Nachfolgend werden die Fragen der Online-Umfrage zu VRClassroom aufgeführt. Die Ergebnisse dieser Studie werden in \ref{subsec:evaluation} besprochen.

\begin{enumerate}
\item Die Bedienung von VRClassroom wirkt einfach und intuitiv. (Likert-Skala)

\item Ich glaube, dass meine Schüler 3-dimensionale Sachverhalte mit dem Einsatz von VRClassroom besser verstehen würden als ohne. (Likert-Skala)

\item Ich glaube ich hätte Spaß an der Nutzung von VRClassroom. (Likert-Skala)

\item Ich glaube, dass meine Schüler Spaß an der Verwendung von VRClassroom hätten. \\(Likert-Skala)

\item Das finde ich an VRClassroom nicht gut: (Freitextfeld)

\item Das finde ich an VRClassroom besonders gut: (Freitextfeld)

\item Folgende zusätzliche Funktionen würde ich mir für VRClassroom wünschen: (Freitextfeld)

\item Folgende Inhalte würde ich mir für VRClassroom wünschen: (Freitextfeld)

\item Das müsste sich ändern, damit ich VRClassroom sinnvoll im Unterricht einsetzen kann: (Freitextfeld)

\item Ich finde die Nutzung von 360°-Fotos hilfreich zur Vermittlung des Lehrstoffs. \\(Likert-Skala)

\item Ich finde die Nutzung von 360°-Videos hilfreich zur Vermittlung des Lehrstoffs. \\(Likert-Skala)

\item Ich finde die Nutzung von 3D-Modellen hilfreich zur Vermittlung des Lehrstoffs. \\ (Likert-Skala)

\item Ich finde das Setzen von Markern auf 360°-Fotos hilfreich. (Likert-Skala)

\item Ich finde das Setzen von Markern auf 360°-Videos hilfreich. (Likert-Skala)

\item Ich finde das Setzen von Markern auf 3D-Modellen hilfreich. (Likert-Skala)


\bigskip 

\textbf{Ich glaube der Einsatz von VRClassroom kann für diese Fächergruppe einen Mehrwert bringen...} (Likert-Skala)

\item ... Naturwissenschaftliche Fächer
\item ... Gesellschaftswissenschaftliche Fächer
\item ... Sprachen 
\item ... Religion/ Ethik/ Philosophie
\item ... Kunst/ Musik
\item ... Sport 

\bigskip 
\item Für folgenden Lehrstoff kann ich mir den Einsatz von VRClassroom besonders gut vorstellen: (Freitextfeld)

\bigskip 

\textbf{Ich glaube VRClassroom ist für folgende Klassenstufen geeignet...} (Likert-Skala)

\item ... Grundschule

\item ... Unterstufe

\item ... Mittelstufe

\item ... Oberstufe

\item ... Erwachsene


\bigskip 

\item Ich würde VRClassroom einem Kollegen weiterempfehlen:\\(0 = auf keinen Fall, 10 = auf jeden Fall)

\item Ich glaube, dass VR (virtual reality) zukünftig eine wichtige Rolle im Unterricht spielen wird. (Likert-Skala)

\item Ich kann mir vorstellen das VRClassroom-System im Unterricht zu verwenden. (Likert-Skala)

\item Wenn VRClassroom marktreif wäre, würde ich es durchschnittlich wie oft pro Klasse nutzen:
\begin{itemize}
\item Nie
\item 1x im Jahr
\item alle 3 Monate
\item 1x im Monat
\item 1x pro Woche
\item öfter
\end{itemize}

\item In folgenden Szenarien (innerhalb und außerhalb des Unterrichts) kann ich mir den Einsatz von VRClassroom vorstellen: (Freitextfeld)

\item Weitere Anmerkungen: (Freitextfeld)

\item Ich bin...?
\begin{itemize}
\item männlich
\item weiblich
\end{itemize}

\item  Ich bin in dieser Altersgruppe:
\begin{itemize}
\item unter 30
\item 30-39
\item 40-49
\item 50-59
\item über 60
\end{itemize}

\item  Ich bin technikaffin. (Likert-Skala)

\item Ich habe schonmal eine VR-Brille benutzt.
\begin{itemize}
\item nein
\item ja, folgende: (Freitextfeld)
\end{itemize}

\item  Ich unterrichte an dieser Schule:
\begin{itemize}
\item Grundschule
\item Mittelschule
\item Realschule
\item Gymnasium
\item Berufsschule
\item Berufoberschule
\item Fachoberschule
\item andere Schulform: (Freitextfeld)
\end{itemize}


\item Diese Medien nutze ich bisher im Unterricht: (Freitextfeld)
\end{enumerate}

\bigskip

\textbf{Verwendete Likert-Skala:}
\begin{itemize}
\item Ich stimme voll zu
\item Ich stimme etwas zu
\item Ich stimme eher nicht zu
\item Ich stimme überhaupt nicht zu
\end{itemize}



\clearpage

\section*{Inhalt der beigelegten CD}
Die beigelegte CD enthält folgende Dateien:

\begin{itemize}
\item Sourcecode von VRClassroom
\item Kompilierte VRClassroom App für MacOS, Linux und Windows
\item VRClassroom Video, das als Grundlage für Online-Umfrage diente
\item Thesis als Tex-Datei und PDF
\item Ergebnisse der Online-Umfrage
\end{itemize}

Das Github-Repository, in dem der Sourcecode von VRClassroom, die für MacOS, Linux und Windows kompilierten VRClassroom Apps sowie die Thesis sind ist zudem unter \mbox{https://github.com/vronifuchsberger/VRClassroom} erreichbar.
Das für die Umfragen produzierte Video zu VRClassroom ist auf Youtube unter \mbox{https://youtu.be/b60PQ3bqjk0} zu erreichen.

%______________________________________________________________________

\cleardoublepage

\bibliographystyle{IEEEtran}
\bibliography{literature}

\end{document}
