\documentclass[11pt,a4paper,twoside]{article}

\usepackage[T1]{fontenc} % sonst geht \hyphenation nicht mit Umlauten
\usepackage[latin1]{inputenc} % man kann schreiben äöüß, statt "a"o"u"s
%\usepackage[utf8]{inputenc} % wie oben, aber UTF-8 als Encoding statt ISO-8859-1 (latin1)
\usepackage[ngerman,english]{babel} % deutsche Trennregeln, "Inhaltsverzeichnis" etc.
%\usepackage{ngerman} % Alternative zum Babel-Paket oben
\usepackage{mathptmx} % Times-Roman-Schrift (auch für mathematische Formeln)
\usepackage{framed}
\usepackage{longtable}
\usepackage{tabu}


% Zum Setzen von URLs
\usepackage{color}
\definecolor{darkred}{rgb}{.25,0,0}
\definecolor{darkgreen}{rgb}{0,.2,0}
\definecolor{darkmagenta}{rgb}{.2,0,.2}
\definecolor{darkcyan}{rgb}{0,.15,.15}
\usepackage[plainpages=false,bookmarks=true,bookmarksopen=true,colorlinks=true,
  linkcolor=darkred,citecolor=darkgreen,filecolor=darkmagenta,
  menucolor=darkred,urlcolor=darkcyan]{hyperref}

% pdflatex: Bilder in den Formaten .jpeg, .png und .pdf
% latex: Bilder im .eps-Format
\usepackage{graphicx}
\usepackage{subcaption}
\usepackage{sidecap}

\usepackage{fancyhdr} % Positionierung der Seitenzahlen
\fancyhead[LE,RO,LO,RE]{}
\fancyfoot[CE,CO,RE,LO]{}
\fancyfoot[LE,RO]{\Roman{page}}
\renewcommand{\headrulewidth}{0pt}
\setlength{\headheight}{13.6pt} % behebt headheight Warning

% Korrektes Format für Nummerierung von Abbildungen (figure) und
% Tabellen (table): <Kapitelnummer>.<Abbildungsnummer>
\makeatletter
\@addtoreset{figure}{section}
\renewcommand{\thefigure}{\thesection.\arabic{figure}}
\@addtoreset{table}{section}
\renewcommand{\thetable}{\thesection.\arabic{table}}
\makeatother

\sloppy % Damit LaTeX nicht so viel über "overfull hbox" u.Ä. meckert

% Ränder
\addtolength{\topmargin}{-16mm}
\setlength{\oddsidemargin}{25mm}
\setlength{\evensidemargin}{35mm}
\addtolength{\oddsidemargin}{-1in}
\addtolength{\evensidemargin}{-1in}
\setlength{\textwidth}{15cm}
\addtolength{\textheight}{34mm}
%______________________________________________________________________

\begin{document}

\pagestyle{empty} % Vorerst keine Seitenzahlen
\pagenumbering{alph} % Unsichtbare alphabetische Nummerierung

\begin{center}
\textsc{Ludwig-Maximilians-Universität München}\\
Department ``Institut für Informatik''\\
Lehr- und Forschungseinheit Medieninformatik\\
Prof.\ Dr.\ Heinrich Hußmann

\vspace{5cm}
{\large\textbf{Masterarbeit}}\vspace{.5cm}

{\LARGE Entwicklung eines Systems zur Nutzung von VR-Brillen im Unterricht}\vspace{1cm}

{\large Veronika Fuchsberger}\\\href{mailto:veronika.fuchsberger@campus.lmu.de}{veronika.fuchsberger@campus.lmu.de}

\end{center}
\vfill

\begin{tabular}{ll}
Bearbeitungszeitraum: & 01. 08. 2018 bis 30. 01. 2018\\
Betreuer: & Christoph Krichenbauer\\
Verantw. Hochschullehrer: & Prof. Heinrich Hußmann
\end{tabular}
%______________________________________________________________________

\clearpage
\section*{Zusammenfassung}
In der folgenden Arbeit wurden aktuelle VR-Headsets verglichen und nach Preissegmenten und Anwendungsfunktionen eingeordnet. Zudem wurde bisher existierende Software, die Virtual Reality für den Schulunterricht zugänglich macht diskutiert und bewertet und auf die Challenges eingegangen, die Virtual Reality-Systeme generell und im Bezug auf die Nutzung im Schulunterricht momentan haben.
Weiterhin wurde in der Arbeit auf die verschiedenen Inhalte für VR-Systeme eingegangen, welche Quellen es für diese Inhalte gibt und wie sie bearbeitet und selbst erstellt werden können.

Anschließend wurde basierend auf den zuvor erörterten Anforderungen an eine Software, die Lehrkräfte im Unterricht nutzen können, um 360°-Inhalte zu zeigen, VRClassroom entwickelt. Ein zwei-teiliges System, mit dem der Lehrer in eine App auf seinem Computer Inhalte hineinladen kann, die dann in einer Web-App auf VR-Brillen gezeigt werden. Die Lehrkräfte führen dabei das komplette VR-Erlebnis für die Schüler und können 360°-Fotos und -Videos hineinladen, Markierungen setzen und das Video synchronisiert für alle abspielen und pausieren. Außerdem können auch 3D-Modelle in die App geladen werden, die die Lehrkraft dann je nach Bedarf skalieren und drehen kann und an interessanten Stellen Markierungen setzen kann. Um zu sehen, welche Geräte verbunden sind, wird in der Lehrer-App ein Liste mit allen Geräten mit einem Aktivitätsindikator angezeigt. 

Anschließend wurde eine Online-Befragung mit Lehrkräften durchgeführt, bei der die Teilnehmer ein Video gezeigt wurde, das die Nutzung von VRClassroom erklärt, zu dem sie nachfolgend einige Fragen beantworteten. Die Mehrheit der Teilnehmer war dem System gegenüber sehr aufgeschlossen und konnte sich gut vorstellen VRClassroom zukünftig im Unterricht zu verwenden, um 360°-Inhalte zu zeigen.

Abschließend wurde diskutiert wie Zukunft von Virtual Reality im Schulunterricht aussehen könnte und wie VRClassroom weiterentwickelt werden könnte, um die Bedürfnisse von Lehrkräfte noch besser zu erfüllen beziehungsweise welche weiteren Funktionen das System noch interessanter machen würden.

\selectlanguage{english}
\section*{Abstract}

Short abstract of the work, maximum of 250 words.

\selectlanguage{ngerman}
\clearpage


\vfill % Sorgt dafür, dass das Folgende an das Seitenende rutscht

\noindent Ich erkläre hiermit, dass ich die vorliegende Arbeit
selbstständig angefertigt, alle Zitate als solche kenntlich gemacht
sowie alle benutzten Quellen und Hilfsmittel angegeben habe.

\bigskip\noindent München, \today

\vspace{4ex}\noindent\makebox[7cm]{\dotfill}

%______________________________________________________________________

\cleardoublepage
\pagestyle{fancy}
\pagenumbering{roman} % Römische Seitenzahlen
\setcounter{page}{1}

% Inhaltsverzeichnis erzeugen
\tableofcontents

%Abbildungsverzeichnis erzeugen - normalerweise nicht nötig
%\cleardoublepage
%\listoffigures
%______________________________________________________________________

\cleardoublepage

% Arabische Seitenzahlen
\pagenumbering{arabic}
\setcounter{page}{1}
% Geändertes Format für Seitenränder, arabische Seitenzahlen
\fancyhead[LE,RO]{\rightmark}
\fancyhead[LO,RE]{\leftmark}
\fancyfoot[LE,RO]{\thepage}

\section{Einleitung}
VR-Headsets immer mehr verbeitet, es gibt immer mehr Content
Umfrage, wie viele Leute schon VR-Headsets benutzt haben?
Statistiken zu:
Medien im Unterricht
Erfolg neuer Lehrkonzepte bzgl neue Medien


%______________________________________________________________________

% Der Befehl \cleardoublepage erscheint nur vor \section, nicht vor
% den "kleineren" Gliederungsbefehlen wie \subsection!
\cleardoublepage % Neue rechte Seite anfangen
\section{Existierende VR-Hardware-Systeme}
Es gibt inzwischen eine große Zahl verschiedener VR-Hardware-Systeme, die sich in drei Gruppen mit unterschiedlichen Anwendungsszenarien unterteilen: Die Computer-gestützten VR-Systeme, die Stand-alone VR-Systeme und die Smartphone-gestützten VR-Systeme.

\subsection{Merkmale von VR-Systemen}
Um die verschiedenen VR-Hardware-Systeme einordnen zu können, gibt es einige Merkmale, auf die ein Käufer achten sollte. Denn die verschiedenen Headsets sind für unterschiedliche Anwendungsszenarien entwickelt worden, sodass ein potenzieller Käufer zuerst entscheiden sollte wie er das VR-Headset einsetzen möchte.

Diese Merkmale können dazu herangezogen werden: Degrees of Freedom (DoF), die Displaygröße und -auflösung, die Frequenz des Displays, Field of View, Rechenleistung und Gewicht des Headsets, die verwendete Tracking-Methode, die mitgelieferten Controller beziehungsweise mögliche Input-Methoden, das Gewicht des Headsets und der Verkaufspreis.

\subsubsection{Degrees of Freedom}
Das Konzept der Degrees of Freedom, kurz DoF, entspringt ursprünglich der Mechanik. Wie Gans in seiner Arbeit ``Engineering dynamics: From the lagrangian to simulation'' erläutert, hat ein Körper grundlegend sechs Freiheitsgrade. Die Bewegungen in x-Richtung, y-Richtung und z-Richtung. Hinzu kommen dann noch die Rotationen um die jeweiligen Achsen. Kurz beschreibt er Freiheitsgrade als die minimale Anzahl an Variablen um ein System zu spezifizieren. \cite{Gans2013}

Freiheitsgrade wurden zudem viel genutzt um die Bewegungen von maritimen Fahrzeugen zu benennen: Die Bewegung in x-Richtung wird ``surge'' genannt, in y-Richtung ``sway'' und in z-Richtung ``heave''. Die Rotation um die x-Achse heißt ``roll'', um die y-Achse ``pitch'' und um die z-Achse ``yaw''. Diese Bezeichnungen beziehen sich dabei auf ein Schiff, das in x-Richtung ausgerichtet ist. \cite{Fossen1995}

Wie in Grafik \ref{fig:dof} zu erkennen, sind die Degrees of Freedom für VR-Headsets aus diesen Definitionen abgeleitet. Unterstützt ein System nur die Rotationsbewegungen um die drei Achsen wird es als ein 3DoF-System bezeichnet, kann es auch die Bewegungen im Raum verarbeiten ist es ein 6DoF-System. Um ein 6DoF VR-Gerät zu entwickeln, wird also positional Tracking der Nutzer benötigt. 

\begin{figure}[ht]
  \centering
  \includegraphics[width=\textwidth]{images/dof.eps}
 \caption{Degrees of Freedom eine Virtual Reality Headsets.}
  \label{fig:dof}
\end{figure}

\subsubsection{Display, Frequenz und Field of View}
Maße, Auflösung, 2 einzelne/ ein großes
Vielleicht Display und Frequenz zu einem Punkt verbinden?
Spalten zwischen Pixeln?

Bildwiederholungsrate:
- was ist nötig, dass es gut aussieht
- was ist in Headsets verbaut?

Was ist Field of View?
Theorie: Was ist notwendig, dass immersion funktioniert?
Was ist verbaut?

Augenabstand IPD


\subsubsection{Rechenleistung}
Generell lässt sich sagen, je höher die Rechenleistung, desto aufwändigere VR-Szenen können dargestellt werden. Deshalb können (zumindest momentan) stand-alone Geräte und Smartphone-gestützte Geräte nicht mit den Computer-gestützten konkurrieren, was die Kombination aus Display-Auflösung, Bildwiederholungsrate und Field of View angeht.
Da die Rechenleistung bei Computer- und Smartphone-gestützten VR-Headsets allerdings vom benutzen Rechner beziehungsweise Smartphone abhängt, ist die Rechenleistung nur bei den stand-alone Geräten ein Merkmal, das berücksichtigt werden muss.

Fast alle hier aufgeführten Geräte haben dabei den gleichen Prozessor eingebaut, den Qualcomm Snapdragon 835, nur die Oculus Go hat einen Qualcomm Snapdragon 821. Damit sind die anderen Geräte etwa 30\% schneller und verbrauchen dabei ungefähr 40\% weniger Energie als die Oculus Go.

Quellen:
https://vrodo.de/lenovo-mirage-solo-test-besser-als-oculus-go/
https://vrodo.de/oculus-quest-vs-oculus-go-vergleich-das-sind-die-unterschiede/
https://www.vive.com/cn/product/vive-focus-en/


\subsubsection{Inputmethoden}
Wie in Kapitel XXX (hier Ref einfügen) beschrieben, gibt es drei grundsätzliche Input-Methoden für VR-Headsets: Input über Controller mit verschiedenen Buttons und einem ``Raycaster'' in der VR-Welt, das Auslösen von Touch-Events auf dem Display des VR-Headsets und Voice-Input. 

Da nur solche Headsets Touch-Events auf dem Display registrieren, die ein Smartphone als Display nutzen, sind es ebenso nur diese, die Input durch Touch-Interaktionen mit dem Display als Input erlauben. Das sind vor Allem Google Cardboards, die eine Art ``Arm'' nutzen um auf das Display zu drücken oder einige Nachbauten des ursprünglichen Google Cardboards, die Magneten verwenden um ein Touch-Event auf dem Display auszulösen.
(Diese Technik noch genauer erläutern?) Bei beiden Möglichkeiten ist es allerdings nur möglich an einer bestimmten Stelle ein Touch-Event auszulösen, sie müssen also in Verbindung mit einem Art Gaze-Punkt verbunden werden, um dem Nutzer sinnvollen Input zu erlauben.

Gaze Input? (-> EyeVR paper)

Bisher gibt es keine Geräte, die mit Voice-Input kontrolliert werden können. Allerdings wäre das sehr wünschenswert, da damit die Nutzung von VR-Systemen auch für Menschen mit körperlichen Behinderungen möglich wird.

\subsubsection{Tracking}
Trackingmethode: kein tracking/ inside out/ Sensoren im Raum

nur relevant für Rechner-gestützt und stand-alone


\subsubsection{Gewicht}
Was wiegen Geräte
was ist Ziel, das sich nicht schwer anfühlt?


\subsubsection{Preis}


Die Preise sind in Dollar, wie sie auf dem amerikanischen Markt zu kaufen sind. Darin sind keine Umsatzsteuern enthalten.



\subsubsection{Liste Quellen etc}
% HTC Vive: https://www.vive.com/de/product/?gclid=EAIaIQobChMIgvauiYfA3wIVBcYYCh3oUwkIEAAYASAAEgLADfD_BwE#vive-spec
Augenabstand:	Einstellung der Pupillendistanz und des Objektivabstands
Sensoren:	SteamVR Tracking, G-Sensor, Gyroskop, Nähesensor
Controller:	Multifunktions-Trackpad, Greifknöpfe, zweistufiger Abzug, Systemknopf, Menütaste
Outside Tracking, Kamera in Headset
% Preis: https://store.eu.vive.com/store?Action=DisplayPage&Locale=de_DE&SiteID=htcemea&id=ThreePgCheckoutShoppingCartPage

Oculus Rift: https://www.vrbound.com/headsets/oculus/rift
Preis: Oculus.com/rift

Playstation VR: https://www.vrbound.com/headsets/sony/playstation-vr, https://www.playstation.com/de-de/explore/playstation-vr/tech-specs/

HTC Vive Focus: https://www.vive.com/cn/product/vive-focus-en/

Oculus Go: https://www.vrbound.com/headsets/oculus/go
Sensoren: Orientational Tracking
has no IPD adjustment


Oculus Quest: https://uploadvr.com/oculus-quest-specs-price-release-date/
has an IPD adjustment
Oculus says Quest?s weight isn?t locked in just yet, but presently it?s about 100 grams heavier than the Rift, which would put it around 570 grams

Mirage Solo: https://www.lenovo.com/us/en/virtual-reality-and-smart-devices/virtual-and-augmented-reality/lenovo-mirage-solo/Mirage-Solo/p/ZZIRZRHVR01
Preis 400\$
General Usage: 2.5 hours
Dimensions: 204 mm x 269.5 mm x 179.86 mm
Sensoren: P-Sensor, Gyroscope, Accelerometer, Magnetometer
Controller: 3DoF Daydream Motion Controller

% Google Cardboard: https://store.google.com/product/google_cardboard

% Samsung Gear VR: https://www.samsung.com/de/wearables/gear-vr-r324/SM-R324NZAADBT/?cid=de_ppc_google_im-wearables-gearvr-q3restructured_20180720_samsungvr-broad&tmcampid=7&tmad=c&tmplaceref=c_DE_IMECOM_Warm_Brand_GearVR_Broad&tmclickref=b_%2Bsamsung%20%2Bvr&gclid=EAIaIQobChMIpLnFnuvC3wIVCKQYCh0XVweYEAAYASAAEgLilfD_BwE

% Google Daydream View: https://store.google.com/product/google_daydream_view_specs


Vergleich: https://www.vrnerds.de/vr-brillen-vergleich/
https://upload-magazin.de/blog/25071-standalone-vr-headsets/

\begin{table}[]
%\begin{tabular}{l|l|l|l|l|l}
\begin{tabu} to \textwidth {XXXXXX}
\textbf{VR-Headset} & HTC Vive & Oculus Rift & Playstation~VR & HTC Vive \newline Focus & Oculus Go \\
\hline
\textbf{DoF} & 3Dof? & gut &hoch & weit & 3DoF   \\
\textbf{Display} & 1080 x 1200 Pixel pro Auge (2160 x 1200 Pixel zusammen), Dual AMOLED 3,6?? diagonal & 2160x1200, 3.5in, OLED  & 5.7in, OLED, 1920x1080 & weit & 2560x1440  \\
\textbf{Frequenz} & 90 Hz & 90 hz & 120 hz & weit & 60-72Hz   \\
\textbf{Field~of~View} & 110° & 100 ° & 100 ° & weit & 100°  \\
\textbf{Inputmethoden } & 3Dof? & Oculus Touch, Xbox One Controller &hoch & weit & Oculus Go Controller  \\
\textbf{Tracking} & 3Dof? & Accelerometer, Gyroscope, Magnetometer & Accelerometer, Gyroscope & weit & orientational  \\
\textbf{Gewicht} & h & 380-470g & 610g & weit & 468g  \\
\textbf{Preis} & 599 Euro & 449 Euro &hoch & weit & 199 Euro 
\end{tabu}
%\end{tabular}

\bigskip

\begin{tabu} to \textwidth {XXXXXX}
\textbf{VR-Headset} & Oculus Quest & Lenovo \newline Mirage Solo & Google \newline Cardboard & Samsung \newline Gear VR & Google \newline Daydream  \\
\hline
\textbf{DoF} & 6 DoF & 6DoF &hoch & weit & sechs  \\
\textbf{Display} & 1600 x 1440 per eye, OLED  & gut & 4 bis 6 Zoll, hängt von genutztem Smartphone ab & hängt von genutztem Smartphone ab & hängt von genutztem Smartphone ab  \\
\textbf{Frequenz} & 72Hz & gut & hängt von genutztem Smartphone ab & hängt von genutztem Smartphone ab & hängt von genutztem Smartphone ab  \\
\textbf{Field~of~View} & 3Dof? & gut & abhängig von Smartphone & 101° & abhängig von Smartphone \\
\textbf{Inputmethoden } & 3Dof? & gut & Touch-Taste an Cardboard & Gear VR Controller & Daydream Controller \\
\textbf{Tracking} & 3Dof? & gut & abhängig von Smartphone & Beschleunigungssensor, Lagesensor, Annäherungssensor + abhängig von Smartphone & abhängig von Smartphone   \\
\textbf{Gewicht} & 3Dof? & 645g & 96 g & 345g & 261 g  \\
\textbf{Preis} & 399 Dollar& 400 Dollar  & 20 Euro & weit & hoch  

\end{tabu}
  \caption{Übersicht  der Merkmale der verschiedenen VR-Headsets}~\label{tab:tableage}
\end{table}





\subsection{Computer-gestützte VR-Hardware}
Die erste Gruppe sind die Computer-gestützten Hardware-Systeme: Sie sind mit einem Kabel mit dem Rechner verbunden, der die Rechenleistung für die eigentlich Brille übernimmt. Sie eignen sich besonders für extrem rechenaufwändige Anwendungen, aber schränken durch ihre Kabel die Bewegungsfreiheit des Nutzers ein.

\subsubsection{Oculus Rift}
\subsubsection{HTC Vive und HTC Vive Pro}
\subsubsection{Playstation VR}


\subsection{Stand-alone VR-Hardware}
Die zweite Gruppe sind die Stand-alone Hardware-Systeme wie etwa die Oculus Go, Oculus Quest oder Google Daydream. Es handelt sich hierbei um vollumfängliche Systeme, die ohne weiteres Equipment auskommen und auch keinen Computer benötigen. Da sie mit einem Akku betrieben werden, ist die Betriebsdauer allerdings eingeschränkt und auch die Rechenleistung ist deutlich geringer als die der Computer-gestützten Systeme.

\subsubsection{Oculus Go}
\subsubsection{Oculus Quest}
\subsubsection{Lenovo Mirage Solo}
\subsubsection{HTC Vive Focus}

\subsection{Smartphone gestützte VR-Hardware}
Die dritte Gruppe bilden die Smartphone-gestützten VR-Systeme. Damit sind alle Systeme gemeint, bei denen das Smartphone integriert wird, um die VR-Inhalte zu zeigen. Diese liegen preislich auf einem sehr niedrigen bis mittlerem Niveau und bilden so eine gute Möglichkeit für alle Leute in die Welt der VR-Systeme einzutauchen, ohne direkt mehrere Hundert Euro ausgeben zu müssen.

\subsubsection{Google Cardboard}
\subsubsection{Samsung Gear VR}
\subsubsection{Google Daydream View}

\cleardoublepage % Neue rechte Seite anfangen

\section{Aktuelle Einsatzgebiete von VR}
medizin
Gaming
Forschung
verschiedene Therapien zB Angsttherapien

\subsection{3D in VR}
WebVR macht das schon.
Dadurch muss die Szene doppelt gerendert werden.
Wie entsteht 3d generell?
Augenabstand
\subsection{Eingabemethoden in VR}
Controller
Voice Control

Input schon bei Geräten besprochen?

\subsection{VR-Systeme im Bildungsbereich}
Google Expeditions
Hier kommen noch mehr Sachen aus den Papern rein!

Paper: Virtual reality for collaborative e-learning 2008

% TODO!

\cleardoublepage % Neue rechte Seite anfangen
\section{Aktuelle Challenges an VR-Systemen}
5 Major Challenges in VR: https://channels.theinnovationenterprise.com/articles/5-major-challenges-of-vr-industry

1. Preis
	-> inziwschen für alle Preisklassen was dabei
2. Fehlende Inhalte
	-> wir langsam, aber immer noch ein Problem
3. fehlende Businessmodelle
	-> hängt irgendwie mit 5. zusammen: Idee: Pakete von Inhalten kaufen? zB 5. Klasse Biologie
4. Folgen für Gesundheit?
	-> siehe Mindestalter, evtl noch ein weiteres Kapitel zu Gesundheitsprobleme generell?
5. VR nur as gimmick bekannt
	-> Facebook arbeitet an VR-Meetingräumen


Hier wird gerade besonders daran geforscht oder das gibt es noch nicht, ist aber sehr gewünscht.
\subsection{Virtual Reality Sickness}
https://vrodo.de/virtual-reality-laut-umfrage-haben-60-prozent-probleme-mit-vr-uebelkeit/
Umfrage auf reddit: https://www.strawpoll.me/12015620/r
Unterschied zu motion sickness
Symptome
Lösungen?

Paper: Psychometric evaluation of the Simulator Sickness Questionnaire as a measure of cybersickness

https://www.ncbi.nlm.nih.gov/pubmed/6847562
https://www.twentymilliseconds.com/post/all-about-motion-sickness/
http://fortune.com/2018/02/06/virtual-reality-motion-sickness/
https://www.iflscience.com/technology/turns-out-answer-virtual-reality-sickness-right-front-your-face/


\subsection{Mindestalter zur Nutzung von VR-Systemen}
Fast alle Hersteller geben in ihren Nutzungsbedingungen oder Sicherheitsanweisungen ein Mindestalter für die Benutzung ihrer VR-Systeme an. Wie in ~\ref{tab:table1} zu sehen, sind sich die Hersteller sehr einig, dass VR-Headsets nicht für kleine Kinder geeignet sind und frühestens für Jugendliche in Frage kommen.
\bigskip

Oculus weist konkret darauf hin, dass eine Nutzung ihrer Geräte unter 13 Jahren ihren Nutzungsbedingungen widerspricht und diese erst für diese Altersgruppe entwickelt sind.  ``The Services are intended solely for users who are aged 13 or older. Any registration for, or use of, the Services by anyone under the age of 13 is unauthorised, unlicensed and in breach of these Terms.'' Es werden allerdings keine genauen Gründe für diese Altersrestriktion angegeben.
  ~\cite{FacebookTechnologiesLLC2018}

Samsung geht dabei noch einen Schritt weiter und warnt vor einer Nutzung unter 13 Jahren, da sich jüngere Kinder in einer ``critical period in visual development''  ~\cite{SAMSUNG} befinden. Zudem sollen auch Kinder über 13 Jahren nur unter Aufsicht einer erwachsenen Person die Gear VR benutzen und dabei darauf achten regelmäßig Pausen zu machen. Eine lange Nutzung soll generell vermieden werden und die Kinder sollen während und nach der Nutzung beobachtet werden, ob sich ihre Fähigkeiten in der Hand-Augen-Koordination, Balance oder Multi-Tasking verschlechtern.

Außerdem wird eine Liste an Symptomen aufgeführt bei deren Anzeichen eine Nutzung sofort unterbrochen werden soll. Das sind: ``seizures, loss of awareness, eye strain, eye or muscle twitching, involuntary movements, altered, blurred, or double vision or other visual abnormalities, dizziness, disorientation, impaired balance, impaired hand-eye coordination, excessive sweating, increased salivation, nausea, lightheadedness, discomfort or pain in the head or eyes, drowsiness, fatigue, or any symptoms similar to motion sickness'', also verschiedenste Probleme beim Sehen und an den Augen sowie Probleme der Konzentration, Koordination und Balance.   ~\cite{SAMSUNG}

HTC dagegen gibt für die Nutzung der HTC Vive beziehungsweise Vive Solo kein genaues Mindestalter an. Sie geben allerdings an, dass das Gerät nicht dafür ausgelegt ist von kleinen Kindern genutzt zu werden. Sie warnen davor, dass Kinder Kleinteile verschlucken könnten oder sich und Andere auf anderem Wege damit verletzen können. Für ältere Kinder empfehlen sie die Aufsicht einer erwachsenen Person und dass die Nutzungszeit nicht zu lang ist. \cite{HTC2016}

Zudem wird für die Nutzung der HTC Vive ein HTC Account benötigt, der laut HTC erst ab 14 Jahren erlaubt ist. \cite{HTCCorporation}


In ihren FAQs gibt Sony an, dass man zur Nutzung ihrer Playstation VR Konsole mindestens zwölf Jahre oder älter sein sollte. Weitere Angaben oder Gründe dieses Mindestalter sind auch hier nicht zu finden. \cite{SonyEntertainmentLLC2017}
\bigskip

Gegenüber all der Warnungen der Geräte-Hersteller gibt Martin Banks, Professor of Optometry, Vision Science, Psychologie, and Neuroscience an der University of California in Berkeley in einem Interview im Frühling 2016 an, dass er ``no concrete evidence that a child of a certain age was somehow adversely affected by wearing a VR headset,'' [keine konkreten Beweise, dass ein Kind in einem gewissen Alter durch das Tragen von VR-Brillen negativ beeinflusst wurde] gefunden hat. Er ist überzeugt, dass die Hersteller der VR-Headsets die Nutzung durch Kinder ausschließen, um sicher sein zu können, dass nicht später bekannt werdende Probleme bei Kindern, die VR-Headsets nutzen, ihnen angelastet werden können. 

Weiter gibt er an, dass die Angst, dass die Entwicklung des Auges negativ beeinflusst wird im Gegensatz zur Nutzung von Büchern oder Smartphones viel unproblematischer ist, das durch die in die VR-Brillen eingebauten Optiken das Auge gar nicht auf eine so nahe Sache fokussiert, sondern auf weiter entfernte und somit keine Schäden der Augen nach sich zieht. 

Banks sieht als Gefahren lediglich die gleichen, die auch für Erwachsene bestehen: Das sind hauptsächlich Virtual Reality Sickness, auch bekannt als Cybersickness, und die Gefahr mit Personen oder Gegenständen im Raum zu kollidieren, während das VR-Headset getragen wird.  Ansonsten bewertet er die Nutzung der VR-Brillen von Kindern unproblematisch. \cite{Hill2016}

\bigskip

Momentan gibt es keine veröffentlichten Forschungsarbeiten zu den Gefahren für Kinder bei der Nutzung von VR-Brillen, dagegen sind viele Arbeiten zu finden, die VR-Systeme in der Therapie von verhaltensauffälligen, lernverzögerten oder behinderten Kindern erfolgreich einsetzen. Es gibt beispielsweise Arbeiten, die .... (HIER NOCH GUTE BEISPIELE RAUSSUCHEN)


\begin{table}[]
\begin{tabular}{p{0.2\linewidth}|p{0.2\linewidth}|p{0.5\linewidth}}
\textbf{VR-Gerät} & \textbf{Mindestalter} & \textbf{Weitere Angaben} \\ \hline
Oculus Rift \newline Oculus Go \newline  Oculus Quest & 13 Jahre & keine  \\
HTC Vive \newline HTC Vive Solo & keine genaue Altersangabe & HTC Account erst ab 14 Jahren  \\
Google Daydream & 13 Jahre & keine  \\
Samsung Gear VR & 13 Jahre & nur unter Aufsicht eines Erwachsenen  \newline regelmäßig Pausen machen \newline Warnung vor einer Vielzahl an Symptomen aus dem Bereich Koordination, Balance und Sehen \\
Playstation VR & 12 Jahre & keine  \\
Google Cardboard & keine Angabe & nur unter Aufsicht eines Erwachsenen
\end{tabular}
  \caption{Übersicht der verschiedenen VR-Headsets mit ihren jeweiligen Nutzungsmindestaltern}~\label{tab:tableage}
\end{table}




\cleardoublepage % Neue rechte Seite anfangen
\section{360°-Inhalte}
Neben der verwendeten Hardware spielen die verfügbaren Inhalte für das Virtual Reality Erlebnis eine erhebliche Rolle. Nur wenn die gezeigten Inhalte richtig in der 360°-Umgebung dargestellt werden bringt die Darstellung Nutzung von VR-Headsets die gewünschte Immersion.

Für 360°-Inhalte gibt es keine speziellen Dateiformate, die Inhalte werden in den gewohnten Formaten gespeichert. Grundsätzlich gibt es fünf verschiedene Medientypen von 360°-Inhalten: 360°-Fotos, -Videos, 3D-Modelle und spatial Audio, auch bekannt als 3D Audio. 

\subsection{Projektionen von 360°-Fotos und -Videos}
Um 360°-Bildmaterial in 2D Bildformaten zu speichern, werden sie auf verschiedene Arten ``zerschnitten'' oder verzerrt, um dann zur richtigen Darstellung wieder zusammengesetzt werden zu können.
Das Panorama wird dazu auf ein zwei-dimensionales Bild gemappt, um problemlos in den bekannten Formaten speicherbar zu sein. Die Information, wie das Panorama gemappt wurde, wird dann in den Metadaten der Datei gespeichert und 360°-fähige Software kann mit diesen Informationen die Panoramas dann wiederum richtig darstellen.

Die Projektionen wurden ursprünglich dazu entwickelt Weltkarten drucken zu können, auf denen die gesamte Erdkugel zu erkennen ist. Daher sind sie auch als ``map projections'' bekannt.

Im Folgenden werden die bekanntesten und am weitesten in der 360°-Szene verbreiteten Projektionen erläutert und verglichen:

\subsubsection{Equirectangulare Projektion}
\subsubsection{Rectilineare Projektion}
\subsubsection{Cylindrische Projektion}
\subsubsection{Mercator-Projektion}
\subsubsection{Circulare Projektion}
\subsubsection{Pannini/ Vedutismo-Projektion}
\subsubsection{Stereographische Projektion}
\subsubsection{Cubemap/ Skybox-Projektion}
 == cubic?
 
 
 
% https://en.wikipedia.org/wiki/Skybox_(video_games)
https://explorationjunkie.com/panoramic-image-projections/
http://hugin.sourceforge.net/docs/manual/Projections.html

\subsection{360°-Fotos}
Google Maps projection: Mercator: https://setcompass.com/GoogleMapsProjection.htm

\subsubsection{Quellen für 360°-Fotos}
Es gibt bisher keine Plattform, die sich darauf spezialisiert hat 360°-Fotos, die in der Lehre genutzt werden können, anzubieten. 

Die verbreiteten Anbieter für Stockphotos wie zum Beispiel Shutterstock oder iStockphoto bieten zwar bereits 360°-Fotos an, aber die meisten dieser Stockphotos zeigen keine relevanten Inhalte, sondern nur eine schöne Landschaft oder einen Raum. Sie sind also für zur Nutzung im Unterricht nicht geeignet. Zudem sind diese Bilder oft teuer oder mit einem Abo beim jeweiligen Anbieter verbunden.

Die aktuell beste Quelle für 360°-Fotos ist die Foto-Sharing-Plattform Flickr, auf der die Urheber das Herunterladen ihrer Bilder erlauben und auch die Nutzungslizenz dafür angeben können. Eine Suche nach ``equirectangular'' und der Creative Commons Lizenz ist dort einfach möglich und man wird schnell fündig. 
Ein engagierter Lehrer hat sogar eine Gruppe gegründet, die speziell dafür gedacht ist, dass Lehrkräfte aus aller Welt ihre 360°-Fotos teilen können, damit Andere sie auch in ihrem Unterricht nutzen können.

Flickr-Group: https://www.flickr.com/groups/360images4schools/
Flickr Search equirectangluar + Creative Commons: % https://www.flickr.com/search/?text=equirectangular&license=2%2C3%2C4%2C5%2C6%2C9


Google Photo Sphere Community: https://plus.google.com/communities/115970110085205516914/stream/abbc1e71-8239-4ab0-9de3-3f6429e7681f
Download nicht immer möglich

www.360cities.net
Realtiv teuer, dafür tolle Fotos


https://www.airpano.com
Teuer, hat auch Videos

\subsubsection{Erstellung und Bearbeitung von 360°-Fotos}
Metadaten wichtig: https://www.panotwins.de/technical/how-to-add-mandatory-photo-sphere-meta-data-to-an-equirectangular-image/
% http://u88.n24.queensu.ca/~bogdan/#m_workspace
https://developers.google.com/streetview/spherical-metadata

https://vuze.camera/vr-software/
Vuze 360 Kameras

https://www.360rize.com/vr/
360 rize Kamera

https://theta360.com/de/

SAMSUNG Gear 360 360-Grad-Kamera
https://www.samsung.com/de/wearables/gear-360-r210/

https://www.panono.com/en/home/
Panono 360p Kamera

Ansonsten: Racks zusammenbauen mit mehreren Kameras

\subsection{360°-Videos}
Aufhänger?
https://filmpuls.info/360-videos-zukunft/

Youtube 360 Videos
https://youtube-eng.googleblog.com/2017/03/improving-vr-videos.html

Vimeo:
https://vimeo.com/blog/post/introducing-vimeo-360

\subsubsection{Quellen für 360°-Videos}
Vimeo:
Videobesitzer kann es zum download anbieten

Youtube 360 Videos:
offiziell nicht runterladbar
Aber Videos in Smartphone App + Cardboard anschaubar, Apps auf Plattformen: Oculus (Youtube VR), Youtube VR in SteamVR ( HTC Vive), 

https://vr.zdf.de
Aber nicht runterladbar -> nicht für VRClassroom geeignet

www.360cities.net und https://www.airpano.com

https://de.videoblocks.com/videos/footage/360-files
verschiedenste Videos (aber wieder eher Stockmaterial, das nicht direkt verwendbar ist)


\subsubsection{Erstellung und Bearbeitung von 360°-Videos}
https://de.wikipedia.org/wiki/360-Grad-Video

Entweder direkt 360 Kamera oder mehrere und dann stitchen (stitchen besser bei statischen Inhalten)

\subsection{3D-Modelle}
\subsubsection{Quellen für 3D-Modellen}
% https://www.turbosquid.com/Search/Index.cfm?FuseAction=SEOTokenizeSearchURL&stgURlFragment=3D-Models/free
https://www.cgtrader.com/3d-models
https://sketchfab.com/models/popular
https://free3d.com

\subsubsection{Erstellung und Bearbeitung von 3D-Modellen}
3D-Modelling Software:
Autodesk 3ds Max
Blender
Cinema 4D
Paint 3D (on Windows 10)

\subsection{360° Sound/ Spatial Audio}
https://flypaper.soundfly.com/produce/wtf-vr-and-spatial-audio-mixing-360-audio/
https://www.youtube.com/watch?v=mcUwiYwgBHw
https://medium.com/@superavi/goodbye-stereo-hello-360º-sound-6a9d64fbd1f

https://facebook.github.io/react-360/docs/audio.html

https://developers.google.com/vr/ios/spatial-audio
Audioquellen im Raum platzieren (Mono-Soundquelle)
oder als ambisonic background sound (zB Regen/ Meeresrauschen etc)

https://creator.oculus.com/learn/spatial-audio/
Was ist spatial audio?
Content creation

https://sonicscoop.com/2018/02/05/audio-mixing-for-vr-the-beginners-guide-to-spatial-audio-3d-sound-and-ambisonics/

http://www.bbc.co.uk/guides/zrn66yc

https://www.vrtonung.de/spatial-audio-support-for-360-video-platform/

Paper: Def von spatial Audio

\subsubsection{Quellen für Spatial Audio-Dateien}
\subsubsection{Erstellung und Bearbeitung von Spatial Audio}
\subsection{Virtual Reality Spiele und Anwendungen}


\subsubsection{Quellen für Games}
HTC: SteamVR Store
Listings für alle Geräte: https://www.wearvr.com/
Oculus devices: https://www.oculus.com/experiences
Playstation VR Store: https://store.playstation.com/de-de/grid/STORE-MSF75508-PLAYSTATIONVR/1
WebVR: https://itch.io/games/tag-webvr, 

\subsubsection{Erstellung und Bearbeitung von Spielen und Anwendungen}
% https://www.instavr.co/?gclid=EAIaIQobChMIxY_KroDm3wIVT4uyCh35XQs5EAAYASAAEgIT9fD_BwE
VR Apps erstellen

https://site.vizor.io/products
Vizor 360
Tool, um interactive 360 experiences ohne Programmieren zu erstellen
Einfach Fotos hochladen, miteinander verbinden, personalisieren, fertig!

https://playcanvas.com
Web-basierte Game Engine
damit kann man auch VR-Anwendungen erstellen
kollaborativ
WebGl editor im Browser mit live updates auf mehreren Geräten -> schnelles Testen auf Geräten möglich
open-source
hat mobile browser support (sogar auf iPhone 4s) und schnelle Ladezeiten (durch script concatenation, minifaction, deferred loading nicht essentieller assets und andere)


Unity: C\#
Unreal Engine: C++, Blueprints
% -> https://www.reddit.com/r/learnVRdev/comments/65cvxo/how_to_start_making_vr_games_for_beginners_what/

mobile VR can be JS or native
WebVR is JS
Google Cardboard: Android

\subsection{360°-Inhalte in 3D}
https://vuze.camera/camera/vuze-xr-camera/
180 in 3D oder 360 in 2D

https://www.aerofotografie.de/3d-360-grad-video-produktion/
Wie funktioniert das?
https://www.aerofotografie.de/360-grad-3d-kamera/
Welche Kameras gibt es?

https://patents.google.com/patent/US7463280B2/en

\subsection{360°-Inhalte in React360}
Photos+Videos: https://facebook.github.io/react-360/docs/photos-and-videos.html
Audio: https://facebook.github.io/react-360/docs/audio.html
3D-Modelle: https://facebook.github.io/react-360/docs/objects.html


%\_____________________________________________________________________

\cleardoublepage
\section{Software Entwicklung von VR-Systemen}
\subsection{Native VR-Applikationen}
Unity: C\#
Unreal: C++
\subsection{WebVR + Mobile VR: VR im Browser}
Was ist WebVR?
https://webvr.info
(https://en.wikipedia.org/wiki/WebVR)
Request a list of the available VR devices.
Checks to see if the desired device supports the presentation modes the application needs.
If so, application advertises VR functionality to the user.
User performs an action that indicates they want to enter VR mode.
Request a VR session to present VR content with.
Begin a render loop that produces graphical frames to be displayed on the VR device.
Continue producing frames until the user indicates that they wish to exit VR mode.
End the VR session.

getPose liefert die Position, Ausrichtung, Bewegung und weitere Daten über den Nutzer
Web Audio API erlaubt es, dreidimensionale Klangeindrücke zu erzeugen

Browser-Unterstützung!
https://webvr.rocks

Was ist Mobile VR?
auf Smartphones und Standalone-Geräten
https://www.slashgear.com/mobile-vr-what-it-needs-to-succeed-08530006/

\subsubsection{A-Frame}
https://aframe.io/docs/0.8.0/introduction/

Web (Three.js) Framework für VR
ursprünglich von Mozilla entwickelt, jetzt open-source
basiert auf html
entity componetn framework
deklarative, erweiterbare und zusammensetzbare Struktur auf Basis von three.js

A-Frame nutzt den DOM, aber Updates von 3D-Objekten werden alle im Speicher mit kleinem Overhead mit einem einzigen requestAnimationFrame Call berechnet. Dadurch sind 90+ FPS Apps möglich

Kompatibel mit fast alles Libraries und Frameworks, auch: React, Preact, Vue.js, d3.js, Ember.js, jQuery

Hat einen integrierten visuellen 3D inspector (<ctrl> + <alt> + i, um in der Szene umzusehen)

Viele Core-Components: geometries, materials, lights, animations, models, raycasters, shadows, positional audio, text, and Vive / Touch / Windows Motion / Daydream / GearVR / Cardboard controls. 
Get even further with community components such as particle systems, physics, multiuser, oceans, mountains, speech recognition, motion capture, teleportation, super hands, and augmented reality

Unterstützt Headseats: Vive, Rift, Windows Mixed Reality, Daydream, GearVR, Cardboard und im Web. Auch AR

kann positional tracking und unterstützt Controller
\subsubsection{React360}
\subsubsection{Vergleich von A-Frame und React360}
Warum für React360 entschieden?
\subsubsection{Three.js}
\subsubsection{WebGL}
Sollte ich das vielleicht auch noch erwähnen?

%\_____________________________________________________________________



\cleardoublepage
\section{Software Projekt: VRClassroom}
Aufbauend auf den Erkenntnissen aus ANDEREN KAPITELN wurde im Rahmen dieser Arbeit VRClassroom entwickelt. VRClassroom ist ein zweiteiliges System, das es Lehrkräften ermöglichen soll 360°-Fotos, -Videos und 3D-Modelle im Unterricht zeigen zu können, dabei die Anzeige auf allen Geräten steuern zu können und trotzdem den Überblick in der Klasse behalten zu können. 

VRClassroom bietet dafür die Lehrer-App, die auf einem Rechner läuft und die Schüler-App, die über den Browser der mobilen Geräte beziehungsweise VR-Headsets erreicht werden kann.

Die Lehrer-App ist eine Electron-App, die wiederum eine React-App für das Interface enthält und die Schüler-App startet. Die Schüler-App ist eine React360-Applikation. Der Code von VRClassroom ist also komplett in Javascript gehalten, nur unterschieden durch die verschiedenen Libraries Node.js, React.js, React360. Im Anschluss an diese Arbeit wird VRClassroom unter einer Creative Commons Lizenz veröffentlicht und so kann ein Entwickler schnell alle Teile des Codes verstehen ohne sich in mehrere Programmiersprachen einarbeitenzu müssen.

Grafik \ref{fig:electron}

\begin{figure}[ht]
  \centering
  \includegraphics[width=\textwidth]{images/electron.eps}
 \caption{Aufbau von Electron-Appliaktionen.}
  \label{fig:electron}
\end{figure}

\subsection{Nutzungsszenario und Anwendungsfokus}
VRClassroom wurde speziell für die Nutzung in der Schule entwickelt.
Dabei kann die Lehrkraft das VR-Erlebnis für die SchülerInnen führen und ihnen so komplexe 3-dimensionale Inhalte besser vermitteln und eine spannende neue Art des Lernens zu erleben. 
Außerdem wurde Funktionen wie etwa die Seitenleiste mit den verbundenen Geräten entwickelt, die es der Lehrkraft erleichtern sollen VRClassroom in einer Schulklasse einzusetzen und sicher zu stellen, dass alle Geräte verbunden sind und die Inhalte angezeigt werden.

Da viele Schulen noch überhaupt keine Ausrüstung an VR-Headsets haben, lag der Fokus besonders auf der Nutzung des VRClassroom Systems mit einem Smartphone in einem Google Cardboard. Allerdings wurde es bewusst so entwickelt, dass auch mit ``echten'' VR-Headsets das System problemlos weiter genutzt werden kann. So wird den Schulen ermöglicht mit einer sehr geringen Investition zu testen, ob es für sie in Frage kommt und können zu einem späteren Zeitpunkt zu einem elaborierteren Hardware-System wechseln ohne auf neue Software umsteigen zu müssen.

Weitere Einsatzszenarien für VRClassroom könnten Besprechungen im Arbeitsumfeld gehen, bei denen es sich um plastische Inhalte handelt wie zum Beispiel Architekturbüros, Designagenturen oder Landschaftsgärtner. Mit einem 360°-Foto oder -Video könnte der Ist-Zustand besprochen werden und anschließend die Entwürfe in 3D-Modellen vorgeführt werden. Dadurch könnten sich Kunden besser in die Entwürfe hineinversetzen und bewusster entscheiden, was sie letztendlich haben möchten.

\subsection{Anwender}
Da an Schulen Lehrkräfte und Schülerinnen und Schüler verschiedenster Altersstufen und Leveln an Technikaffinität zu finden sind sollte die Software

\begin{itemize}
  \item Hauptszenario: Lehrer und Schüler im Klassenzimmer
  \item mögliche andere Szenarien: Lerngruppen, Workmeetings
  \item Anwender: Lehrer und Schüler aller Fachrichtungen und mit unterschiedlichsten Skillleveln
  \item deshalb muss es sowohl für Lehrer als auch für Schüler möglichst intuitiv und einfach zu bedienen sein!
  \item Augenmerk bei der Entwicklung lag besonders darauf
\end{itemize}

\subsection{Grundstruktur}
Das VRClassroom System besteht aus zwei verschiedenen Applikationen: Zum einem der Schüler-App, die auf den VR-Systemen läuft mit denen die Schüler sehen können, was der Lehrer ihnen präsentiert, und die Lehrer-App, in der die Lehrkraft die verschiedenen Inhalte hineinladen, Markierungen auf die Inhalte setzen kann und und sehen kann, welche Schüler-Geräte aktuell verbunden sind.

Da viele Lehrer in ihrer Ausbildung oft nicht mit vielen neuen Technologien in Berührung gekommen sind, sondern es gewohnt sind mit den ``klassischen'' Medien zu arbeiten, lag der Augenmerk bei der Entwicklung darauf, dass das benötigte technische Verständnis möglichst niedrig ist und auch Personen, die sich selbst als nicht technikaffin bezeichnen würden, keinerlei Probleme bei der Nutzung haben. Gleiches gilt selbstverständlich auch auf der Seite der Schüler. Da diese aber hauptsächlich passiv agieren, standen hier die Lehrkräfte im Mittelpunkt der Aufmerksamkeit.

Grafik \ref{fig:architecture}

\begin{figure}[ht]
  \includegraphics[width=\textwidth]{images/architecture1.eps}
 \caption{Aufbau des VRClassroom Systems.}
  \label{fig:architecture}
\end{figure}

\subsection{Lehrer-Applikation}
Die Lehrer-Applikation besteht aus einer Electron-App, die in zwei logische Teile zerlegt ist. Das ist zum einen der main-Prozess, der es erlaubt Zugriffe auf das File-System des Rechners zu machen und für rechenaufwändige Hintergrundprozesse genutzt wird, und zum Anderen der render-Prozess. Der render-Prozess ist der Teil des Programms, das die Lehrkraft letztendlich auf ihrem Bildschirm sieht.

Die Lehrer-App enthält genau genommen zwei render-Prozesse: Die teacher-App, in der die Lehrkraft alle verbundenen Geräte sehen kann und verschiedene Inhalte hineinladen kann, und die student-App, die als iFrame in die teacher-App eingebunden ist und auf den Geräten der Schülern läuft.

Die teacher-App enthält außerdem noch den QR-Code Generator, der in einem zweiten Fenster geladen wird.

Um eine Liste der verbundenen Geräte zu halten und Veränderungen der Inhalte auf die Schüler-Geräte zu synchronisieren, startet die teacher-App einen Websocket-Server, mit dem sich alle Schüler-Geräte verbinden. 

\subsubsection{Verbundene Geräte}
Wie in der Grafik zu sehen hält die teacher-App eine Liste mit allen verbundenen Geräte dieser Session. Sind die Geräte gerade aktiv, werden sie mit einem grünen Icon dargestellt, sind sie inaktiv, mit einem Roten.

Das soll der Lehrkraft erleichtern zu überprüfen, ob die Schüler den gezeigten Stoff verfolgen oder sich anderweitig beschäftigen.

Haben die Schüler bereits einen Namen eingegeben, wird dieser in der Liste angezeigt. Ist dies nicht der Fall wird aus dem user-agent versucht möglichst genau zu schließen, um welches Gerät es sich handelt, sodass der Lehrer zumindest einschränken kann, um welchen Schüler bzw welche Schülerin es sich handeln könnte. 

NEU: Bei Modellen und Videos der loading-Status der einzelnen Geräte

\bigskip

Ein User-Agent kann zum Beispiel wie folgt aussehen: 
\begin{framed}
Mozilla/5.0 (iPhone; CPU iPhone OS 5\_0 like Mac OS X) AppleWebKit/534.46 (KHTML, like Gecko) Version/5.1 Mobile/9A334 Safari/7534.48.3
\end{framed}

Daraus lässt sich schließen, dass es sich um eine iPhone handelt, welches Safari benutzt, um die Schüler-App zu laden. Die Oculus Geräte hingegen geben in ihrem User-Agent an, den Oculus Browser zu verwenden und sind so auch gut von den anderen verbundenen Geräten zu unterscheiden. Da aber hier nicht ersichtlich wird welches Oculus-Gerät es genau ist wird nur ``Oculus device'' angegeben und nicht genauer spezifiziert, ob es sich dabei um eine Go, Quest oder Rift handelt.
 %hier Grafik connected clients einfügen% 

\begin{itemize}
  \item startet Websocket Server
  \item generiert QR-Code
  \item Lehrer kann verschiedene Apps starten: 
  \begin{itemize}
  	\item 360° Photos und 3D-Modelle anzeigen und Markierungen einfügen
	\item 360° Videos synchronisiert auf allen Geräten zeigen und play/pause aus Lehrer-App steuern
	\item Streetview photos von locations laden und anzeigen (?)
  \end{itemize}
  \item hält history der zuvor gezeigten Inhalte, um sie vereinfacht wieder anzuzeigen 
 \end{itemize}

\subsubsection{Hineinladen von Inhalten}

 
\subsubsection{Controls}
Wie in Grafik \ref{fig:controls} zu sehen sind die Controls, mit denen Aktionen des aktuell geladenen Inhalts ausgelöst werden können durch einen Overlay über dem iFrame der React360-App dargestellt. 

Für jeden der drei Typen an Inhalten, 360°-Fotos, -Videos und 3D-Modelle unterscheiden sich die Bedienelemente und Funktionen, das Setzen einer Markierung ist allerdings bei allen Medientypen gegeben.

\begin{figure}
  \includegraphics[width=0.9\linewidth]{images/controls-overlay.png}
  \caption{Die Kontrollleiste als Overlay über den iFrame der React360-App.}
  \label{fig:controls}
\end{figure}

\paragraph{Marker}
\label{subsec:marker}

\begin{SCfigure}
  \centering
  \caption{Die leuchtende Farbe und das Drehen der Marker erhöht die Erkennbarkeit.}
  		\includegraphics[width=0.4\textwidth]{images/marker.png}
	\label{fig:marker}
\end{SCfigure}

Soll ein Marker gesetzt werden, muss zuerst in den ``Markierung setzen''-Modus gewechselt werden, indem in der Control Bar auf ``Marker setzen'' geklickt wird. Danach kann an beliebiger Stelle eine Markierung gesetzt werden.

Um die 3D-Koordinaten des Markers zu bekommen wird mit Hilfe von Mouseposition und Fenstergröße ein Strahl berechnet, der aus der 2D-Koordinate der Mouseposition ausgeht. 
Bei 360°-Fotos und -Videos wird die Entfernung zur Kamera auf einen festen Wert gesetzt, der so gewählt wurde, dass er gerade noch im Zylinder der Welt liegt. Mit der Entfernung zur Kamera lässt sich die 3D-Koordinate dann leicht berechnen.

Um Markierungen auf 3D-Modellen setzen zu können muss ein Schnittpunkt des 3D-Modells mit dem virtuellen Strahl berechnet werden. 
(Soll das wirklich rein?) In React360 gibt es momentan keine Möglichkeit auf diese Weise auf die in der 3D-Welt platzierten 3D-Modelle zuzugreifen, sodass

Ist der Schnittpunkt des Strahls mit dem 3D-Modell berechnet, wird derjenige Schnittpunkt als Koordinate verwendet, der am nächsten zur Kamera ist. An dieser Stelle wird dann der Marker platziert. Zudem werden die Marker invers zur Nähe zur Kameraposition skaliert, sodass sie immer in der gleichen Größe dargestellt werden, egal wo am Modell sie gesetzt sind.

Das verwendete Modell erinnert wie in Grafik \ref{fig:marker} zu sehen an Stecknadeln, sodass Nutzern die Bedeutung direkt verständlich ist. Um die Markierungen leichter erkennbar zu machen sind sie in einer leuchtenden Farbe und drehen sich, sodass die Aufmerksamkeit direkt auf die markierte Stelle gelenkt wird.



\paragraph{Photo Controls}
Ist ein Foto geladen beinhaltet die Kontrollzeile nur den Dateinamen des 360°-Fotos und die Buttons zum Setzen von Markierungen, die wie bereits beschrieben bei allen Medientypen gegeben sind. Grafik \ref{fig:photocontrols} zeigt die Kontrollleiste für 360°-Fotos.

\begin{figure}[h]
  \includegraphics[width=\linewidth]{images/photocontrols.png}
  \caption{Die Kontrolleiste für 360°-Photos.}
  \label{fig:photocontrols}
\end{figure}


\paragraph{Video Controls}
Wird ein Video gezeigt, hat die Lehrkraft mehrere unterschiedliche Funktionen in der Kontrollleiste: 
Wie in Grafik \ref{fig:videocontrols} zu sehen sind die Elemente ähnlich wie bei bekannten Videoplayern wie Quicktime oder Netflix gehalten. Ganz links ein kombinierter Play/Pause-Button, der für alle verbundenen Geräte synchronisiert das Abspielen beziehungsweise Pausieren des Videos auslöst, daneben die aktuelle Abspielzeit, gefolgt von einem Slider, der grafisch die aktuelle Position im Video darstellt. Außerdem kann mit dem Slider zu anderen Zeitpunkten im Video gesprungen werden.
Rechts daneben wird die Gesamtdauer des Videos angezeigt. Der letzte Video-spezifische Button ist der Sound-Button, der im aktiven Zustand auf alle Geräten den Ton des Videos abspielt. Der Sound-Button ist standardmäßig deaktiviert. 

Zudem sind wiederum die Buttons für das Setzen von Markierungen vorhanden. Sie sind während dem Abspielen des Videos deaktiviert und können nur genutzt werden, wenn das Video pausiert ist. Wird das Video dann wieder weiter abgespielt, werden alle gesetzten Markierungen automatisch zurückgesetzt.

\begin{figure}[h]
  \includegraphics[width=\linewidth]{images/videocontrols.png}
  \caption{Die Kontrolleiste für 360°-Videos.}
  \label{fig:videocontrols}
\end{figure}

\paragraph{Model Controls}
Wie in Grafik \ref{fig:modelcontrols} zu sehen ist auch in der Kontrollleiste bei 3D-Modellen ein Slider vorhanden. Dieser kann sowohl zum Drehen des Modells als auch zum Skalieren benutzt werden. Dazu sind links vom Slider die Buttons ``Drehen'' und ``Skalieren'' mit denen zwischen den zwei Funktionalitäten des Sliders gewechselt werden kann. Der aktive Modus wird durch die blaue Farbe dargestellt.

Auch auf 3D-Modellen können Markierungen gesetzt werden. Dabei ist zu beachten, dass nur auf dem Modell eine Markierung gesetzt werden kann. Wird außerhalb des 3D-Modells geklickt, wird keine Markierung gesetzt.

\begin{figure}[h]
  \includegraphics[width=\linewidth]{images/modelcontrols.png}
  \caption{Die Kontrolleiste für 3D-Modelle.}
  \label{fig:modelcontrols}
\end{figure}
 
\subsection{Kommunikation zwischen Lehrer-App und Schüler-App}
WebSocket Server from Teacher App -> Schüler Gerät meldet sich bei Laden der URL an
Teacher App antwortet mit aktuellem Content
Teacher App schickt bei jeder Änderung ein komplettes Content-Objekt (einige Einträge ggf. leer)
Nachrichten von Schüler Geräten: visibilitychange, loading, Name
Nachrichten von Teacher-App: Jede Veränderung des Inhalts, URL zu Inhalt

\subsection{QR-Code Fenster}
Das QR-Code Fenster ist ein zweites Browserfenster, das aus dem main-Prozess der Electron App auf dem Lehrer-Computer gestartet wird. Es zeigt, wie in \ref{fig:qrcode} zu sehen, einen QR-Code, den die SchülerInnen mit ihren Smartphones scannen können, um bequem die URL zu laden, auf der sie die Schüler-Applikation erreichen können. Der QR-Code wird dynamisch beim Öffnen der App generiert.

Für Geräte, die keine Kamera haben oder wenn das Scannen des QR-Codes fehlschlägt, wird zudem unterhalb des QR-Codes die URL angezeigt, unter der die Schüler-Applikation zu erreichen ist. 

Der QR-Code wird in diesem extra Fenster generiert und angezeigt, damit die Lehrkraft dieses Fenster auf einem Beamer anzeigen kann, um den SchülerInnen den Zugang zur Schüler-App ohne umständliches URL-Abtippen zu ermöglichen.

\begin{figure}
  \includegraphics[width=0.8\linewidth]{images/QR-Code.png}
  \caption{Qr-Code Fenster.}
  \label{fig:qrcode}
\end{figure}

%% HIER SCREENSHOT QR-Code Fenster einfügen

\subsection{Schüler-App}
In der VRClassroom App nehmen die Schüler eine passive Rolle ein und können selbst nicht in der 3D-Welt navigieren. 

Die WebApp für die Schüler ist eine React360-App, sie stellt eine 3D-Welt da, in deren Mitte sich die Kamera, also der Viewport der Geräte, befindet. Sie stellt die in der Lehrer-App hineingeladenen 360°-Fotos und Videos in einer Kugel um den Viewport da, sodass die Nutzer sich in alle Richtungen umsehen können. 3D-Modelle werden in kurzer Entfernung vor dem Viewport angezeigt, als befänden sich sich im Raum vor der Person.

Grafik \ref{fig:react360}

\begin{figure}[ht]
  \includegraphics[width=\textwidth]{images/react360.eps}
 \caption{Struktur der React360 App von VRClassroom.}
  \label{fig:react360}
\end{figure}

%% Screenshot VRClassroom App?

\subsubsection{Eingabe des Namens}
Läd ein Gerät zum ersten Mal die VRClassroom-App wird dem Nutzer neben der Begrüßung, die immer angezeigt wird bevor Inhalte hineingeladen werden, eine Tastatur im VR-Raum angezeigt. Damit können die Schüler ihren Namen eingeben, der dann in der Liste der verbundenen Geräte angezeigt wird. Wird ein Smartphone als VR-Headset genutzt, muss die Tastatur mittels der Toucheingabe auf dem Display gemacht werden, bevor die WebVR Ansicht geladen wird. VR-Headsets, die einen Controller haben, können den Namen mit dem Controller auf der Tastatur tippen. 

Damit die Schüler nicht jedes Mal wieder ihren Namen eingeben müssen, wird der eingegebene Name im Browser des Schüler-Gerätes gespeichert und beim erneuten Laden der VRClassroom Applikation direkt wieder an die Lehrer-App übermittelt.

%% Screenshot VRClassroom App mit Tastatur + Greeting

\subsubsection{Anzeigen von 360°-Fotos}
Um 360°-Fotos in der 3D-Szene anzuzeigen lädt die React360-App das Foto von dem Link, den die Teacher-App geschickt hat und setzt sobald das gesamte Foto geladen wurde das Bild als Hintergrund der Szene. Gleichzeitig wird bei erfolgreichem Ladens des Fotos eine Nachricht an die Teacher-App geschickt, um dem Status des Geräts von ``loading'' wieder auf ``active'' zu setzen.
Das 360°-Bild wird dazu von innen an die Kugel in der die React360-Szene ist projiziert und ergibt so durch seine equirectangulare Projektion ein unverzerrtes 360°-Bild.

Das Bild wird angezeigt bis VRClassroom von der Lehrkraft beendet wird oder etwas anderes in die App hineingeladen wird.

\subsubsection{Abspielen von 360°-Videos}
Sendet die Teacher-App die Nachricht, dass ein Video angezeigt werden soll, startet die React360-App sofort mit dem Laden des Videos und lädt soviel von dem Video wie es geht. 
Ist eine Zeit des Videos vorgeladen, sodass React360 davon ausgeht, dass das Video ruckelfrei abgespielt werden kann, sendet wird wiederum die Nachricht, dass das Gerät aus dem ``loading''-Status wieder auf ``active'' gesetzt wird.
Ähnlich wie bei 360°-Fotos wird das 360°-Video dann als Hintergrund-Video der Szene gesetzt, um als 360°-Video abgespielt werden zu können

Die React360-App spielt das Video nicht automatisch ab, sobald genug geladen ist, sondern wartet auf das Signal der Teacher-App, um das Abspielen zu starten. Alle Funktionen werden erst ausgeführt wenn das Signal der Teacher-App kommt, die Funktion auszuführen. Zu den Funktionen zählen: Play, Pause, Springen zu einer anderen Stelle im Video und den Ton des Videos abspielen.

Falls ein Schüler-Gerät sich verspätet verbindet oder ein Paket verloren gegangen ist, sendet die Teacher-App jede Sekunde eine neue Nachricht in der immer der Link zum aktuellen Video und die aktuelle Abspielposition enthalten ist. Unterscheidet sie sich mehr als eine Sekunde von der Abspielposition in der React360-App wird zu der Abspielposition, die in der Nachricht steht gesprungen.

\subsubsection{Abspielen von Ton in Videos}
Da die meisten Browser das automatische Abspielen von Videos mit Ton verbieten, werden 360°-Videos von React360 standardmäßig beim erstellen des Videoplayer-Komponenten ``muted'' auf ``true'' gesetzt. Die Hersteller wollen dadurch die Ablenkungen, die beim surfen auf den Nutzer zukommen, abmildern. Erst wenn der Nutzer ein ``user gesture click'' also einen Klick auf der Website gemacht hat, darf die Tonspur automatisiert abgespielt werden. Wird versucht ein Video mit Ton abzuspielen, ohne dass ein Klick gemacht wurde, wird der Videoplayer blockiert und kann nicht mehr abspielen. \cite{Decker2017}

Damit React360 in einer single-threaded Umgebung wie einem Webbrowser flüssig ablaufen kann und nicht durch ``blocking behavior'' irgendeiner Art das Rendern unterbrochen wird, ist eine React360-App in zwei Teile aufgeteilt: Die React-Applikation und den Code, der die React Komponenten in 3D Elemente auf dem Bildschirm umwandelt. Die App selbst läuft in einem Webworker, einem anderen Prozess als der des Hauptbrowserfensters. \cite{FacebookInc.2018}

Das führt dazu, das die React360-Elemente nicht als html-Elemente gelten und ein Click-Event auf einem VRButton nicht als Interaktion zählt, um die Erlaubnis zu haben Ton abzuspielen.

Um dieses Hindernis zu umgehen ist nun ein durchsichtiger, Bildschirm-füllender Button über die React360-App gelegt, der bei einem Klick verschwindet, um die Erlaubnis vom Browser zu bekommen Ton abzuspielen. Wurde der Button am Schüler-Gerät geklickt wird ein flag gesetzt, dass die App Ton abspielen darf. Ob dann tatsächlich Ton beim Video abgespielt wird, kann der Lehrer aus der teacher-App einstellen. Auch dort ist das Abspielen von Ton an den Schüler-Geräten standardmäßig erst einmal abgestellt, damit der Lehrer entscheiden kann, ob er nur den Ton aus dem Rechner über Boxen für Alle abspielen möchte oder aus jedem Schüler-Gerät einzeln der Ton kommen soll.

\subsubsection{Anzeigen von 3D-Modellen}
Wird die Nachricht empfangen, dass der ``mediatype'' auf ``model'' gesetzt wurde, wird als Hintergrund der Standardhintergrund gesetzt, wie er auch angezeigt wird, bevor Medien von der Lehrkraft in die App geladen wurden. Das Laden und Darstellen der 3D-Modelle passiert in der ModelView. Hier wird zuerst analysiert, ob es sich um eine gltf-Datei oder eine .obj-Datei handelt. Valide gltf-Dateien haben die Endung .gltf oder .glb, ein Container-Dateiformat, das alle Texturen und Styling-Dateien enthält. Handelt es sich um eine .obj-Datei wird versucht eine gleichnamige .mtl-Datei zu laden. Diese enhält alle Informationen über Texturen und Materialien, die .obj-Datei ist allein das Modell.

Im Anschluss wird die Datei geladen und angezeigt. Damit die Modelle zu erkennen sind und nicht schwarz gerendert werden wird zudem eine Punktlichtquelle installiert, die das Modell von oben rechts beleuchtet. Außerdem wird das Modell mit einem leichten Ambientlight versehen, damit auch Teile des Modells, die sonst im Schatten liegen, erkennbar werden.

Auf die ähnliche Weise werden die Marker gerendert, die von der Lehrkraft gesetzt werden können, wie bereits in ref{subsec:marker} beschrieben. Die Marker haben keine Punktlichtquelle, sondern nur ein Ambientlight. Sie werden zudem nicht an eine feste Position, sondern an die von der Lehrkraft ausgewählt Stelle in der VR-Szene gerendert und ihre Größe wird zudem in Abhängigkeit ihrer Entfernung zur Kamera skaliert.

\subsubsection{WebVR Polyfill}
WebVR ist eine Javascript-API, um Virtual Reality-Inhalte im Browser anzuzeigen. \cite{WebVR} 

Wie in Grafik \ref{fig:WebVRfig}a links zu sehen wird auf einer Web-App, die mit WebVR angesehen werden kann ein Icon in der unteren rechten Ecke des Fensters zu angezeigt, mit dem dann die WebVR-Ansicht geladen werden kann.

Wie Grafik \ref{fig:WebVRfig}b zeigt teilt WebVR dafür den Bildschirm in zwei Bilder für die Linsen in einem VR-Headset wie zum Beispiel dem Google Cardboard auf. Die zwei Bilder sind dabei nicht Bildschirm-füllend, sondern in einer annähernd ovalen Form, die von einem schwarzen Rand umgeben wird, sodass sie gut auf die Linsen passen. Durch die Krümmung der Linsen wird daraus dann eine drei dimensionale 360°-Welt, in der der Nutzer sich umsehen kann.

\begin{figure}[h!]
  \centering
  \begin{subfigure}[b]{0.4\linewidth}
    \includegraphics[width=\linewidth]{images/WebVR1.png}
    \subcaption{``View in VR'' Button in VRClassroom.}
  \end{subfigure}
  \begin{subfigure}[b]{0.4\linewidth}
    \includegraphics[width=\linewidth]{images/WebVR2.png}
    \subcaption{VR-Szene mit WebVR Ansicht.}
  \end{subfigure}
  \caption{VR-Szene mit und ohne WebVR Ansicht.}
  \label{fig:WebVRfig}
\end{figure}

Bisher ist weder in Chrome noch in Safari WebVR standardmäßig unterstützt. In Chrome auf Android kann es durch setzen der ``WebVR''- und ``Gamepad-Extension''-flags auf ``enabled'' eingeschaltet werden.  \cite{Jones2017}
In Safari ist es bisher gar nicht möglich WebVR zu nutzen.

Da aber VRClassroom auf allen mobilen Geräten funktionieren soll und möglichst auch keine Einstellungen auf der Nutzerseite benötigen soll, wurde ein Polyfill benutzt, um die WebVR Funktionen für die VRClassroom-App auf allen Geräten nutzen zu können. 
Dafür 

%______________________________________________________________________
\cleardoublepage

\section{Nutzerstudie und Evaluation}


\subsection{Ursprünglich geplanter Ablauf der Studie}
Da das System speziell für die Nutzung im Schulunterricht entwickelt wurde, sollte auch die Nutzerstudie in diesem Szenario durchgeführt werden. Dafür wurden Lehrkräfte angefragt, die dann einen Teil ihrer Unterrichtszeit für einen Test der Software verwenden wollten und im Anschluss bereit waren einen kurzen Fragebogen dazu zu beantworten.

Insgesamt hatten sich 15 Lehrkräfte bereit erklärt mit ihrer Klasse das VRClassroom System auszuprobieren und im Anschluss einen Fragebogen auszufüllen. 

\bigskip

Für den Test sollte allen SchülerInnen ein Cardboard zur Verfügung gestellt werden, das die Kinder mir ihrem eigenen Smartphone als VR-Brille genutzt sollten. 
Der Rechner, auf dem das Programm installiert war, sollte den Lehrkräften bereitgestellt und den Lehrkräften im Vorfeld eine kurze Einführung in die Software gegeben werden. Dies ist trotz Absage der Studie durch ein Video geschehen. Das Video wurde zudem für die Online-Umfrage benutzt. 

\bigskip


Für die Durchführung der Studie war ein Zeitrahmen von 15 bis 20 Minuten veranschlagt. In dieser Zeit sollte die Lehrkraft die App starten, sich alle Schüler-Geräte damit verbinden und die Lehrkraft die eigentlichen Inhalte präsentieren.

Damit alle Studienteilnehmer, Lehrer und Schüler gleichermaßen, die gleichen Erlebnisse haben und sich zu allen Funktionen des Systems eine Meinung bilden können, hatte jede Lehrkraft ein vollständiges Set an Inhalten mit jeweils mindestens einem 360°-Fotos, einem 360°-Video und einem 3D-Modell.

\subsubsection{Freigabe Kulturministerium}
Da alle Studien, die an Schulen gemacht werden, einer Freigabe des Kultusministeriums bedürfen, wurde im Vorfeld der Studie beim Kultusministerium eine solche Freigabe beantragt. Für den Antrag werden alle Fragebögen und der gesamte Ablauf der Studie an das Kultusministerium zur Prüfung vorgelegt. Das Kultusministerium hat die Durchführung von Studien beschränkt, um den Datenschutz der Schülerinnen und Schüler zu sichern. Da allerdings in der Studie nur die Lehrkräfte befragt werden und die Fragebögen komplett anonym gehalten sind, sind die Daten der Schüler zu keinem Zeitpunkt in Gefahr.

Leider wurde der Antrag nie beantwortet, sodass die gesamte Studie kurzfristig abgesagt werden musste. Es hatten sich bereits 15 Lehrkräfte bereit erklärt mit ihrer Klasse das VRClassroom System auszuprobieren und im Anschluss den Fragebogen zu beantworten, die sehr an dem System interessiert waren. Da das VRClassroom-System speziell für den Einsatz im Unterricht entwickelt wurde, wäre eine Nutzerstudie im Unterricht sehr sinnvoll gewesen, um abschätzen zu können, ob es im aktuellen Zustand einen guten Mehrwert bietet oder noch Anpassungen braucht um gut im Unterrichtsablauf zu funktionieren.

\subsection{Qualitative Nutzerstudie}
\subsubsection{Fragebogen}
\subsubsection{Erkenntnisse}

\subsection{Online-Umfrage}
Um die Meinung möglichst vieler verschiedener Lehrkräfte zum entwickelten VRClassroom System zu erfahren, wurde neben der qualitativen Studie, in der die Studienteilnehmer das System ausprobieren und im Anschluss Feedback geben konnten, noch eine Online-Befragung von Lehrkräften in Bayern durchgeführt. 

Dafür wurde ein Video produziert, welches die Idee und Nutzung von VRClassroom erklärt und die einzelnen Funktionen zeigt, die die App anbietet. Im Anschluss haben die teilnehmenden Lehrkräfte einen kurzen Fragebogen ausgefüllt.

Alle Fragen waren optional gestellt, konnten also wenn gewünscht unbeantwortet bleiben, sodass davon ausgegangen werde kann, dass die gegebenen Antworten alle der Wahrheit entsprechen und nicht einfach eine der möglichen Antworten gewählt wurde, weil eine nötig war.


Am Ende der Befragung wurden noch einige personenbezogene Fragen gestellt.
Es wurde nach dem Geschlecht, dem Alter (innerhalb von Altersgruppen) und der Schulart, an der die Person unterrichtet gefragt.
Zudem wurden die Teilnehmer gefragt, ob sie sich selbst als technikaffin bezeichnen würden und ob sie schon einmal eine VR-Brille benutzt haben.
Als Folgefrage sollte dann noch angegeben werden, welche VR-Brille schon einmal ausprobiert wurde.

Insgesamt haben an der Befragung 57 Lehrkräfte teilgenommen, wobei das Geschlechterverhältnis fast ausgewogen war.
48\% der teilnehmenden Lehrkräfte gaben an weiblich zu sein und 52\% männlich. 4 Personen haben die Frage nicht beantwortet.

Zudem waren die 


Antworten der offenen Fragen einbeziehen!

Grafik \ref{fig:survey1}

\begin{figure}[ht]
  \includegraphics[width=\textwidth]{images/survey1.eps}
 \caption{Ich brauche noch einen sinnvollen Text.}
  \label{fig:survey1}
\end{figure}


Grafik \ref{fig:survey2}

\begin{figure}[ht]
  \includegraphics[width=\textwidth]{images/survey2.eps}
 \caption{Ich brauche noch einen sinnvollen Text.}
  \label{fig:survey2}
\end{figure}


Grafik \ref{fig:survey3}

\begin{figure}[ht]
  \includegraphics[width=\textwidth]{images/survey3.eps}
 \caption{Ich brauche noch einen sinnvollen Text.}
  \label{fig:survey3}
\end{figure}


Grafik \ref{fig:survey4}

\begin{figure}[ht]
  \includegraphics[width=\textwidth]{images/survey4.eps}
 \caption{Ich brauche noch einen sinnvollen Text.}
  \label{fig:survey4}
\end{figure}


Grafik \ref{fig:survey5}

\begin{figure}[ht]
  \includegraphics[width=\textwidth]{images/survey5.eps}
 \caption{Ich brauche noch einen sinnvollen Text.}
  \label{fig:survey5}
\end{figure}


Grafik \ref{fig:survey6}

\begin{figure}[ht]
  \includegraphics[width=\textwidth]{images/survey6.eps}
 \caption{Ich brauche noch einen sinnvollen Text.}
  \label{fig:survey6}
\end{figure}


Grafik \ref{fig:survey7}

\begin{figure}[ht]
  \includegraphics[width=\textwidth]{images/survey7.eps}
 \caption{Ich brauche noch einen sinnvollen Text.}
  \label{fig:survey7}
\end{figure}


Grafik \ref{fig:survey8}

\begin{figure}[ht]
  \includegraphics[width=\textwidth]{images/survey8.eps}
 \caption{Ich brauche noch einen sinnvollen Text.}
  \label{fig:survey8}
\end{figure}


Grafik \ref{fig:survey9}

\begin{figure}[ht]
  \includegraphics[width=\textwidth]{images/survey9.eps}
 \caption{Ich brauche noch einen sinnvollen Text.}
  \label{fig:survey9}
\end{figure}


%______________________________________________________________________
\cleardoublepage

\section{Ausblick}
Allen Prognosen nach wird VR (Quelle dazu?) in den kommenden Jahren eine immer größere Rolle einnehmen, von Ausbildung, Beruf bis in die Freizeit hinein, wird es immer mehr VR-Geräte und -Programme geben. 
Im Bildungsbereich bietet VRClassroom bereits jetzt eine Möglichkeit für Lehrkräfte ohne großen finanziellen Aufwand und ohne viel Vorwissen zu benötigen, VR-Inhalte in ihren Unterricht einfließen zu lassen. Im Folgenden wird darauf eingegangen, wie die Zukunft von Virtual Reality aussehen kann beziehungsweise was an nächsten Entwicklungsstufen passieren muss, damit VR den prognostizierten Durchbruch auch haben wird.
Außerdem werden bestehende Probleme von VRClassroom besprochen und wie das System in der Zukunft weiterentwickelt werden könnte.

\subsection{Zukunft von VR}
VR wird sich noch weiter verbreiten, da Geräte günstiger werden und mehr Inhalte da sind
Trotzdem noch langer weg, siehe Studie nur 40\% der Lehrer haben schon einmal ein VR-Headset verwendet.
Was muss sich ändern, damit es sich noch mehr verbreitet?
 -> standalone Geräte, müssen mehr Rechenleistung, bessere Displays und weniger Gewicht haben
 -> besseres Tracking?
 -> mehr Anwendungen abseits von Games
 
 Geräteweiterentwicklungen
 - Gestenerkennung ohne Controller?
 - Gazeinput wie zB EyeVR Paper?

\subsection{Probleme von VRClassroom}
In Tests haben sich noch ein paar Probleme gezeigt, die das VRClassroom System in seinem aktuellen Zustand noch hat, die beim ``echten'' Einsatz im Schulunterricht das Erlebnis stören könnten.
Diese Problem konnten aber leider nicht im Rahmen dieser Arbeit gelöst werden, das sie nicht wirklich Probleme der entwickelten Software sind, sondern technischen Grenzen von Geräten, die sich in der Zukunft aller Voraussicht nach weiter verschieben werden, sodass sie dann kein Problem mehr darstellen.

\subsubsection{Anzeigen großer Dateien}
Eines der größten Probleme, die VRClassroom momentan hat, ist die Anzeige von komplexen 3D-Modellen oder 360°-Videos mit einer großen Dateigröße. 360°-Videos sind besonders problematisch, wenn sie eine extrem hohe Auflösung (>4096) und eine hohe Framerate haben. 

Für diese Probleme gibt es allerdings momentan nicht wirklich eine Lösung, da die Daten über das WLAN übertragen werden müssen und bei 25-30 Geräten, die verfügbare Bandbreite zwischen allen Geräten geteilt werden muss und das einfach nicht genug pro Gerät hergibt, um so hohe Datenmengen schnell genug zu übertragen. 

(Hier vielleicht noch Vorrechnen mit einem geläufigen Router einfügen?)

Zudem war bei Tests vereinzelt zu sehen, dass bei extrem komplexen Modellen WebVR crasht und ein Neuladen der Schüler-App herbeigeführt wird, sodass das Gerät aus dem Cardboard genommen und die WebVR-Ansicht neu aktiviert werden muss.

\subsubsection{Synchronisierung}
Ein weiteres Problem bei VRClassroom ist es, alle Veränderungen exakt synchron auf allen Geräten darzustellen. Besonders beim Abspielen von 360°-Videos fällt dieses Problem auf. 

Der Websocket-Server broadcastet die Nachricht über den veränderten Inhalt an alle Clients und da die Nachricht nur eine kurze JSON-Datei ist, wird sie auch instantan auf allen Geräten empfangen. Trotzdem kommt es zu geringen Verschiebungen der Anzeigeaktualisierung bei den einzelnen Geräten.
Der Ton der Videos ist dann um einen Bruchteil einer Sekunde verschoben, was zu unangenehmen Vermischungen der Sounds der verschiedenen Geräte und Halleffekten führt. 

Das kann dadurch umgangen werden, dass nur der Sound aus der Teacher-App abgespielt wird, die Schüler Ohrstöpsel tragen oder sie die Abspiellautstärke auf ihren Geräten weit herunterdrehen, dass es nicht durch den ganzen Raum schallt. Eine echte Lösung dafür gibt es allerdings nicht.

\subsection{Mögliche Weiterentwicklungen an VRClassroom}
VRClassroom ist ein vollfunktionsfähiges System, dass laut Umfrageergebnissen bereits bei einigigen Lehrkräften das Interesse geweckt hat, es in ihrem Unterricht einzusetzen. Trotzdem sind in der Entwicklung und bei der Online-Befragung noch mögliche Weiterentwicklungen von VRClassroom aufgefallen, die zukünftig noch integriert werden könnten.

\subsubsection{Hosten von VRClassroom Online statt Verbindung mit Lehrer-Rechner}
Eine mögliche Weiterentwicklung von VRClassroom könnte sein das ganze VRClassroom System nicht lokal auf dem Rechner der Lehrkraft laufen zu lassen, mit dem sich dann alle VR-Geräte verbinden, sondern auch eine Version anzubieten, die Online gehostet ist.

Die Entscheidung VRClassroom so zu entwickeln, dass es lokal läuft wurde allerdings ganz bewusste gefällt: Durch Vorgespräche mit den Lehrkräften, die an der Studie teilnehmen wollten und aus eigener Erfahrung wurde schnell klar, dass jede Schule unterschiedlich gut mit Netzwerk versorgt ist. Es gibt Schulen, die für Schüler gar kein WLAN anbieten oder dieses stark beschränken, sodass direkte Verbindungen zwischen den Geräten im Netzwerk unterdrückt werden, oder andere Restriktionen in ihrem Netz haben.
Um diese Probleme zu umgehen, wurde dann die Entscheidung gefällt, VRClassroom so zu entwickeln, dass es unabhängig vom Internet funktionieren kann. Einzig die Streetview-Funktion fällt dann weg. So kann dann einfach ein lokales Netzwerk eröffnet werden mit dem sich alle Geräte verbinden und VRClassroom problemlos genutzt werden. Dies könnte zum Beispiel mit einem Aufbau gelöst werden, bei dem der Rechner auf dem VRClassroom installiert ist und ein vorkonfigurierter Router immer gemeinsam genutzt werden und sobald Strom angesteckt wird der Router sein lokales Netzwerk aufmacht.

Ein weiteres Argument für die aktuelle Implementierung ist das schon zuvor in (HIER REF) besprochene Problem des Ladens von komplexen Modellen oder hochaufgelösten Videos, also großen Datenmengen. Laden alle Geräte lokal vom Lehrercomputer wird nicht die gesamte Internetverbindung der Schule lahmgelegt und auch die Übertragungsrate ist höher als beim Laden aus dem Internet. 

Wird VRClassroom nur in einem lokalen Netzwerk benutzt fällt allerdings wie bereits oben erwähnt die Funktionen zum Anzeigen von Google Streetview Fotos weg, denn dafür wird die Verbindung zum Internet benötigt. 
Zudem könnte es für technisch weniger verzierte Lehrkräfte schwieriger sein nachzuvollziehen, wo das Problem liegt, wenn etwa ein Schüler Gerät sich nicht verbinden kann, weil es nicht im Netzwerk ist, in dem VRClassroom läuft. 
Diese Punkte würden wiederum eher dafür sprechen die Struktur zu ändern, sodass VRClassroom online gehostet wird. 

Für den Rahmen der Arbeit ist die Entscheidung dafür gefallen, VRClassroom über den Rechner der Lehrkraft zu hosten, aber eine Online-Version wäre defintiv eine interessante Weiterentwicklung.

\subsubsection{Skalierung von 3D-Modellen auf echte Größe}
Um den Schülern zu vermitteln wie groß beziehungsweise klein die Dinge in echt sind, die gerade besprochen werden, könnte eine Funktion in der Lehrer-App eingefügt werden, die die 3D-Modelle auf Originalgröße skaliert. Dazu müsste selbstverständlich alle 3D-Modelle in Originalgröße vorliegen, was nur sehr selten der Fall ist, da diese Funktion für die meisten Verwendungszwecke nicht benötigt wird und daher einfach unbeachtet bleibt.

Um die gezeigten Inhalte gut zu begreifen und auch in der echten Welt einschätzen zu können, würden Schüler allerdings sehr davon profitieren diesen Einblick zu haben. Denn erst wenn man direkt davor steht, kann man begreifen wie groß der Eiffelturm ist oder wie klein eine extrem giftige Spinne wie die schwarze Witwe in Wahrheit ist.

Wie schon erwähnt müssen dazu zum einen die Modelle in der richtigen Größe vorliegen, aber zum anderen müsste auch die VR-Classroom-App angepasst werden, um diese Funktion sinnvoll zu erfüllen. Wird das Modell auf Originalgröße skaliert, die kleiner ist als die Größe, in der man sie genauer betrachten würde ist das kein Problem und könnte direkt erfüllt werden. Problematisch wird die Skalierung wenn sie extrem viel größer wird als das Modell normal angesehen wird, wie zum Beispiel beim Betrachten eines Gebäudes. Wird das Modell in der App extrem groß skaliert, wird es ab einer gewissen Größe nicht mehr vollständig angezeigt und Teile abgeschnitten. Das passiert, da React360 darauf ausgelegt ist, dass die Kamera in einer Art Kugel ist, in der sie sehen kann. Das dient dazu den Rendering-Aufwand nicht zu extrem werden zu lassen. (QUELLE dafür?) Alles was die Kugel durchbricht wird nur bis zum Rand der Kugel gerendert und alles was außerhalb liegt abgeschnitten. 

\subsubsection{Navigation von 3D-Modellen aus Schüler-App}
Momentan können Schüler keinen Einfluss darauf nehmen, welchen Teil eines 3D-Modells sie sehen können und welchen nicht, da nur aus der Teacher-App gesteuert werden kann wie das Modell skaliert und gedreht wird. 
Eine mögliche Erweiterung von VRClassroom könnte sein, dass die Schüler die 3D-Modelle selbstständig drehen und skalieren können beziehungsweise zu anderen Punkten am Modell springen können. 


\subsubsection{Ausfragemodus: Ausgewählter Schüler setzt Markierung}
Eine mögliche Weiterentwicklung von VRClassroom wäre eine Art ``Ausfragemodus'', bei dem die Lehrkraft aus der Liste der verbundenen Geräte eins auswählen kann, das dann eine Markierung setzen kann. Das ausgewählte Gerät hätte dann die Möglichkeit einmalig eine Markierungen zu setzen.

Dafür wäre es notwendig für die Schüler-Geräte eine Möglichkeit zu geben einen Punkt auszuwählen, an dem die Markierung gesetzt werden soll. Bei der aktuellen Implementierung mit einem Smartphone in einem Google Cardboard ist das noch nicht möglich. Dazu müsste mit dem Touch-Event vom Cardboard an der aktuellen Gaze-Position des Nutzers an dieser Stelle ein Marker gesetzt werden. Mit einem Headset, das bereits einen zugehörigen Controller hat, wäre das einfacher zu lösen, da lediglich die Position des ``Rays'' des Controllers abgefragt werden müsste. 

Da die Markierungen auch in der Teacher App über das iFrame mit der Student-App gesetzt werden ist die Kommunikation der Markerposition an die Teacher-App bereits implementiert und müsste nur leicht abgeändert werden. Möglicherweise wäre es gut die von Schülern gesetzten Marker noch in einer anderen Farbe darzustellen wie die der Lehrkraft, um für andere Schüler leichter erkennbar zu machen, welche Marker von welcher Person kommen. 

Momentan können Schüler nicht beeinflussen wie sie 3D-Modelle sehen, sondern nur den von der Lehrkraft ausgewählten Blickwinkel. Um auch auf 3D-Modellen sinnvoll Markierungen setzen zu können wäre es dann auch sehr sinnvoll den Schülern die Möglichkeit zu geben den Blickwinkel des 3D-Modells zu verändern. 

%______________________________________________________________________

\cleardoublepage
\fancyhead[LE,RO,LO,RE]{} % Keine Kopfzeile mehr oben auf jeder Seite
\section*{Inhalt der beigelegten CD}
%______________________________________________________________________

\cleardoublepage

\bibliographystyle{IEEEtran}
\bibliography{literature}

\end{document}
