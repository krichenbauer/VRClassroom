\documentclass[11pt,a4paper,twoside]{article}

\usepackage[T1]{fontenc} % sonst geht \hyphenation nicht mit Umlauten
\usepackage[latin1]{inputenc} % man kann schreiben äöüß, statt "a"o"u"s
%\usepackage[utf8]{inputenc} % wie oben, aber UTF-8 als Encoding statt ISO-8859-1 (latin1)
\usepackage[ngerman,english]{babel} % deutsche Trennregeln, "Inhaltsverzeichnis" etc.
%\usepackage{ngerman} % Alternative zum Babel-Paket oben
\usepackage{mathptmx} % Times-Roman-Schrift (auch für mathematische Formeln)
\usepackage{framed}
\usepackage{longtable}
\usepackage{tabu}


% Zum Setzen von URLs
\usepackage{color}
\definecolor{darkred}{rgb}{.25,0,0}
\definecolor{darkgreen}{rgb}{0,.2,0}
\definecolor{darkmagenta}{rgb}{.2,0,.2}
\definecolor{darkcyan}{rgb}{0,.15,.15}
\usepackage[plainpages=false,bookmarks=true,bookmarksopen=true,colorlinks=true,
  linkcolor=darkred,citecolor=darkgreen,filecolor=darkmagenta,
  menucolor=darkred,urlcolor=darkcyan]{hyperref}

% pdflatex: Bilder in den Formaten .jpeg, .png und .pdf
% latex: Bilder im .eps-Format
\usepackage{graphicx}
\usepackage{subcaption}
\usepackage{sidecap}

\usepackage{fancyhdr} % Positionierung der Seitenzahlen
\fancyhead[LE,RO,LO,RE]{}
\fancyfoot[CE,CO,RE,LO]{}
\fancyfoot[LE,RO]{\Roman{page}}
\renewcommand{\headrulewidth}{0pt}
\setlength{\headheight}{13.6pt} % behebt headheight Warning

% Korrektes Format für Nummerierung von Abbildungen (figure) und
% Tabellen (table): <Kapitelnummer>.<Abbildungsnummer>
\makeatletter
\@addtoreset{figure}{section}
\renewcommand{\thefigure}{\thesection.\arabic{figure}}
\@addtoreset{table}{section}
\renewcommand{\thetable}{\thesection.\arabic{table}}
\makeatother

\sloppy % Damit LaTeX nicht so viel über "overfull hbox" u.Ä. meckert

% Ränder
\addtolength{\topmargin}{-16mm}
\setlength{\oddsidemargin}{25mm}
\setlength{\evensidemargin}{35mm}
\addtolength{\oddsidemargin}{-1in}
\addtolength{\evensidemargin}{-1in}
\setlength{\textwidth}{15cm}
\addtolength{\textheight}{34mm}
%______________________________________________________________________

\begin{document}

\pagestyle{empty} % Vorerst keine Seitenzahlen
\pagenumbering{alph} % Unsichtbare alphabetische Nummerierung

\begin{center}
\textsc{Ludwig-Maximilians-Universität München}\\
Department ``Institut für Informatik''\\
Lehr- und Forschungseinheit Medieninformatik\\
Prof.\ Dr.\ Heinrich Hußmann

\vspace{5cm}
{\large\textbf{Masterarbeit}}\vspace{.5cm}

{\LARGE Entwicklung eines Systems zur Nutzung von VR-Brillen im Unterricht}\vspace{1cm}

{\large Veronika Fuchsberger}\\\href{mailto:veronika.fuchsberger@campus.lmu.de}{veronika.fuchsberger@campus.lmu.de}

\end{center}
\vfill

\begin{tabular}{ll}
Bearbeitungszeitraum: & 01. 08. 2018 bis 30. 01. 2018\\
Betreuer: & Christoph Krichenbauer\\
Verantw. Hochschullehrer: & Prof. Heinrich Hußmann
\end{tabular}
%______________________________________________________________________

\clearpage
\section*{Zusammenfassung}
Die Arbeit setzt sich mit dem Einsatz von Virtual Reality-Headsets im Schulunterricht auseinander. Dazu wurden aktuelle VR-Headsets verglichen und nach Preissegmenten und Anwendungsfunktionen eingeordnet. Zudem wurde bisher existierende Software, die Virtual Reality für den Schulunterricht zugänglich macht diskutiert und bewertet. Herausforderungen für Virtual Reality-Systeme im Generellen und im Speziellen für den Einsatz im  Schulunterricht werden diskutiert.
Weiterhin geht die Arbeit auf verschiedenen Inhaltstypen für VR-Systeme ein, welche Quellen es für diese Inhalte gibt und wie sie bearbeitet und selbst erstellt werden können.

Anschließend wurde basierend auf den zuvor erörterten Anforderungen ``VRClassroom'' entwickelt. Eine Software die Lehrkräfte im Unterricht nutzen können, um 360°-Inhalte zu präsentieren. Das System besteht aus zwei Komponenten: Die Lehrkraft kann an einem Computer Inhalte laden, die dann in einer Web-App auf VR-Brillen der Schüler gezeigt werden. Die Lehrkräfte führen dabei das komplette VR-Erlebnis für die Schüler und können 360°-Fotos und -Videos laden und Markierungen darauf setzen. Das Video wird bei allen Schülern synchron abgespielt und kann von der Lehrkraft pausiert werden. Außerdem können 3D-Modelle in die App geladen werden, die die Lehrkraft skalieren und drehen kann und an interessanten Stellen Markierungen setzen. Um zu sehen, welche Geräte verbunden sind, wird in der Lehrkraft ein Liste mit allen verbundenen Geräten und einem Aktivitätsindikator angezeigt. 

Anschließend wurde eine Online-Befragung mit Lehrkräften durchgeführt um die entwickelte Software zu evaluieren. Den Teilnehmern wurde ein Video gezeigt, das die Nutzung von VRClassroom erklärt und anschließend ein Fragebogen vorgelegt. Die Mehrheit der Teilnehmer war dem System gegenüber sehr aufgeschlossen und konnte sich gut vorstellen VRClassroom zukünftig im Unterricht zu einzusetzen, um 360°-Inhalte zu zeigen.

Abschließend wird diskutiert wie Zukunft von Virtual Reality im Schulunterricht aussehen könnte. Dazu werden auch mögliche Weiterentwicklungen von VRClassroom besprochen.

\selectlanguage{english}
\section*{Abstract}
This works topic is the use of virtual reality headsets in education. The first chapter gives an overview of recent VR headsets and compares them in functionality, retail price and other characteristics. Afterwards, existing software for education is discussed and assessed.

Based on the gathered information VRClassroom was developed. VRClassroom is a software which gives teacher the opportunity to present their students 360 content without losing the overview over the class. The software consists of two components: The teacher app, in which they put the content, and the students app, which shows the content provided by the teacher.
The teacher app gives an overview of the connected student-devices and their activity status, show the current content and offers controls to modify it. The teacher can use 360 photos, 360 videos and 3D models with VRClassroom. They can directly load Google Streetview panoramas, too. 360 videos are played synchronously on all devices once the teacher hits play. 3D models can be scaled and rotated by the teacher. Additionally, the teacher can place markers on all three content types.

The developed software was evaluated with 59 teachers. They were presented with a video explaining the use of VRClassroom and answered a questionnaire afterwards. The majority of the participants liked VRClassroom and could see themselves using VRClassroom in the future.

The last chapters discusses the future of Virtual Reality in education and possible advancements of VRClassroom.

\selectlanguage{ngerman}
\clearpage


\vfill % Sorgt dafür, dass das Folgende an das Seitenende rutscht

\noindent Ich erkläre hiermit, dass ich die vorliegende Arbeit
selbstständig angefertigt, alle Zitate als solche kenntlich gemacht
sowie alle benutzten Quellen und Hilfsmittel angegeben habe.

\bigskip\noindent München, \today

\vspace{4ex}\noindent\makebox[7cm]{\dotfill}

%______________________________________________________________________

\cleardoublepage
\pagestyle{fancy}
\pagenumbering{roman} % Römische Seitenzahlen
\setcounter{page}{1}

% Inhaltsverzeichnis erzeugen
\tableofcontents

%Abbildungsverzeichnis erzeugen - normalerweise nicht nötig
%\cleardoublepage
%\listoffigures
%______________________________________________________________________

\cleardoublepage

% Arabische Seitenzahlen
\pagenumbering{arabic}
\setcounter{page}{1}
% Geändertes Format für Seitenränder, arabische Seitenzahlen
\fancyhead[LE,RO]{\rightmark}
\fancyhead[LO,RE]{\leftmark}
\fancyfoot[LE,RO]{\thepage}

\section{Einleitung}
VR-Headsets immer mehr verbeitet, es gibt immer mehr Content
Umfrage, wie viele Leute schon VR-Headsets benutzt haben?
Statistiken zu:
Medien im Unterricht
Erfolg neuer Lehrkonzepte bzgl neue Medien %TODO

\subsection{Definition von Virtual Reality}
Was ist VR, wie unterscheidet es sich zu AR? %TODO

Virtual Reality kurz VR bezeichnet
Augmented Reality (AR) dagegen integriert virtuelle Objekte in den realen Raum. Dazu wird in ein Kamerabild an eine bestimmte Stelle ein Objekt gezeichnet, das dort physisch nicht existiert. AR wird meist nur für 3D-Modelle verwendet, während VR auch 360°-Fotos oder Videos anzeigen kann.

\subsection{Aufbau von VR-Headsets}
Ein VR-System ist ein Gerät, dass VR-Inhalte anzeigen kann. Das können zum Einen so genannte Head-mounted Displays (HMD), auch bekannt als VR-Brillen oder VR-Headset, sein oder eine CAVE. Eine CAVE ist ein Raum, bei dem alle Seiten Displays sind, die virtuelle Inhalte anzeigen. In beiden Fällen können Nutzer mit dem VR-System interagieren.
Da eine CAVE ein gesamter Raum ist, der pro Nutzer benötigt wird, ist sie für VR-Anwendungen im Unterricht nicht geeignet. Daher werden in dieser Arbeit ausschließlich VR-Headsets behandelt.

Ein VR-Headset besteht aus einer Art Brille, in der sich ein Display und Linsen befinden und mit Riemen am Kopf befestigt wird. Die Linsen dienen dazu das auf dem Display gezeigte auf ein weiteres Blickfeld zu bringen und einen angenehmen Fokuspunkt für die Augen zu schaffen, der weiter entfernt ist als das Display. Zusätzlich sind noch verschiedene Sensoren verbaut, mit denen Bewegungen und Position des Nutzers registriert werden.
Bei einem VR-Headset sieht der Nutzer nur die ihm gezeigten Inhalte und nicht den um ihn liegenden Raum. 

Die meisten VR-Headsets haben zusätzlich einen Controller, mit dem der Nutzer mit der VR-Welt interagieren kann.
%______________________________________________________________________

% Der Befehl \cleardoublepage erscheint nur vor \section, nicht vor
% den "kleineren" Gliederungsbefehlen wie \subsection!
\cleardoublepage % Neue rechte Seite anfangen
\section{Existierende VR-Hardware-Systeme}
Es gibt inzwischen eine große Zahl verschiedener VR-Hardware-Systeme, die sich in drei Gruppen mit unterschiedlichen Anwendungsszenarien unterteilen: Die Computer-gestützten VR-Systeme, die Stand-alone VR-Systeme und die Smartphone-gestützten VR-Systeme.

\subsection{Merkmale von VR-Systemen}
Um die verschiedenen VR-Hardware-Systeme einordnen zu können, gibt es einige technische Merkmale, die Headsets in unterschiedliche Kategorien unterteilen. Diese verschiedenen Kategorien von Headsets dienen unterschiedlichen Anwendungsszenarien und sollten entsprechend der Nutzung ausgewählt werden.

Folgende Merkmale können zur Differenzierung herangezogen werden: Freiheitsgrade (Degrees of Freedom, bzw. DoF), die Displaygröße und -auflösung, die Bildwiederholrate des Displays (Frequenz), das Sichtfeld (Field of View), die Rechenleistung und das Gewicht des Headsets, die verwendete Tracking-Methode, die mitgelieferten Controller beziehungsweise mögliche Eingabemethoden und der Verkaufspreis.

\subsubsection{Freiheitsgrade (Degrees of Freedom)}
Das Konzept der Freiheitsgrade oder Degrees of Freedom, kurz DoF, entspringt ursprünglich der Mechanik. Wie Gans in seiner Arbeit ``Engineering dynamics: From the lagrangian to simulation'' erläutert, hat ein Körper grundlegend sechs Freiheitsgrade. Die Bewegungen im Raum entlang der x-, y- und z-Achse. Hinzu kommen die Rotationen um die jeweiligen Achsen. Kurz beschreibt er Freiheitsgrade als die minimale Anzahl an Variablen um ein System zu spezifizieren. \cite{Gans2013}

Freiheitsgrade werden oft auch genutzt um die Bewegungen von maritimen Fahrzeugen zu benennen: Die Bewegung in x-Richtung wird ``surge'' genannt, in y-Richtung ``sway'' und in z-Richtung ``heave''. Die Rotation um die x-Achse heißt ``roll'', um die y-Achse ``pitch'' und um die z-Achse ``yaw''. Diese Bezeichnungen beziehen sich dabei auf ein Schiff, das in x-Richtung ausgerichtet ist. \cite{Fossen1995}

Wie in Grafik \ref{fig:dof} zu erkennen, sind die Degrees of Freedom für VR-Headsets aus diesen Definitionen abgeleitet. Unterstützt ein System nur die Erkennung von Rotationsbewegungen um die drei Achsen wird es als ein 3DoF-System bezeichnet. Kann es zusätzlich die Bewegungen im Raum messen ist es ein 6DoF-System. Um ein 6DoF VR-Gerät zu entwickeln, wird also ``positional tracking'' des Nutzers benötigt. 

\begin{figure}[ht]
  \centering
  \includegraphics[width=\textwidth]{images/dof.eps}
 \caption{Degrees of Freedom eine Virtual Reality Headsets.}
  \label{fig:dof}
\end{figure}

\subsubsection{Display, Frequenz und Field of View}
Bei Displays von VR-Headsets gibt es zwei grundsätzliche Ansätze: Ein durchgängiges Display, das das Bild für beide Augen darstellt oder zwei kleinere unabhängige Displays. 
Auf ein einzelnes großes Display setzt eigentlich nur Sony mit ihrer Playstation VR, alle anderen Hersteller von VR-Headsets, die fest integrierte Displays haben, arbeiten mit getrennten Displays für die beiden Augen.

Bei Smartphone-gestützten VR-Headsets hängt die Größe des Displays, sowie die Auflösung und damit auch das Sichtfeld (``Field of View'') vom eingelegten Smartphone ab. Da in aktuellen Smartphones sehr hochaufgelöste Displays eingebaut werden, stehen diese den festintegrierten Displays nicht oder nur minimal nach. Nutzt man beispielsweise ein Galaxy S9+ hat man ein Display mit 2960x1440 Pixel Auflösung und 530ppi (pixel per inch), was einer Auflösung von 1480x1440 pro Auge entspricht. Es ist damit höher aufgelöst als alle fest eingebauten Displays (abgesehen von der Oculus Quest, die erst im Laufe des Jahres auf den Markt kommen soll).

In den Tabellen \ref{tab:headsets1} und \ref{tab:headsets2} sind die Angaben zur Auflösung der Displays als Auflösung pro Auge zu verstehen.

Neben der Auflösung ist auch der Abstand der einzelnen Pixel zueinander entscheidend für die Qualität der visuellen Darstellung. Der Screen Door Effect und der Pixel Fill Factor beziehen sich auf den nicht leuchtenden Abstand zwischen zweier Pixel. Der Screen Door Effect bezeichnet den Zustand, wenn diese Abstände deutlich als Gitter über dem Inhalt erkennbar werden. Der Pixel Fill Factor beschreibt die Dichte der Pixel zueinander. Ist der Pixel Fill Factor hoch, wird der Screen Door Effect schwächer oder verschwindet sogar komplett.
Da bei VR-Headsets das Display extrem nah am Auge ist, wurden bei den ersten Geräten von Oculus und HTC starke Screen Door Effekte wahrgenommen. Aber mit den Versionen, die dann letztendlich in den Handel kamen, sind diese Probleme großteils ausgeräumt worden.
% Paper: A Review Paper on Oculus Rift-A Virtual Reality Headset

Die Arbeit von XXX et al. schließt aus ihren Nachforschungen, dass das menschliche visuelle System schon ab 13ms komplette verschiedene Bilder wahrnehmen und damit unterscheiden kann. Das entspräche einer benötigten Bildwiederholrate (Framerate) von knapp 80Hz. Die Forschungen zeigen allerdings auch, dass kleinere Veränderungen mit bis zu 500Hz noch deutlich schneller wahrgenommen werden können. 
Die aktuellen VR-Headsets sind mit Bildwiederholraten zwischen 60Hz und 120Hz noch deutlich davon entfernt. Allerdings ist zu vermuten, dass keine 500Hz notwendig benötigt werden, da die Studie mit hochfrequent flackernden Lichtern durchgeführt wurde, was in den meisten Situationen nicht der Realität entspricht.
% https://www.nature.com/articles/srep07861 (500Hz Flicker)
% Detecting Meaning in Rapid Serial Visual Representation ({RSVP}) at 13 ms Per Picture (Paper)

Wie Lin et al. in ``Effects of field of view on presence, enjoyment, memory, and simulator sickness in a virtual environment'' ausführen haben Menschen ein aus beiden Augen zusammengesetztes Sichtfeld von 180°. Ein weiteres Sichtfeld als 180° wäre also nicht sinnvoll, da es nicht mehr wahrnehmbar wäre.
Aktuelle Headsets sind mit einem Sichtfeld von 100°-110° allerdings noch etwas von der Abbildung des vollen Sichtfelds entfernt. 
Augenabstand IPD. %TODO
Wie aber Lin et al. ebenso bemerkten sie jedoch in ihrer Studie, dass ein weiteres Field of View als 140° kaum noch positivere Effekte im Bezug auf Immersionsgefühl und Spaß (Enjoyment) hatten. Zusätzlich fanden sie heraus, dass das field of view mit dem Auftreten von Cybersickness korreliert. Je weiter also das Field of View ist, desto mehr litten die Studienteilnehmer an den Symptomen.

% Human Field of View 180° auf 135°
% \cite{Stone2017}

\subsubsection{Rechenleistung}
Generell lässt sich sagen, das mit höherer Rechenleistung aufwändigere VR-Szenen in höherer Qualität dargestellt werden können. Bessere Display-Auflösung, Bildwiederholungsrate und Field of View benötigen eine höhere Rechenleistung um die Bildinhalte zu generieren. Deshalb können (zumindest momentan) stand-alone Geräte und Smartphone-gestützte Geräte nicht mit den Computer-gestützten Geräten in diesen Kategorien konkurrieren.
Da die Rechenleistung bei Computer- und Smartphone-gestützten VR-Headsets allerdings vom benutzen Rechner beziehungsweise Smartphone abhängt, ist die Rechenleistung nur bei den stand-alone Geräten ein Merkmal, das berücksichtigt werden kann.

Fast alle in dieser Arbeit untersuchten Geräte haben dabei den gleichen Prozessor verbaut. Während alle anderen Geräte einen Qualcomm Snapdragon 835 nutzen, hat lediglich die Oculus Go hat einen Qualcomm Snapdragon 821 verbaut. Damit sind die anderen Geräte nominell etwa 30\% schneller und verbrauchen dabei ungefähr 40\% weniger Energie als die Oculus Go.

Quellen:
https://vrodo.de/lenovo-mirage-solo-test-besser-als-oculus-go/
https://vrodo.de/oculus-quest-vs-oculus-go-vergleich-das-sind-die-unterschiede/
https://www.vive.com/cn/product/vive-focus-en/


\subsubsection{Eingabemethoden}
Es zwei grundsätzliche Eingabemethoden für Virtual Reality, die momentan verwendet werden: Eingaben über einen oder mehrere externe Controller oder aber die Eingabe am Headset.

 % mit verschiedenen Buttons und einem ``Raycaster'' in der VR-Welt und das Auslösen von Touch-Events auf dem Display des VR-Headsets. 

Eine einfache Methode für die Eingabe am Headset selbst ist bei Smartphone-gestützten Systemen das Auslösen von Touch-Eingaben auf dem Smartphone-Display. Ein Beispiel dafür sind Google Cardboards, die einen Auslöser am Gehäuse haben, der eine Art ``Arm'' auf das Display drückt. Einige Nachbauten des ursprünglichen Google Cardboards verwenden Magnete um Touch-Eingaben auf kapazitiven Touchscreens auszulösen.
%TODO (Diese Technik noch genauer erläutern?)
Egal mit welcher der beiden Techniken das Touch-Event ausgelöst wird, erfolgt das Touch-Event immer an der gleichen Stelle auf dem Display. Der Nutzer kann also keine bestimmte Stelle in der Szene auswählen. Deshalb wird bei dieser Eingabemetohde meist der Mittelpunkt der aktuellen Blickrichtung als Position oder Richtung des ausgelösten Events interpretiert.

Ganz ohne Eingabe am Touchscreen können Gaze-Events (engl. ``starren'') zur Eingabe verwendet werden. Reagiert ein Bereich in einer Szene auf Gaze-Events, so löst das Ereignis aus, wenn der Bereich für eine bestimmte Zeit ``angestarrt'' wird. Üblicherweise wird der Mittelpunkt des Blickfelds als Zeiger für Gaze-Event verwendet.

Externe 3DoF-Controller bieten meist mehrere Buttons zur Eingabe und stellen einen Zeiger da, mit dem Bereiche in der Szene ausgewählt werden können. Üblicherweise ist der Ursprung des Zeigers an einem Ort fixiert. Über Beschleunigungssensoren wird die Richtung des Zeigers bestimmt, in der ein Strahl ausgesendet wird (``Raycaster'').

Bei 6DoF-Controllern, wird deren Position im Raum durch Positional-Tracking-Systeme bestimmt. Anders als bei 3DoF-Controllern, ist damit die Position des Controllers nicht fixiert, sondern kann im Raum verändert werden.

Bisher setzen keine Systeme auf Spracheingabe. Eine Entwicklung in diese Richtung ist aber abzusehen, da die Eingabe sehr natürlich ist und dadurch die Zugänglichkeit zu VR-Systemen erhöht werden kann.

In ihrer Arbeit haben XXX et al. ``EyeVR'' entwickelt. Dazu haben sie eine Kamera in ein bestehendes VR-Headset eingebaut, die die Bewegungen der Augen registriert. Sie haben dazu eine Software entwickelt, die sowohl die Pupillenbewegungen als auch die Bewegungen des Lids registriert und als Input verarbeiten kann. 
% EyeVR Paper

\subsubsection{Positional Tracking}
Um sechs Freiheitsgrade für VR-Headsets zu bestimmen, muss die Position im Raum oder die Veränderung der Position relativ zum Raum berechnet werden. Positional Tracking wird also generell nur von 6DoF-Headsets genutzt.

Momentan gibt es dazu zwei verschiedene Ansätze: Die eine Möglichkeit ist es Tracker im Raum aufzustellen, die dann die Position des Headsets und gegebenenfalls der Controller im Raum bestimmen. Die Computer-gestützten VR-Headsets nutzen alle diese Technik, sie wird outside-in-Tracking genannt.  \cite{Stone2017}

Die zweite Möglichkeit ist das inside-out-Tracking. Für das inside-out-Tracking müssen keinerlei Sensoren im Raum platziert werden. Es werden nur Sensoren verwendet, die in das Headset integriert sind.  \cite{Stone2017}
Dafür nutzt die Lenovo Mirage Solo beispielsweise das WorldSense-System von Google. Es verwendet in das Headset integrierten Kameras zur Bestimmung der Position nutzt.
% https://developers.google.com/vr/discover/worldsense

Die Oculus Quest dagegen soll mit dem von Facebook selbst entwickelten ``Insight''-Tracking kommen. Dafür sind vier Kameras an den vorderen vier Ecken des Displays angebracht. Diese bestimmen die Position im Raum anhand von Objekten im Raum, die von den Kameras erkannt werden. Das Insight-Tracking soll zusätzlich auch die Position der beiden Controller bestimmen, sodass auch diese sechs Degrees of Freedom unterstützen.
% https://www.roadtovr.com/oculus-quest-hands-specs-tech-details-oculus-connect-5/

\subsubsection{Gewicht}
Die Spanne zwischen den Gewichten der einzelnen Virtual Reality-Headsets reicht von 610g im Maximum zu nur knapp 100g im Minimum. Die Entwicklung der Geräte geht zu leichteren Modellen. Allerdings ist bei den neu vorgestellten Modellen, die ohne Kabel funktionieren wiederum das Gewicht des Akkus ein neues Problem. Die Hersteller versuchen das doch noch recht hohe Gewicht durch gute Riemensysteme auszugleichen. Dennoch kann das Gewicht bei längerer Nutzung unangenehm sein.

Die Playstation VR hat ein kleines Gegengewicht im Bügel, sodass sich das Gewicht etwas besser verteilt.

Das leichteste aller VR-Headsets ist das Google Cardboard mit einem eingelegten Smartphone, es hat dann ein Gesamtgewicht von etwa 270g, wenn man die 96g des Cardboards mit einem durchschnittlichen aktuellen Smartphone addiert. Allerdings bietet das Gerät auch keine Polsterungen oder Riemen.
Bei allen Geräten, die in Tabelle \ref{tab:headsets3} aufgeführt sind, bezieht sich das Gewicht immer auf das Headset ohne Smartphone.

\subsubsection{Preis}
\label{subsec:price}
Wie in \ref{sec:challenges} erläutert sieht die Autorin des Artikels ``5 Major Challenges For The VR Industry'' den Preis als eines der Hautprobleme, dass sich VR noch nicht weiter durchgesetzt hat.
Da aber besonders die Smartphone-gestützten VR-Systeme sehr günstig zu bekommen sind, gilt dieses Argument eigentlich nicht wirklich.

Trotzdem wird von Experten (QUELLE) erwartet, dass die Preise für VR-System weiter sinken werden, sodass auch die hochwertigen Lösungen erschwinglicher für alle Nutzergruppen werden. %TODO Quelle

Die Preise in den Tabellen \ref{tab:headsets1}, \ref{tab:headsets2} und \ref{tab:headsets3} sind in US-Dollar, wie sie auf dem amerikanischen Markt zu kaufen sind. Darin ist keine Umsatzsteuern enthalten.

Bei PlaystationVR handelt es sich um ein Bundle-Preis, bei dem 2 Spiele enthalten sind.

\subsection{Computer-gestützte VR-Hardware}
Die erste Gruppe sind die Computer-gestützten Hardware-Systeme: Sie sind mit einem Kabel mit dem Rechner verbunden, der die Rechenleistung für die eigentlich Brille übernimmt. Sie eignen sich besonders für rechenaufwändige Anwendungen, aber schränken durch ihre Kabel die Bewegungsfreiheit des Nutzers ein.

\begin{table}[]
\begin{tabular}{p{0.14\linewidth}|p{0.07\linewidth}|p{0.14\linewidth}|p{0.18\linewidth}|p{0.18\linewidth}|p{0.08\linewidth}|p{0.06\linewidth}}
\textbf{VR-Gerät} & \textbf{DoF} & \textbf{Display} & \textbf{Input} & \textbf{Tracking} & \textbf{Gewicht} & \textbf{Preis} \\ \hline
Oculus Rift & 6 DoF & 1080x1200 \newline 90Hz \newline 100° & Oculus Touch, Xbox One Controller & 2 optische Sensoren im Raum & 380-470g &  349\$ \\
Playstation VR & 6 DoF & 960x1080 \newline120Hz \newline 100° & PlayStation Move Controller
 & Playstation Camera & 610g & 349 \$ \\
HTC Vive & 6 DoF & 1080x1200  \newline 90Hz \newline 110° & SteamVR-Tracking Controller & 2 Laser-Sensoren im Raum & 555g & 499\$
\end{tabular}
  \caption{Vergleich der Computer-gestützten VR-Headsets.}
  ~\label{tab:headsets1}
\end{table}

% https://www.tomshardware.com/reviews/vive-rift-playstation-vr-comparison,4513-6.html
% Trackingsysteme und Controller Vergleich

% Vergleich: https://www.vrnerds.de/vr-brillen-vergleich/


\subsubsection{Oculus Rift}
Die Oculus Rift ist die erste von Oculus herausgebrachte VR-Brille. Sie wird durch einen Windows-Computer bespielt und wird durch zwei Kameras, die auf dem Schreibtisch um den Bildschirm des Computer platziert werden können, optisch getrackt. 

Mit der Oculus Rift kommen zwei Oculus Touch Controller, diese haben jeweils zwei Action-Buttons, einen Menü-Button und einen Thumbstick sowie einen rückseitigen Button, der für den Zeigefinger gedacht ist. Die Buttons können erkennen, ob ein Finger aufliegt, während der Button aber noch nicht gedrückt wurde. Dies erlaubt neben dem Klicken der Buttons auch die Position der Finger zu bestimmen, und so zu erkennen, ob der Nutzer beispielsweise ein Faust bildet.

Der Augenabstand (Inter-pupillary distance; IPD) ist verstellbar zwischen 58 und 72mm, der Linsendurchmesser liegt bei circa 50mm. 

Die Oculus Rift ist mit bis zu 470g deutlich leichter als die beiden anderen Kabel-gebundenen VR-Brillen. Auch die Controller sind mit 160g merkbar leichter als die der HTC Vive.

Für ein vollständiges Setup wird ein Windows-Rechner (mit Bildschirm, Mouse und Keyboard), die Oculus Rift Brille, die Oculus Touch Controller und die zwei Sensoren benötigt. 

% https://www.vrbound.com/headsets/oculus/rift
% https://www.oculus.com/rift/

\subsubsection{HTC Vive}
Die HTC Vive hat ein sehr ähnliches Display verbaut wie die Oculus Rift. Sie haben die gleiche Auflösung und Bildwiederholungsrate von 90Hz, allein das Field of View ist mit 110° etwas weiter als bei anderen VR-Headsets. Im Vive-Headset lässt sich die Pupillendistanz und der Objektivabstand einstellen.

Für ein vollständiges Setup wird ein Windows-Rechner (mit Bildschirm, Mouse und Keyboard), die HTC Vive Brille, die HTC Vive Controller und die zwei Laser-Sensoren, genannt ``Lighthouses'' benötigt. Die Lighthouses müssen an diagonalen Enden des Raums überhalb des Aktionsbereich aufgestellt werden, was die initiale Installation etwas aufwändiger macht. Durch die Lasertechnologie in den Lighthouses ist das mögliche Spielfeld der HTC Vive deutlich größer als das der anderen beiden VR-Systeme.

Um das Tracking einzustellen muss zu Beginn der ersten Nutzung das System kalibriert und der Spielbereich virtuell markiert werden.

Die Controller haben ein klickbares Trackpad, Menü- und System-Buttons auf der Vorderseite, einen Abzugsknopf für den Zeigefinger auf der Rückseite und Greifknöpfe an den Seiten des Controllers.

Die HTC Vive hat zusätzlich zu den Sensoren auch noch eine Kamera eingebaut, um gegebenenfalls im Weg stehende Gegenstände zu erkennen, um Unfälle zu verhindern.

Preislich liegt die Vive mit 499\$ am oberen Ende des Spektrums der VR-Headsets.

% https://www.vive.com/de/product/?gclid=EAIaIQobChMIgvauiYfA3wIVBcYYCh3oUwkIEAAYASAAEgLADfD_BwE#vive-spec

\subsubsection{Playstation VR}
Die Playstation VR ist dafür entwickelt mit der Playstation 4 Konsole benutzt zu werden, sie wurde also spezielle für den Anwendungsfall des Gamings entwickelt. Sie unterstützt sogar das Spielen mit zwei Spielern, wobei ein Spieler die Playstation VR nutzt und der zweite Spieler das Bild auf dem Fernseher sieht. Beide nutzen die Playstation Move Controller.

Die Controller haben die üblichen Buttons eines Playstation-Controllers und ihre Position im Raum wird getrackt.

Wie in Tabelle \ref{tab:headsets1} zu sehen ist die Auflösung der Playstation VR deutlich geringer als die der anderen beiden Headsets. Dafür hat sie eine extrem hohe Bildwiederholrate von 120Hz, was gerade für schnelle Spiele einen Vorteil bietet. Allerdings ist sie mit 610g recht schwer, sodass ein langes Tragen nicht sehr angenehm ist.

Zum Tracking der Position im Raum nutzt die Playstation VR die bereits existierende Playstation Camera. Darin sind 2 Kameras verbaut, die beide Tiefenwahrnehmung haben. Sie tracken optisch sowohl die Position des Headsets als auch die der Playstation Move Controller.

Für ein vollständiges Setup wird ein Playstation 4 Konsole, die Playstation VR Brille, die Playstation Move Controller und die Playstation Camera benötigt. 

% https://www.vrbound.com/headsets/sony/playstation-vr
% https://www.playstation.com/de-de/explore/playstation-vr/tech-specs/


\subsection{Stand-alone VR-Hardware}
Die zweite Gruppe sind die Stand-alone Hardware-Systeme wie etwa die Oculus Go, Oculus Quest oder Google Daydream. Es handelt sich hierbei um vollumfängliche Systeme, die ohne weiteres Equipment auskommen und auch keinen Computer benötigen. Da sie mit einem Akku betrieben werden, ist die Betriebsdauer allerdings eingeschränkt und auch die Rechenleistung ist deutlich geringer als die der Computer-gestützten Systeme.

% https://upload-magazin.de/blog/25071-standalone-vr-headsets/
% Vergleich: https://www.vrnerds.de/vr-brillen-vergleich/

\begin{table}[]
\begin{tabular}{p{0.14\linewidth}|p{0.08\linewidth}|p{0.14\linewidth}|p{0.15\linewidth}|p{0.15\linewidth}|p{0.1\linewidth}|p{0.08\linewidth}}
\textbf{VR-Gerät} & \textbf{DoF} & \textbf{Display} & \textbf{Input} & \textbf{Tracking} & \textbf{Gewicht} & \textbf{Preis} \\ \hline
Oculus Go & 3 DoF & 1280x1440  \newline 60-72Hz \newline 100° & Oculus Go Controller & kein Tracking & 468g & 199\$ \\
Oculus Quest & 6 DoF & 1600x1440 \newline OLED \newline 72Hz \newline 100° & Oculus Touch Controller & Inside-out & 570g & 399\$ \\
Lenovo Mirage Solo &1280x1440 \newline OLED \newline 75Hz \newline 110° & Daydream Motion Controller & &Inside-out  & 645g & 349,99\$ \\
HTC Vive Focus & 6 DoF & 1440x1600 \newline  \newline 75Hz \newline 110° & & Inside-out & 670g & 599\$
\end{tabular}
  \caption{Vergleich der Stand-alone VR-Headsets.}
  ~\label{tab:headsets2}
\end{table}

\subsubsection{Oculus Go}
Mit ihrem Preis von knapp 200\$ ist die Oculus Go mit Abstand das günstigste VR-Headset. Sie kommt mit einem kleinen 3DoF Controller, der ein klickbares Touchfeld, einen Button für den Zeigefinger, sowie einen kleinen ``Zurück''- und ``Oculus''-Button.

Die Oculus Go hat pro Auge ein Display von 1280x1440 Pixeln mit einer Refreshrate von 60 bis 72Hz und einem Field of View von 100°.

Sie erkennt durch einen kleinen Sensor am inneren der Brille, ob sie aufgesetzt ist oder nicht und schaltet sich nach längerer Zeit, in der sie abgesetzt ist, selbstständig ab. 
Sie bietet die Möglichkeit für Nutzer mit Sehschwächen an die Sehstärke angepasste Linsen auf die festeingebauten Linsen zu setzen, sodass die eigene Brille nicht benötigt wird. Diese müssen aber separat erworben werden.
Der Linsenabstand kann an der Oculus Go nicht individuell eingestellt werden und ist auf 64mm festgelegt.

Die Oculus go kommt mit eingebauten Lautsprecher, die so positioniert sind, dass man trotzdem noch Umgebungsgeräusche hören kann und nicht abgeschottet wird.

Eine Akkuladung hält je nach Nutzung zwischen 2 und 2,5 Stunden, wobei die Ladezeit mit fast 3 Stunden relativ lange ist.

Da es sich bei der Oculus Go um ein 3DoF-Headset handelt, unterstützt sie kein Positional Tracking, sondern nur orientational Tracking über die eingebauten Sensoren.

% https://www.vrbound.com/headsets/oculus/go

\subsubsection{Oculus Quest}
Die Oculus Quest ist noch nicht auf dem Markt, sondern für Frühling 2019 angekündigt. Einige Informationen zu dem Headset sind jedoch schon bekannt.

Die Oculus Quest ist ein 6 DoF-VR-Headset. Die Position im Raum wird durch vier an den äußeren Ecken des Headsets eingebauten Kameras bestimmt. Sie übernehmen auch das Tracking der Oculus-Touch-Controller, die damit ebenfalls sechs Freiheitsgrade erhalten.
Sie hat auch die Möglichkeit, den Linsenabstand individuell einzustellen, sodass für jeden Nutzer das Virtual Reality Erlebnis perfekt ist und das Risiko von Cybersickness durch Überanstrengung der Augen sinkt.

Die Oculus Quest soll mit einem OLED-Display mit einer Auflösung von 1600x1440 pro Auge und einer Refreshrate von 72Hz ausgestattet sein und ist damit das VR-Headsets mit dem am besten aufgelösten Display. Nur bei der Refreshrate haben die Computer-gestützten Headsets hier noch etwas voraus. Die Quest kommt, genau wieder die Go, mit eingebauten Lautsprechern, die es ermöglichen sowohl den Sound der VR-Welt zu hören, als auch noch Geräusche aus dem Raum mitzubekommen.

Die mitgelieferten Oculus-Touch-Controller sind denen der Oculus Rift sehr ähnlich. Jedoch befindet sich der Ring mit dem die Controller getrackt werden nun oberhalb, damit er von den Kameras im Headset gesehen werden kann. Sie können alle Bewegungen im Raum erkennen und auch, ob die Buttons mit den Fingern berührt werden.

Das Gesamtgewicht der Oculus Quest ist noch nicht bekannt, soll aber etwa 100g schwerer als das Rift-Headset sein und läge damit dann bei 570g. Auch die Akkulaufzeit der Oculus Quest wurde noch nicht herausgegeben.
% https://uploadvr.com/oculus-quest-specs-price-release-date/

\subsubsection{Lenovo Mirage Solo}
Die Lenovo Mirage Solo liegt bei einem Preis von 350\$. Sie ist ebenfalls ein 6DoF-Headsets, der mitgelieferte Daydream Motion Controller kann allerdings nur drei Freiheitsgrade. Auch die Mirage Solo nutzt inside-out-Tracking für die Positionsbestimmung im Raum. Dafür sind in das Display vorne zwei Kameras integriert. Berichten zufolge ist der nutzbare Spielbereich, der mit dem von Google entwickelten WorldSense getrackt wird, nur etwa einen Quadratmeter groß, sodass nur wenig Bewegung möglich ist.

Die Controller sind die gleichen, die Google für das Daydream-View-Headset verwendet. Zudem sind keine Lautsprecher eingebaut. Mit 645g ist die Mirage Solo das schwerste VR-Headset, das momentan auf dem Markt ist.
% https://www.lenovo.com/us/en/virtual-reality-and-smart-devices/virtual-and-augmented-reality/lenovo-mirage-solo/Mirage-Solo/p/ZZIRZRHVR01

Lenovo gibt die Akkulaufzeit mir 2,5 Stunden an, die bei Tests wohl sogar auf 3 Stunden ausgedehnt werden konnte.
Auf der Mirage Solo läuft Google Daydream, über das auch Apps geladen werden können. Im Vergleich zum Oculus Store sind dort Adi Robertson zufolge noch deutlich weniger und auch weniger elaborierte Titel zu finden.
% https://www.theverge.com/2018/5/4/17318648/lenovo-mirage-solo-google-daydream-standalone-vr-headset-review

\subsubsection{HTC Vive Focus}
Bisher ist die HTC Vive Focus nur China auf dem Markt, allerdings wurde Ende 2018 angekündigt, dass sie demnächst auch in Europa und den USA verkauft werden wird. Mit ihrem Preis von 599\$ ist sie die mit deutlichem Abstand teuerste Stand-alone VR-Brille.
Wie die Oculus Quest hat sie eine Auflösung von 1440x1600 pro Auge und eine Bildfrequenz von 75Hz und einem Field of View von 110°. Sie ist sich also insgesamt sehr ähnlich zur Oculus Quest.

Die Controller der Vive Focus haben nur 3DoF. Sie haben ein Trackpad, einen Trigger für den Zeigefinger und zwei Buttons auf der Oberseite. Sie erinnert formmäßig an eine Fernbedienung.

Wie die Mirage Solo nutzt die Vive Focus zwei ins Headset verbaute Kameras für ein Inside-Out Tracking mit einem Spielbereich von 1,5mx1,5m. 
Der Linsenabstand kann mechanisch eingestellt werden, bei größeren Brillen ist der Platz in der Brille allerdings etwas knapp.

Sie hat eine Laufzeit von 3 Stunden. Die Anzahl der verfügbaren Anwendungen ist noch gering.
% https://www.vive.com/cn/product/vive-focus-en/
% https://www.wareable.com/vr/htc-vive-focus-release-date-price-specs-5084

\subsection{Smartphone gestützte VR-Hardware}
Die dritte Gruppe bilden die Smartphone-gestützten VR-Systeme. Damit sind alle Systeme gemeint, bei denen das Smartphone genutzt wird, um die VR-Inhalte zu zeigen.
Wie in Tabelle \ref{tab:headsets3} zu sehen sind damit die Merkmale für das Display, die Rechenleistung und das Tracking nicht beinhaltet, da diese abhängig vom eingelegten Smartphone sind. Diese liegen preislich auf einem sehr niedrigen bis mittlerem Niveau und bilden so eine gute Einstiegsmöglichkeit.

\begin{table}[]
\begin{tabular}{p{0.19\textwidth}|p{0.07\textwidth}|p{0.15\textwidth}|p{0.26\textwidth}|p{0.08\textwidth}|p{0.06\textwidth}}
\textbf{VR-Gerät} & \textbf{DoF} & \textbf{Field of View} & \textbf{Input} & \textbf{Gewicht} & \textbf{Preis} \\ \hline
Google Daydream View & 3 DoF & 100° &  Daydream controller & 261g & 99\$ \\
Samsung Gear VR & 3 DoF & 101° & Gear VR Controller & 345g & 130\$ \\
Google Cardboard & 3 DoF &  90° & Hebel, der Touch-Event auf Display auslöst & 96g & 15\$
\end{tabular}
  \caption{Vergleich der Smartphone-gestützten VR-Headsets.}
  ~\label{tab:headsets3}
\end{table}

\subsubsection{Google Cardboard}
% Google Cardboard: https://store.google.com/product/google_cardboard
In das Google Cardboard können Smartphones mit Displaygrößen zwischen 4 und 6 Zoll eingelegt werden. Um die Google Cardboard App zu nutzen, müssen die Geräten dann mindestens Android 4.1 oder iOS 8.0 haben.

Es kann kein Controller verbunden werden. Es kann lediglich mit einem Hebel ein Touch-Event auf dem Display ausgelöst werden. In der Google Cardboard App wieder die Interaktion beispielsweise so ausgelöst, dass der Mittelpunkt des Displays ``geklickt'' wird, wenn das Touch-Event ausgelöst wird. So sind einfache Eingaben möglich. 

Das Field of View der Google Cardboards kann je nach eingelegtem Smartphone bis zu 90° breit sein.

Durch die einfache Konstruktion und den günstigen Preis ist das Google Cardboard weit verbreitet. Zudem gibt es davon diverse Arten von Nachbauten, die teilweise statt dem Hebel, der das Touch-Event auslöst, schwache Magneten an der Seite des Cardboards bewegen um so Touch-Events auszulösen.

\subsubsection{Samsung Gear VR}
% Samsung Gear VR: https://www.samsung.com/de/wearables/gear-vr-r324/SM-R324NZAADBT/?cid=de_ppc_google_im-wearables-gearvr-q3restructured_20180720_samsungvr-broad&tmcampid=7&tmad=c&tmplaceref=c_DE_IMECOM_Warm_Brand_GearVR_Broad&tmclickref=b_%2Bsamsung%20%2Bvr&gclid=EAIaIQobChMIpLnFnuvC3wIVCKQYCh0XVweYEAAYASAAEgLilfD_BwE

Die Samsung Gear VR Brille funktioniert nur mit den Galaxy Geräten ab Version 6 oder neuer und benötigt mindestens Android 5. Die Geräte werden auf dem USB-Stecker der Brille gesteckt, um so eine Verbindung herzustellen.
Wird ein Gerät zum ersten Mal verbunden werden Anweisungen angezeigt, die Oculus App herunterzuladen und einen Account zu erstellen beziehungsweise zu verbinden. Im Anschluss wird beim erneuten Verbinden mit dem Gerät die Oculus-App geöffnet um dort dann VR-Apps zu nutzen.

Da das Display vom Smartphone abhängig ist, kann hier nicht mit den anderen Geräten verglichen werden, aber das Field of View kann je nach eingelegtem Geräte bis zu 101° weit sein.

Im die Samsung Gear VR kommt mit einem Controller, der ein Touchpad und ``Zurück''-, ``Home''- und Lautstärke-Buttons hat. Auch den Controller der GearVR kann am Band des Headsets verstaut werden.

Die Gear VR ist nicht geeignet mit Brille genutzt zu werden. Dafür kann mit einem Rad an der Oberseite der Fokus eingestellt werden, sodass sie von Nutzern mit Sehschwächen trotzdem getragen werden kann.

Mit einem Preis von knapp 130\$ ist die GearVR die teuerste der drei Smartphone-gestützten VR-Brillen und schon nahe am Preis einer Oculus Go, die ohne ein extra Smartphone auskommt. Außerdem ist sie mit 345g ohne eingelegtem Smartphone eines der schwersten Headsets.

\subsubsection{Google Daydream View}
% Google Daydream View: https://store.google.com/product/google_daydream_view_specs
Das Google Daydream View Headset funktioniert ebenfalls nicht mit allen Smartphones, sondern nur Android-Smartphones: das Google Pixel 2 und 3, das Galaxy 8 und 9, das Zenfone AR von Asos und das LG V30 werden auf der Hersteller-Webseite als besonders geeignet aufgeführt. Grundsätzlich muss ein Gerät die Google Daydream App installiert haben innerhalb derer dann VR Apps gesucht und mit dem Controller verbunden und interagiert werden kann. 

Google Daydream View kann den aktuellen Bildschirminhalt auf einen Chromecast streamen, sodass andere sehen können, was gerade passiert.

Der Controller hat oben ein Touchpad, sowie einen Apps- und Home-Button. An der linken Seite sind zwei Lautstärketasten platziert. Da allerdings Daydream View selbst keine Lautsprecher hat werden damit die integrierten Smartphone Lautsprecher beziehungsweise die verbundenen Kopfhörer gesteuert.

Das Band mit dem die Brille am Kopf befestigt wird ist so gestaltet, dass man am hinteren Ende den Controller einstecken kann, sodass er nicht verloren gehen kann.

% ---------------------------------------------------------------------------------------------------------------------------
%% Wohin hiermit?! - Vielleicht zu Geräten?
% \subsection{Eingabemethoden in VR}
% Controller
% Voice Control
% EyeVR GazeInput
% Gesteninput

% ---------------------------------------------------------------------------------------------------------------------------


\cleardoublepage % Neue rechte Seite anfangen

\section{VR-Systeme im Bildungsbereich}
\label{sec:VReducation}

% Hier kommen noch mehr Sachen aus den Papern rein?
% Paper: Virtual reality for collaborative e-learning 2008

Virtual Reality wird in immer mehr verschiedenen Lebensbereichen eingesetzt und inzwischen gibt es neben den klassischen Simulatoren für Industriemaschinen und Flugobjekten auch verschiedene Ansätze von VR-Systemen im Bildungsbereich.

Merchant et al. schließen in ihrer Arbeit in 2014, dass Lernen mit Virtual Reality bessere Lernergebnisse produziert als Lernen ohne VR. Sie sehen damit die finanzielle und zeitliche Invesitition der Einführung von VR-Systemen als sinnvoll.
% Paper: Computers & Education Effectiveness of virtual reality-based instruction on students ? learning outcomes in K-12 and higher education : A meta-analysis

Im folgenden Kapitel werden drei dieser Systeme besprochen, die besonders interessant für den Einsatz im Unterricht sind.

In der Forschung wurden schon seit langer Zeit Systeme entwickelt, mit denen versucht wurde Behinderungen und psychische Störungen von Kindern und Jugendlichen zu verstehen und zu behandeln. Beispiele dafür sind:
Hier noch die Beispiele einfügen


\subsubsection{ClassVR}
Ein Beispiel für ein VR-System im Bildungsbereich, das einen ganz ähnlich Ansatz wie das im Rahmen dieser Arbeit entwickelte System, ist ClassVR. 

ClassVR benötigt allerdings die ClassVR Headsets, und kann nicht mit anderen Geräten benutzt werden. Zudem wird eine Einrichtungsservice auf der Webseite angeboten, was vermuten lässt, dass die initiale Installation von ClassVR und das Setup der Geräte relativ komplex ist.

Die ClassVR Headsets können sowohl für VR, als auch AR genutzt werden. Die Headsets laufen auf Android, 
Die Geräte haben keinen Controller, sondern werden mit einfachen einhändigen Gesten und Kopfbewegungen gesteuert. Außerdem kann auch die Kontrolle durch die Lehrkraft übernommen werden, sodass alle verbundenen Geräte die Inhalte anzeigen, die die Lehrkraft auswählt.
Das interessante an ClassVR ist, dass die Headsets in Boxen gekauft werden, in denen sie gelagert und geladen werden können. Außerdem bietet die Firma auch fertige Inhaltssets an, die dann mit den Headsets genutzt werden können. Momentan sind die Inhalte aber ausschließlich für den Britischen Markt ausgelegt und an dortige Lernpläne angepasst und nur in englischer Sprache erhältlich.

Ein Set aus 8 Headsets in einem Koffer, in dem die Geräte geladen werden können, gibt es ab 2249£. Für einen Satz für eine Klasse mit 24 Kindern liegt der Preis also bei 6747£, was etwa einem Preis von 7650 Euro entspricht. Alternativ kann auch ein Schrank mit 30 Headsets erworben werden, dieser liegt preislich bei 7500£. 
Hinzu kommt der Preis für das Abo für das ClassVR Portal, mit dem die Geräte gesteuert und Inhalte ausgewählt werden können. Das sind jährlich noch einmal 299£
% https://www.lgfl.net/learning-resources/summary-page/classvr-pricing.aspx
% http://www.inclusive.co.uk/inclusive-classvr

Die Lehrkraft kann eine Playlist mit den verfügbaren Inhalten anlegen, die dann auf die Schüler-Headsets geladen wird. Zudem können auch eigene 360°-Fotos und -Videos hochgeladen werden.
Es können 360°-Fotos und -Videos sowie 3D-Modelle angesehen werden. 3D-Modelle werden mit Karten aktiviert, auf denen ein Code ist, der dann das Anzeigen der Modelle aktiviert, sobald die Kamera ihn erkennt. Die Modelle werden dann in das Bild des Klassenzimmers, das die Kamera liefert integriert. Die 3D-Modelle sind relativ simple Modelle mit einem niedrigen Polycount, sodass sie einigermaßen schnell geladen werden können. Sie werden in einer festen Größe angezeigt und können nicht skaliert werden.
Sind die Inhalte auf die Geräte geladen, kann in der Lehrer Preview gesehen werden, welche Geräte verbunden sind. Außerdem kann verfolgt werden, welche Bereich der Inhalt die Schüler in diesem Moment sehen und ein Punkt gesetzt werden, zu dem dann die Aufmerksamkeit der Schüler gelenkt wird. Die Geräte senden zudem einen Lifestream ihres Bildausschnitts an das ClassVR System, der von der Lehrkraft für jedes Gerät verfolgt werden kann. 
Die Kommunikation zwischen der Lehrer-App und den ClassVR-Headsets funktioniert über WLAN. Das ist auch einer der Punkte, die das System in der Praxis problematisch machen könnten. Werden für alle Schüler Inhalte über das internet übertragen und alle Schüler-Headsets streamen wiederum den Bildschirminhalt ins Internet, muss die Internetverbindung der Schule extrem gut sein. Denn die Datenmengen für 360°-Inhalte sind groß.
% http://www.classvr.com

\subsubsection{Google Expeditions}
Google Expeditions ist eine kostenfreie App, die es für die Smartphone gestützten VR-Headsets entwickelt wurde.
Sie kann einzeln genutzt werden oder im Unterricht. Wird sie im Unterricht genutzt, kann die Lehrkraft eine Guided Tour starten, in die sich dann die Schüler einklinken können. Dazu müssen sie im gleichen Netzwerk sein, in dem das Smartphone der Lehrkraft ist.

Es gibt VR-Touren, die aus 360°Foto-Strecken oder -Videos bestehen und AR-Touren, bei denen die Nutzer 3D-Modelle im Raum begutachten können. Um die Modelle zu zeigen müssen dafür spezielle Marker ausgedruckt werden, die im Kamerabild erkannt werden.

Wird eine Tour geleitet, kann der Tourleiter Markierungen setzen, auf die die Teilnehmer der Tour dann durch Pfeile in ihrem Bild hingewiesen werden. 
Momentan gibt es mehr als 900 verschiedene Expeditionen, die gratis genutzt werden können. Zudem können auch selbst Touren erstellt werden.
% https://edu.google.com/products/vr-ar/

\subsubsection{DiscoveryVR}
% https://www.discoveryvr.com
Die Discovery VR App ist eine Entwicklung des Discovery Channels, in der die Nutzer hochqualitative 360°-Videos ansehen können. Die App ist auf allen VR Plattformen kostenlos verfügbar.

Sie ist keine App, die speziell für das Szenario der Nutzung in der Schule entwickelt wurde, allerdings sind die Videos interessant und zum Einsatz im Unterricht geeignet. Die Videos haben dabei entweder eine englische Audiospur oder sind ohne Erzählstimme.


\subsubsection{Unimersiv}
Unimersiv ist eine App, die es für GearVR und Oculus Rift gibt. Innerhalb der App können unterschiedliche VR-Anwedungen, in denen beispielsweise die ISS erkundet werden kann. Im Moment haben sie 9 verschiedene Experiences im Angebot. Schulen können für einmalig 49\$ pro Headset eine Lizenz erwerben, in der alle verfügbaren Inhalte enthalten sind und auch alle neuen Entwicklungen kostenlos geladen werden können.

Außerdem haben sie auch Simulations-Anwendungen für Unternehmen im Angebot. Eine dieser Apps erlaubt es den Mitarbeitern einer Firma die Steuerung von Staplern in einem sicheren Rahmen zu erlernen.
% https://unimersiv.com


%_______________________________________________________________________________________________________________________
\cleardoublepage % Neue rechte Seite anfangen
\section{Challenges von VR-Systemen}
\label{sec:challenges} 
In ihrem Artikel ``5 Major Challenges For The VR Industry'' von März 2018 sieht Wolwort den Preis, fehlende Inhalte für VR-Systeme, fehlende Businessmodelle, die unklaren Folgen für die Gesundheit der Nutzer und das Image als ``gimmick'' von Virtual Reality als Grund dafür, dass VR bisher noch nicht den prophezeiten Durchbruch hat. \cite{Wolwort2018}

Wie bereits in \ref{subsec:price} diskutiert ist der Preis eigentlich kein valides Argument mehr, seitdem es Smartphone-gestützte VR-Headsets gibt. Sie bieten die Möglichkeit für jeden einen Blick in die Virtual Reality Szene zu werfen.

Der zweite Problempunkt der fehlenden Inhalte ist nach wie vor akut: Abseits von der Welt der Spiele gibt es nach wie vor wenig Anwendungen, die für VR-Headsets entwickelt wurden. Besonders im Bildungsbereich sieht die Autorin eine Zukunft mit Virtual Reality-Apps, die eine komplett andere Wahrnehmung der Inhalte bieten. 
Besonders bei Simulator-Trainings wird bereits seit langer Zeit auf VR gesetzt (QUELLE), diese sind aber meist nicht auf konventionellen VR-Headsets nutzbar. Eine neuere Applikation für VR in der Bildung sind Google Expiditions, wie in \ref{sec:VReducation} genauer erläutert.
Facebook hat außerdem die Facebook Spaces App entwickelt, die es den Teilnehmern eines Treffens erlauben sich mit ihren Freunden auszutauschen, Fotos zu teilen, Videotelefonate zu führen und zusammen in VR zu malen. Das lässt vermuten, dass sie auch an VR Meetingräumen arbeiten, die zukünftig die Zahl der notwendigen Dienstreisen der Mitarbeiter verringern könnten.
% Quelle: https://www.facebook.com/spaces

Drittens sieht Wolfort das fehlen von Business-Modellen, mit denen Unternehmen rentabel VR-Software anbieten können. Das Problem wird noch dadurch verschärft, dass es keinen technologischen Standard gibt, sodass native Anwendungen für jede Plattform einzeln entwickelt werden muss.
Wird die im Rahmen dieser Arbeit entwickelte Software VRClassroom genutzt, könnte ein Business-Modell darin bestehen passende Inhalte für den Unterricht anzubieten wie zum Beispiel ``Das Kolosseum''.

Die für die Autorin wohl kritischste Challenge für Virtual Reality sind die unerforschte mögliche Gesundheitsrisiken und besonders darin, dass sie noch nicht erforscht sind. Bekannt und erforscht ist bisher nur die Virtual Reality- oder Cybersickness, auf die in \ref{subsec:cybersickness} genauer eingegangen wird, die allerdings nur temporär existiert. Langfristige Risiken und Folgen starker Nutzung sind noch nicht erforscht worden. Im Gegenteil werden in der Forschung immer öfter VR-Systeme eingesetzt um Verhalten zu verstehen oder therapieren. Das spricht dafür, dass die Forscher darin keinerlei Gefahr sehen.

Als letztes sagt Wolfort, VR werde momentan nur als ``gimmick'' wahrgenommen. Das ist zu Teilen auch verständlich, da bisher nur wenige Anwendungen abseits von Spielen verfügbar sind. Wie aber bereits oberhalb beschrieben und auch im Rahmen dieser Arbeit entwickelt, gibt es nun immer mehr Apps, die einen Usecase abseits von Games haben. Diese Challenge wird also momentan gemeistert. \cite{Wolwort2018}

\subsection{Virtual Reality Sickness}
\label{subsec:cybersickness} 

Da in der Vergangenheit die Begriffe immer wieder vermischt wurden und dadurch oft unklar sind, werden im Folgenden die Begriffe ``motion sickness'',
``simulator sickness'', ``virtual reality sickness'' und ``cybersickness'' noch einmal differenziert und für diese Arbeit definiert.

\paragraph{Motion Sickness}
Motion Sickness beschreibt die Symptome wie sie zum Beispiel beim Fahren in einem Auto als Beifahrer entstehen können. Bekannt und erforscht ist Motion Sickness vor Allem bei Piloten und Astronauten.

\paragraph{Simulator Sickness}
Simulator Sickness bezeichnet man die negativen Effekte, die man verspüren kann, wenn man einen Simulator irgendeiner Art und Weise verwendet hat. Oft handelt es sich dabei um die Differenz der Simulierten Bewegungen zu denen, die der Körper wirklich erfährt, die dann zu Schwindel und Übelkeit führen können.

\paragraph{Virtual Reality Sickness und Cybersickness}
Virtual Reality und Cybersickness sind Synonyme. Sie bezeichnen die Symptome, die Nutzer erleben können, wenn sie Virtual Reality Systeme wie VR-Brillen verwenden. Sie sind verwandt mit der Simulator Sickness, allerdings beziehen sie sich ausschließlich auf die Folgen von Virtual Reality-Anwendungen.

\bigskip

Symptome von Cybersickness sind Schwindelgefühle, Orientierungslosigkeit, Augenschmerzen, Müdigkeit und Übelkeit. In extremen Fällen kann es auch zu Erbrechen führen. Physiologische Indikatoren von  Cybersickness sind Reaktionen des symphatischen Nervensystems wie elektrodermale Aktivität, die sich durch Hautrötungen und Schweiß äußern können, oder eine erhöhte Herzfrequenz. \cite{Stone2017}
 
Eine Umfrage von 2017, die auf reddit unter VR-Nutzern durchgeführt wurde zeigte sich, dass 60\% der insgesamt 862 Teilnehmer Symptome von Cybersickness erleben. 35\% hatten dabei leichte Symptome, 15\% mittel starke und 8\% stark ausgeprägte Symptome. 2\% der Teilnehmer gab an extrem starke Probleme mit Cybersickness zu haben. \cite{Unknown2017}
Diese Umfrage zeigt, dass Cybersickness nach wie vor ein großes Problem im Umgang mit VR-Headsets ist, an dessen Linderung weiter geforscht werden muss, damit Virtual Reality isch weiter durchsetzen kann.

Die Ergebnisse der Online-Befragung passen zu der Studie, die Stone in seiner Dissertation durchgeführt hat. Bei seiner Befragung mit 202 Teilnehmer erlebten 46\% keinerlei Symptome von Cybersickness, 35\% leichte Symptome, 13\% mittelstarke und 7\% sehr starke Symptome. 
Zudem ermittelten sie, dass Teilnehmer, die im VR-Headset noch eine Brille tragen, deutlich öfter Symptome von Cybersickness erlitten.
Trotz der vielen Personen, die Cybersickness-Symptome erlebten gaben 54\% im Anschluss an die Studie an, wieder VR-Headsets nutzen zu wollen, da sie Spaß an der Nutzung hatten.  \cite{Stone2017}

In seinem Artikel ``Everything You Wanted to Know About Simulator Sickness'' beschreibt der Autor, dass besonders Kinder zwischen 2 und 12 Jahren anfällig für Cybersickness. Außerdem sind laut ihm Frauen öfter von Symptomen betroffen als Männer. Zudem sind Personen, die eine Krankheit wie eine Ohrinfektion, Erkältung oder Grippe haben, sowie Personen die unter Alkohol- oder Drogeneinfluss stehen oder einen Kater haben öfter Cybersickness ausgesetzt. \cite{Burke2014}

Eine relativ einfache Maßnahme um das Auftreten von Cybersickness zu verhindern ist das Field of View der VR-Brillen zu verschmälern. Allerdings hängt der Grad der immersion eng mit dem Field of View zusammen. Wird also das Field of View verkleinert, sinkt die Immersion und die Nutzer haben weniger Spaß an dem Virtual Reality Erlebnis. \cite{Lin2002}

In 2015 hat ein Team der Purdue University zudem herausgefunden, dass Cybersickness dadurch bekämpft werden kann, indem dem Nutzer eine virtuelle Nase angezeigt wird. Sie gibt dem Auge einen fixen Punkt im Bild, der dann hilft die Bewegungen im Rest des Bildes zu verarbeiten. Sie erklären, dass sie durch das Phänomen der Seekrankheit auf die Idee gekommen sind die Nase ins Bild zu integrieren, da es sich genau gegenteilig dazu verhält.\cite{whittinghill2015nasum}

Einen weiteren Ansatz zur Lösung des Problems der Cybersickness hat Eric Bear, der Gründer von MONKEYmedia. Er fand heraus, dass wenn die Bewegungen in der VR-Welt durch Neigen des Kopfes ausgelöst werden als stünde der Nutzer auf einem Hoverboard, erlebten deutlich weniger Probanden Symptome der Cybersickness. Zusätzlich kommt diese Art der Bewegung gänzlich ohne die Nutzung von Controllern aus, sodass die Hände für andere Interaktionen frei bleiben.  \cite{Samit2018}

\subsection{Mindestalter zur Nutzung von VR-Systemen}
Fast alle Hersteller geben in ihren Nutzungsbedingungen oder Sicherheitsanweisungen ein Mindestalter für die Benutzung ihrer VR-Systeme an. Wie in Tabelle~\ref{tab:tableage} zu sehen, sind sich die Hersteller sehr einig, dass VR-Headsets nicht für kleine Kinder geeignet sind und frühestens für Jugendliche in Frage kommen.
\bigskip

Oculus weist konkret darauf hin, dass eine Nutzung ihrer Geräte unter 13 Jahren ihren Nutzungsbedingungen widerspricht und diese erst für diese Altersgruppe entwickelt sind.  ``The Services are intended solely for users who are aged 13 or older. Any registration for, or use of, the Services by anyone under the age of 13 is unauthorised, unlicensed and in breach of these Terms.'' Es werden allerdings keine genauen Gründe für diese Altersrestriktion angegeben.
  ~\cite{FacebookTechnologiesLLC2018}

Samsung geht dabei noch einen Schritt weiter und warnt vor einer Nutzung unter 13 Jahren, da sich jüngere Kinder in einer ``critical period in visual development''  ~\cite{SAMSUNG} befinden. Zudem sollen auch Kinder über 13 Jahren nur unter Aufsicht einer erwachsenen Person die Gear VR benutzen und dabei darauf achten regelmäßig Pausen zu machen. Eine lange Nutzung soll generell vermieden werden und die Kinder sollen während und nach der Nutzung beobachtet werden, ob sich ihre Fähigkeiten in der Hand-Augen-Koordination, Balance oder Multi-Tasking verschlechtern.

Außerdem wird eine Liste an Symptomen aufgeführt bei deren Anzeichen eine Nutzung sofort unterbrochen werden soll. Das sind: ``seizures, loss of awareness, eye strain, eye or muscle twitching, involuntary movements, altered, blurred, or double vision or other visual abnormalities, dizziness, disorientation, impaired balance, impaired hand-eye coordination, excessive sweating, increased salivation, nausea, lightheadedness, discomfort or pain in the head or eyes, drowsiness, fatigue, or any symptoms similar to motion sickness'', also verschiedenste Probleme beim Sehen und an den Augen sowie Probleme der Konzentration, Koordination und Balance.   ~\cite{SAMSUNG}

HTC dagegen gibt für die Nutzung der HTC Vive beziehungsweise Vive Solo kein genaues Mindestalter an. Sie geben allerdings an, dass das Gerät nicht dafür ausgelegt ist von kleinen Kindern genutzt zu werden. Sie warnen davor, dass Kinder Kleinteile verschlucken könnten oder sich und Andere auf anderem Wege damit verletzen können. Für ältere Kinder empfehlen sie die Aufsicht einer erwachsenen Person und dass die Nutzungszeit nicht zu lang ist. \cite{HTC2016}

Zudem wird für die Nutzung der HTC Vive ein HTC Account benötigt, der laut HTC erst ab 14 Jahren erlaubt ist. \cite{HTCCorporation}


In ihren FAQs gibt Sony an, dass man zur Nutzung ihrer Playstation VR Konsole mindestens zwölf Jahre oder älter sein sollte. Weitere Angaben oder Gründe dieses Mindestalter sind auch hier nicht zu finden. \cite{SonyEntertainmentLLC2017}
\bigskip

Gegenüber all der Warnungen der Geräte-Hersteller gibt Martin Banks, Professor of Optometry, Vision Science, Psychologie, and Neuroscience an der University of California in Berkeley in einem Interview im Frühling 2016 an, dass er ``no concrete evidence that a child of a certain age was somehow adversely affected by wearing a VR headset,'' [keine konkreten Beweise, dass ein Kind in einem gewissen Alter durch das Tragen von VR-Brillen negativ beeinflusst wurde] gefunden hat. Er ist überzeugt, dass die Hersteller der VR-Headsets die Nutzung durch Kinder ausschließen, um sicher sein zu können, dass nicht später bekannt werdende Probleme bei Kindern, die VR-Headsets nutzen, ihnen angelastet werden können. 

Weiter gibt er an, dass die Angst, dass die Entwicklung des Auges negativ beeinflusst wird im Gegensatz zur Nutzung von Büchern oder Smartphones viel unproblematischer ist, das durch die in die VR-Brillen eingebauten Optiken das Auge gar nicht auf eine so nahe Sache fokussiert, sondern auf weiter entfernte und somit keine Schäden der Augen nach sich zieht. 

Banks sieht als Gefahren lediglich die gleichen, die auch für Erwachsene bestehen: Das sind hauptsächlich Virtual Reality Sickness, auch bekannt als Cybersickness, und die Gefahr mit Personen oder Gegenständen im Raum zu kollidieren, während das VR-Headset getragen wird.  Ansonsten bewertet er die Nutzung der VR-Brillen von Kindern unproblematisch. \cite{Hill2016}

\bigskip

Momentan gibt es keine veröffentlichten Forschungsarbeiten zu den Gefahren für Kinder bei der Nutzung von VR-Brillen, dagegen sind viele Arbeiten zu finden, die VR-Systeme in der Therapie von verhaltensauffälligen, lernverzögerten oder behinderten Kindern erfolgreich einsetzen. Es gibt beispielsweise Arbeiten, die .... (HIER NOCH GUTE BEISPIELE RAUSSUCHEN)


\begin{table}[]
\begin{tabular}{p{0.2\linewidth}|p{0.2\linewidth}|p{0.5\linewidth}}
\textbf{VR-Gerät} & \textbf{Mindestalter} & \textbf{Weitere Angaben} \\ \hline
Oculus Rift \newline Oculus Go \newline  Oculus Quest & 13 Jahre & keine  \\
HTC Vive \newline HTC Vive Solo & keine genaue Altersangabe & HTC Account erst ab 14 Jahren  \\
Google Daydream & 13 Jahre & keine  \\
Samsung Gear VR & 13 Jahre & nur unter Aufsicht eines Erwachsenen  \newline regelmäßig Pausen machen \newline Warnung vor einer Vielzahl an Symptomen aus dem Bereich Koordination, Balance und Sehen \\
Playstation VR & 12 Jahre & keine  \\
Google Cardboard & keine Angabe & nur unter Aufsicht eines Erwachsenen
\end{tabular}
  \caption{Übersicht der verschiedenen VR-Headsets mit ihren jeweiligen Nutzungsmindestaltern}~\label{tab:tableage}
\end{table}



%_______________________________________________________________________________________________________________________

\cleardoublepage % Neue rechte Seite anfangen
\section{360°-Inhalte}
Neben der verwendeten Hardware spielen die verfügbaren Inhalte für das Virtual Reality Erlebnis eine erhebliche Rolle. Nur wenn die gezeigten Inhalte richtig in der 360°-Umgebung dargestellt werden bringt die Darstellung Nutzung von VR-Headsets die gewünschte Immersion.

Für 360°-Inhalte gibt es keine speziellen Dateiformate, die Inhalte werden in den gewohnten Formaten gespeichert. Grundsätzlich gibt es vier verschiedene Medientypen von 360°-Inhalten: 360°-Fotos, -Videos, 3D-Modelle und spatial Audio, auch bekannt als 3D Audio. 

\subsection{Projektionen von 360°-Fotos und -Videos}
Um 360°-Bildmaterial in 2D Bildformaten zu speichern, werden sie auf verschiedene Arten ``zerschnitten'' oder verzerrt, um dann zur richtigen Darstellung wieder zusammengesetzt werden zu können. Das Panorama wird dazu auf ein zwei-dimensionales Bild gemappt, um problemlos in den bekannten Formaten speicherbar zu sein. Die Information, wie das Panorama gemappt wurde, wird dann in den Metadaten der Datei gespeichert und 360°-fähige Software kann mit diesen Informationen die Panoramas dann wiederum richtig darstellen.

Die Projektionen wurden ursprünglich dazu entwickelt Weltkarten drucken zu können, auf denen die gesamte Erdkugel zu erkennen ist. Sie wurden als ``map projections'' bekannt. Daraus haben sich die heutigen ``image projections'' der Panoramafotografie entwickelt. (Quelle map projections?)

Im Folgenden werden die bekanntesten und am weitesten in der 360°-Szene verbreiteten Projektionen erläutert und verglichen:

Die Bilder beziehen sich auf eine Kugel, bei der die Längen- und Breitengrade alle 10° durch eine Linie dargestellt werden.

\subsubsection{Equirectangulare Projektion}
Verbildlichen könnte man sich die Projektion etwa damit, dass eine Kugel in einem Zylinder sitzt, der den gleichen Durchmesser wie Kugel hat und dann die Pole packt und immer weiter nach außen zieht, bis die Kugel die Form des Zylinders angenommen hat.

Bei der equirectangularen Projektion werden die Gradzahlen von Longitude und Latitude auf die x und y-Werte in einem Grid gemappt. Ein Bildpunkt an (53°/ 90°) wird dann im projizierten Bild an (53,100) dargestellt. Es wird keine zusätzliche Skalierung oder Transformation vorgenommen.
Dadurch werden Äquator-nahe Bildbereiche wenig verzerrt und die Pol-nahen Regionen extrem in horizontaler Richtung verzerrt, sodass die Pole der sphere auf die Breite des Äquators gestrecht werden. 
Vertikale Linien bleiben unverzerrt erhalten, horizontale Linien werden bis auf den Äquator verzerrrt. Dadurch bleiben zudem keine Winkel erhalten.

Die equirectangulare Projektion hat dadurch den Nachteil, dass die Bildelemente an den Polen eine deutlich höhere Auflösung haben als die Bildbereiche am Äquator, was meist diejenigen sind, die interessanter für den Betrachter sind. Außerdem ist durch die starke Verzerrung das Bild an den Rändern schwer zu erkennen.

Trotz diesen Nachteilen ist die equirectangulare Projektion die am weitesten verbreitete Projektion in der 360°-Szene. Das liegt vermutlich daran, dass sie durch das einfache Mapping am leichtesten zu verarbeiten und wieder zu einer Kugel zusammensetzbar ist. 

Die Online-Video-Plattform Vimeo nutzt für 360°-Videos die equirectangular Projektion und auch Youtube hat bis 2017 diese Projektion benutzt bis sie auf ihren eigens entwickelten Standard EAC gewechselt haben.
% https://vimeo.com/blog/post/terms-you-need-to-know-to-create-360-video
% https://youtube-eng.googleblog.com/2017/03/improving-vr-videos.html

\subsubsection{Zylindrische Projektion}
Die zylindrische Projektion hat einen änhlichen Ansatz wie die Equirectangulare. Auch hier kann man sich die Kugel in einem stehen Zylinder vorstellen, der die Kugel am Äquator berührt. Dann wird die Oberfläche der Kugel aus dem Mittelpunkt auf den Zylinder projiziert. Dadurch bleiben vertikale Linien und der Äquator unverzerrt, alle anderen Linien werden allerdings verzerrt. 

Durch diese Projektion werden Bildbereiche die Nahe den Polen extrem in vertikaler Richtung verzerrt, sodass sie viel größer erscheinen, als sie eigentlich sind. Daher ist ein vertikales Field of View, das höher als 120° ist, für zylindrische Projektion nicht zu empfehlen.

\subsubsection{Mercator-Projektion}
Die Mercator-Projektion ist die wohl berühmteste Projektion für Karten, die seit ihrer Entwicklung 1569 für Weltkarten genutzt wird und auch heute noch besonders in der Nautik eine große Rolle spielt. 

Eine Mercator-Projektion sieht letztendlich sehr ähnlich wie eine zylindrische Projektion aus, die Berechnung ist aber ungleich viel komplexer. Der große Vorteil einer Mercator-Projektion ist, dass sie winkelttreu ist. So kann also eine Route zwischen zwei Punkten gezeichnet werden und die Gradzahl, die sich auf der Karte abläsen lässt ist exakt die, die man dann auf dem Kompass verfolgen muss, um ans Ziel zu kommen. Dadurch bewirkte die Mercator-Projektion einen Durchbruch in der Kartografie und Seefahrt. Auch heute sind viele Karten noch in der Mercator-Projektion abgebildet.

Problematisch an dieser Projektion ist wie auch bei der zylindrischen Projektion die extreme Verzerrung von Bereichen, die weit vom Äquator entfernt sind. Sie kann auch keine komplette 180° vertikal-Projektion produzieren, sondern nur bis 85° nördlich und südlich, und deckt damit einen Bereich von 170° an. Diese extreme Verzerrung führt beispielsweise dazu, dass Grönland größer als der ganze afrikanische Kontinent wirkt, obwohl es in der Realität nur etwa einem Vierzehntel der Fläche entspricht.

Varianten der Mercator-Projektion sind bis heute der de-facto Standard für digitale Karten. Auch Google Maps hat lange eine Abwandlung der Mercator Projektion, die so genannte ``Web Mercator Projection'' verwendet. Mit einem Tweet im August 2018 haben sie jedoch ihren Wechsel zu einer Kugeldarstellung angekündigt. Bisher ist diese neue 3D Globe Ansicht allerdings nur auf dem Desktop ausgerollt und nicht in den mobilen Apps zu sehen.
% https://twitter.com/googlemaps/status/1025130620471656449?ref_src=twsrc%5Etfw%7Ctwcamp%5Etweetembed%7Ctwterm%5E1025130620471656449&ref_url=http%3A%2F%2Fgeoawesomeness.com%2Fthe-earth-is-not-flat-google-maps-confirms-with-globe-mode%2F
 
\subsubsection{Rectilinear-Projektion}
Die Rectilinear-Projektion ist anders als die vorigen Projektionen keine zylindrische Projektion, sondern lässt sich verbildlichen indem man eine Kugel auf eine Fläche legt und einen Teil der Kugel vom Mittelpunkt der Kugel auf die Ebene projiziert. Ein einzelnes rectilienares Bild stellt also kein komplettes 360°-Panorama dar. Um ein komplettes Panorama zu erhalten werden dann einfach mehrere rectilineare Projektionen zusammen gesetzt.

Ein großer Vorteil dieser Projektion liegt darin, dass gerade Linien erhalten bleiben. Eine starke Verzerrung in dieser Projektion ist in den Ecken des Bildes zu finden und wird umso stärker, je weiter das Field of View ist. Will man also ein möglichst unverzerrtes Panorama, sollten mehrere rectilienare Bilder mit einem kleineren Field of View zusammengesetzt werden.

Bekannt unter dem Begriff Cubemap Projektion oder Skybox aus der Gaming-Szene ist die Kombination mehrer rectilinearer Bilder. Ein bekanntes Beispiel, bei dem Cubemap-Bilder verwendet werden, sind die Google Streetview-Panoramas. 

\subsubsection{Stereographische Projektion}
Die Stereographische Projektion hat den gleichen Ansatz wie die rectilineare Projektion. Der Unterschied ist, dass nicht aus dem Mittelpunkt der Kugel projiziert wird, sondern aus dem Punkt an der Oberfläche der Kugel, der exakt gegenüber dem Punkt liegt, der die Ebene berührt, auf die projiziert wird. 

Auch mit der stereographischen Projektion ist keine volles 360°-Panorama möglich, sondern nur bis 330°. Empfehlenswert ist es allerdings ein Panorama aus drei 120°-Projektionen zusammenzusetzen. 

Stereographische Projektionen erzeugen immer ein Rundes Bild, das einer Fish-Eye Projektion ähnlich sieht.

\subsubsection{Pannini/ Vedutismo-Projektion}
Die Vedutismo oder Pannini-Projektion ist ebenfalls eine zylindrische Projektion und hat damit dem Vorteil, dass alle vertikalen Linien unverzerrt bleiben. Mit dieser Projektion werden jedoch zusätzlich auch noch alle Linien, die durch das Projektionszentrum gehen ebenfalls als gerade Linien dargestellt. Die Perspektive mit einem zentralen Fluchtpunkt wird dadurch also für einen weiten Blickwinkel möglich.

Die Pannini-Projektion lässt sich als eine rectilineare Projektion einer Kugel auf einen Zylinder vorstellen. Das Projektionszentrum kann dabei überall auf der Sichtachse liegen mit der Distanz d zur Zylinder-Achse. Ist d also d=0, wird eine normale rectilineare Projektion erzielt, geht d gegen Unendlich entsteht eine zylindrische Projektion. Durch die Variation von d werden jeweils unterschiedliche Projektion erreicht. 
Die Pannini-Projektion entspricht dabei d=1.

Eine einzelne Pannini-Projektion ist allerdings für maximal 150° in beiden Richtungen möglich, sodass ein volles Panorama wiederum durch mehrere zusammengesetzt werden muss, ganz ähnlich wie bei der rectilinearen Projektion.

\subsubsection{Equi-Angular Cubemap (EAC)}
Wie bereits erwähnt hat Youtube vor einigen Jahren ihren eigenen Standard für 360°-Bildprojektionen Equi-Angular Cubemap entwickelt. Der Name Cubemap verrät bereits, dass auch dieses Bild wieder aus mehreren Projektionen zusammengesetzt wird. Hierbei werden allerdings die einzelnen Projektionen nicht aus rectilinearen Projektionen zusammengesetzt.

Die Teile der EAC-Cubemap sind sehr ähnlich wir rectilineare Projektionen mit dem Unterschied, dass Bildabschnitte in Äquatornähe genauso hoch aufgelöst werden wie Bildbereiche nahe den Polen. Die Pixeldichte bleibt also an allen Bildbereichen gleich, sodass alle Bereiche des 360°-Bildes eine gleiche Auflösung haben. Letztendlich führt das dann zu einer besseren Videoqualität mit gleicher Auflösung.

% https://blog.google/products/google-ar-vr/bringing-pixels-front-and-center-vr-video/
% Quelle:  https://youtube-eng.googleblog.com/2017/03/improving-vr-videos.html

\subsubsection{Fish-Eye Projektion}
Die Fish-Eye Projektion lässt sich wie die Spiegelung in einer Silberkugel vorstellen. Sie kann eine Szene mit bis zu 180° in beiden Dimensionen darstellen und erzeugt damit ein rundes Bild. Bildbereiche nahe dem Zentrum werden dann vergrößert wahrgenommen, während Bildbereiche nahe den Rändern stark verzerrt werden. Die Fish-Eye Projektion ist meist nicht als End-Projektion sondern eher als Output einer Kamera zu finden, das dann noch weiter verarbeitet wird.
% https://www.cambridgeincolour.com/tutorials/image-projections.htm

Viele 360° Kameras, wie zum Beispiel auch die Rico Theta haben zwei Linsen, die jeweils ein 180°-Fish-Eye Panorama erzeugen. Diese können dann in der Nachbearbeitung zu vollen 360°-Panoramas in einer anderen Projektion zusammengesetzt werden. Eine volles Panorama ist mit einer Fish-Eye Projektion nicht darstellbar und auch das zusammensetzen mehrerer Fish-Eye Panoramen wie bei rectilinearen Projektionen ist nicht möglich
% https://theta360.com/en/about/theta/technology.html

\subsection{360°-Fotos}
Bei 360°-Fotos handelt es sich um Bilddateien, die wenn sie mit Programmen angesehen werden, die 360°-Inhalte korrekt anzeigen können, ein volles Panorama um den Nutzer bilden, in dem er sich dann umsehen kann. 

Die wahrscheinlich bekanntesten Anwendungen, die 360°-Bilder anzeigen sind Google Streetview und 360°-Fotos auf Facebook, die der Nutzer einfach durch bewegen des Smartphones erkunden kann. (Quellen!)

\subsubsection{Quellen für 360°-Fotos}
Es gibt bisher keine Plattform, die sich darauf spezialisiert hat 360°-Fotos, die in der Lehre genutzt werden können, anzubieten. 

Die verbreiteten Anbieter für Stockphotos wie zum Beispiel Shutterstock oder iStockphoto bieten zwar bereits 360°-Fotos an, aber die meisten dieser Stockphotos zeigen keine relevanten Inhalte, sondern nur eine schöne Landschaft oder einen Raum. Sie sind also für zur Nutzung im Unterricht nicht geeignet. Zudem sind diese Bilder oft teuer oder mit einem Abo beim jeweiligen Anbieter verbunden.

Die aktuell beste Quelle für 360°-Fotos ist die Foto-Sharing-Plattform Flickr, auf der die Urheber das Herunterladen ihrer Bilder erlauben und auch die Nutzungslizenz dafür angeben können. Eine Suche nach ``equirectangular'' und der Creative Commons Lizenz ist dort einfach möglich und man wird schnell fündig. 
Flickr hat zudem die Funktionalität die equirectigularen 360°-Fotos richtig darzustellen, sodass die Nutzer sich im Bild umsehen können, indem sie den Viewport verschieben.

Ein engagierter Lehrer hat sogar eine Gruppe gegründet, die speziell dafür gedacht ist, dass Lehrkräfte aus aller Welt ihre 360°-Fotos teilen können, damit Andere sie auch in ihrem Unterricht nutzen können.

% Flickr-Group: https://www.flickr.com/groups/360images4schools/
% Flickr Search equirectangluar + Creative Commons: % https://www.flickr.com/search/?text=equirectangular&license=2%2C3%2C4%2C5%2C6%2C9

Eine weitere Möglichkeit an 360°-Fotos zu kommen, die größtenteils ebenfalls von Hobbi-Fotografen stammen, ist die Google Photo Sphere Community, eine Gruppe auf Google Plus. Allerdings ist hier der Download nicht immer erlaubt und die Rechte für die Bilder auch nicht immer angegeben.
% Google Photo Sphere Community: https://plus.google.com/communities/115970110085205516914/stream/abbc1e71-8239-4ab0-9de3-3f6429e7681f

Außerdem gibt es noch die Möglichkeit 360°-Fotos bei 360cities oder Airpano zu lizensieren. 
360cities ist eine Plattform auf der professionelle 360°-Fotographen ihre Panoramen hochladen und zum Verkauf anbieten können. Dabei handelt es sich oft um sehr gute Bilder, es ist aber relativ teuer, sodass sich das für die Lizenzierung für eine einzelne Lehrkraft nicht wirklich anbietet.

Airpano bietet speziell Bilder von Orten auf der Welt an wie zum Beispiel Städten oder geschichtlichen Orten aus der Luftperspektive an. Die Lizenzierung dieser Bilder ist mit ca 1000 Euro pro Jahr extrem teuer, sodass das für die Nutzung im Unterricht eher nicht in Frage kommt.

% www.360cities.net
% https://www.airpano.com

\subsubsection{Erstellung und Bearbeitung von 360°-Fotos}
Es gibt grundlegend zwei verschiedene Arten 360°-Bilder zu erstellen:  Die eine Möglichkeit, die für den Nutzer deutlich einfacher ist, ist die Verwendung einer 360°-Kamera wie zum Beispiel die Vuze 360, 360 Rize, Rico Theta, Samsung Gear 360, oder Panono 360 Kamera. Diese Kameras haben mehrere Linsen eingebaut und generieren aus den Einzelbildern ein fertiges 360°-Foto, das dann direkt weiterverwendet werden kann.

% https://vuze.camera/vr-software/
% https://www.360rize.com/vr/
% https://theta360.com/de/
% https://www.samsung.com/de/wearables/gear-360-r210/
% https://www.panono.com/en/home/

Die zweite Möglichkeit ist mehrere normale Kameras auf ein Rig, also ein Stativ für mehrere Kameras, zu installieren, bei dem jede Kamera einen anderen Bildabschnitt abdeckt und die Bilder dann nachträglich manuell zusammenzusetzen. 
Den Vorgang des Zusammensetzens der Bilder nennt man Stitching. Erstellt man 360°-Fotos durch zusammenstitchen mehrerer ``normaler'' Kamerabilder ist es wichtig, dass dann die richtigen Metadaten zum Bild hinzugefügt werden, damit Software, die 360°-Bilder anzeigen kann, weiß, dass es sich um ein 360°-Bild handelt und welche Projektion es hat.

360°-Bilder, die direkt von einer 360°-Kamera kommen, haben bereits die passenden Metadaten und sie müssen nicht manuell hinzugefügt werden.

Die Metadaten können auf verschiedene Art und Weisen zu den Fotos hinzugefügt werden, unter anderem mit Photoshop oder mit dem ExifMetaLrPlugin für Lightroom. 
In den Metadaten sind ist gespeichert, welche Software zum Stitching verwendet wurde, welche Dimensionen das Bild hat, was die Ausgangsblickrichtung ist und natürlich welche Projektion das Panorama hat und dass es in einem Panorama Viewer angezeigt werden soll. 
Zusätzlich können dort auch noch die Koordinaten, an denen das Bild aufgenommen wurde, der Urheber, das Copyright und einige andere Informationen gespeichert sein, die jedoch nicht spezifisch für 360°-Bilder sind.

% https://www.panotwins.de/technical/how-to-add-mandatory-photo-sphere-meta-data-to-an-equirectangular-image/

\subsection{360°-Videos}
In seinem Artikel ``360 Videos und ihre Zukunft: eine Prognose'' im filmpuls-Magazin, einem Fachmagazin für Filmemacher, erklärt der Autor Kristian Widmer, dass die größte Challenge für die Produktion von guten 360°-Filmen ist, dass komplett eigene Erzählkonzepte entwickelt werden müssen, da die aus dem klassischen Filmformat nicht anwendbar sind. Außerdem können viele Tricks und Praktiken nicht angewandt werden, weil es keine unsichtbaren Bereiche um die Kamera gibt.

Seiner Meinung nach sollten 360°-Filme so angelegt werden, dass der Zuschauer dazu ermuntert wird die Szene zu erkunden und dazu auch 3D-Sound intgeriert werden sollte, um zu suggerieren wo im Panorama gerade interessanter Inhalt zu sehen ist.

Die großen Videoplattformen Vimeo und Youtube haben auch seit einigen Jahren die Funktion 360°-Videos abzuspielen und bieten auch Möglichkeiten die Filme in VR-Headsets zu sehen. Widmer kritisiert hier allerdings die Qualität der Filme und die Tatsache, dass oft nur Stereo-Sound in die Filme integriert ist.
% https://filmpuls.info/360-videos-zukunft/

Allerdings sind erst durch die Angebote von Youtube und Vimeo 360°-Filme und ihre Möglichkeiten weiter in das Bewusstsein der breiten Masse gekommen.

\subsubsection{Quellen für 360°-Videos}
Eine gute Quelle für 360°-Videos is genau wie bei 360°-Fotos Vimeo. Wie bereits oben beschrieben, können die Eigentümer der Videos angeben unter welcher Lizenz sie das Video veröffentlichen und es gegebenenfalls zum Download freigeben. In den meisten Fällen sind sie frei zur privaten Nutzung und nur die kommerzielle Weiterverwendung untersagt.

Auch bei Youtube gibt es viele interessante 360°-Videos. Insgesamt ist es aber nicht so einfach direkt auf der Plattform die Videos zu finden, die interessant sind, da oft bei der Such innerhalb der Plattform normale Videos hineingespült werden. 
Youtube lässt den Download der Videos nicht zu, allerdings stehen Apps auf fast allen Geräten und Plattformen zur Verfügung: die Youtube VR-App gibt es im Oculus und SteamVR-Store, die iOS und Android Youtube App kann die 360° Videos ebenfalls für das Abspielen in VR-Headsets anzeigen.
Da bei Youtube immer ersichtlich ist, wer der Urheber des Videos ist, besteht hier allerdings die Möglichkeit in Kontakt zu treten, um zu Erfragen, ob das Video für die Nutzung im Unterricht bereitgestellt werden könnte.

Auch bei Facebook gibt es 360°-Videos. Wie bei fast allen anderen Videoplattformen sind sie auch hier nicht zum Download freigegeben. Sie sind mit der Facebook App auf den mobilen Geräten, der Facebook 360 App im Oculus Store und im Browser anzusehen. Im VR-Modus mit Bilder für jedes Auge einzeln kann man die Videos allerdings nur in der Facebook 360 App ansehen.

Die BBC hat sogar eine App veröffentlicht, die exklusiv VR-Inhalte für ihre Leser bereitstellt und speziell für die Nutzung mit dem Google Cardboard entwickelt wurde. Darin zeigen sie von gefilmten 360°-Reportage bis hin zu 360°-Animationsfilmen Geschichten, die speziell für das 360°-Medium entwickelt wurden.
% https://www.theguardian.com/technology/ng-interactive/2016/nov/10/virtual-reality-by-the-guardian

Auch die New York Times hat einige Beiträge seit 2016 in 360°. Die Videos können im Browser oder in der New York Times-App angesehen werden. Der Player unterstützt allerdings keine VR-Headsets, die Smartphones können also nur genutzt werden um sich in der Szene umzusehen indem sie in der Hand gehalten und bewegt werden.
Ein Download ist auch hier nicht vorgesehen.
% https://www.nytimes.com/2016/11/01/nytnow/the-daily-360-videos.html

Eine schöne Quelle deutschsprachiger 360°-Videos gibt es vom ZDF, leider sind diese allerdings ebenfalls nicht zum Herunterladen, sondern nur über die ZDF VR-App, die es für Android, iOS und GearVR gibt, oder im Browser anschaubar.
% https://vr.zdf.de

Lizenzierbare 360°-Videos gibt es wie auch Fotos bei 360cities und Airpano. Diese sind aber eher gedacht um in Filme eingebaut zu werden, als einzeln verwendet zu werden. Sie sind also eher vergleichbar zu Stockphotos und damit wirklich nicht im Bildungsbereich verwendbar.
% www.360cities.net und https://www.airpano.com

In die gleiche Kategorie fallen dabei die Videos von Videoblocks.com, die ebenfalls einige schöne 360°-Videos haben, die aber keinen wirklichen Inhalt haben, sondern eher als Bildmaterial für die Weiterverarbeitung gedacht sind. Alle drei Anbieter sind zudem nicht besonders günstig.
% https://de.videoblocks.com/videos/footage/360-files

\subsubsection{Erstellung und Bearbeitung von 360°-Videos}
https://de.wikipedia.org/wiki/360-Grad-Video

Entweder direkt 360 Kamera oder mehrere und dann stitchen (stitchen besser bei statischen Inhalten)

\subsection{3D-Modelle}
3D Modelle können einen Blick auf Dinge gewähren, die so nicht so einfach angesehen werden. Beispielsweise können damit Molekülstrukturen oder menschliche Organe gezeigt und erforscht werden, wie sie wirklich aussehen. Da es bereits eine große Szene für 3D-Modelle gibt, ist es nicht schwierig für die verschiedensten Themengebiete passende Modelle zu finden. Auch für den Bildungsbereich gibt es dort einiges zu finden, was man im Unterricht verwenden kann.

\subsubsection{Quellen für 3D-Modellen}
3D-Modelle sind die einzige Art von Inhalten für 360°, für die es Webseiten gibt, über die man Modelle suche und herunterladen kann. Dabei kann man nach Output-Formaten, Polycount, Lizenzen und Preisen Filtern, sodass man für alle Anwendungsbereiche die passenden Modelle finden kann.

Anbieter gibt es dafür viele. Im Laufe der Entstehung dieser Arbeit sind unter Anderen folgende Webseiten genutzt worden:

\begin{itemize}
      \item Turbosquid.com
      \item cgtrader.com
      \item Sketchfab.com
      \item free3d.com
   \end{itemize}

Da für Computerspiele, neuere Animationsfilme und für 3D-Druck bereits seit Jahren viele 3D-Modelle benötigt werden, hat sich hier anders als bei den anderen Inhalten bereits eine Szene gebildet, in der Modelle erstellt und dann verkauft oder verschenkt werden.
   
\subsubsection{Erstellung und Bearbeitung von 3D-Modellen}
Das Erstellen von 3D-Modellen ist anders als bei den anderen Inhalten deutlich aufwändiger. Man muss sich dafür zuerst in eines der 3D-Modellierungsprogramme hineinarbeiten und auch das Erstellen der Modelle ist sehr zeitaufwändig. 

Es gibt viele verschiedene 3D-Modellierungsprogramme, die auch alle etwas unterschiedlich zu benutzen sind:
Beliebte Programme sind Autodesk 3ds Max, Blender, Cinema 4D, Paint 3D. Das einfachste dieser Programme ist wohl Paint3D, das allerdings auch den geringsten Funktionsumfang hat und nur auf Windows-Rechnern mit Windows 10 benutzbar ist.

Blender ist als gratis Open-Source-Software das wahrscheinlichsten verbreiteste Programm überhaupt, da es so den Nutzern die Möglichkeit bietet in die Welt der 3D-Modellierung hineinzuschnuppern.

Da man mit der Maus in all diesen Programmen sowohl den Viewport steuert, als auch Funktionen auslöst, ist es ratsam eine gute Mouse, am besten sogar eine 3D-Maus zu verwenden.

Da es so viele verschiedene Programme gibt, gibt es ebenso viele Output-Formate für die fertigen Modelle. Jede Software hat eigene Speicherformate, aber die meisten können auch in andere exportieren. Den Export zu OBJ-Dateien, unterstützen dabei alle Programme und ist damit eine gute Speicherform, um 3D-Modelle zu veröffentlichen.

\subsection{360° Sound/ Spatial Audio}
360° Sound, 3D audio oder auch spatial audio bezeichnet Audioinhalte, die Töne die an verschiedenen Orten um den Kopf platzieren und so eine reale Situation erschaffen. 
Das menschliche Gehör ist in der Lage für Soundquellen zu bestimmen von wo sie kommen. Dazu werden die drei Dimensionen der Töne bestimmt, die vom Kopf als Bezugspunkt im Zentrum ausgehen. Die Dimensionen sind die Entfernung vom Kopf, der Azimuth-Winkel und der Elevationswinkel.
Der Azimuth-Winkel beschreibt dabei die horizontale Rotation und der Elevationswinkel die vertikale Rotation. 

Es gibt drei verschiedene Formate für spatial audio: Channel-based, object-based und transform domain-based spatial audio.

Momentan ist das Channel-basierte Audio-Format am weitesten verbreitet. Channel-basierte audio-Inhalte sind playback-orientiert, brauchen als nur minimales Processing beim Abspielen und können direkt an die Lautsprecher gegeben werden, da das Mixing bereits im Vorfeld von einem Sound Engineer vorgenommen wurde. Dabei muss die Standard Konfiguration der Lautsprecher beachtet werden, damit das spatial audio dann auch wie gewünscht funktioniert. Zudem muss die Anzahl der Channels mit dem des Output-Systems, also der Anzahl der Lautsprecher übereinstimmen.
Beispiele für Channel-basierte spatial Audio sind Dolby Atmos und Auro 3D.

Object-basiertes spatial Audio besteht aus einer Soundszene und einfach Soundobjekten. Diese Soundobjekte haben alle eigene Metadaten in denen unter anderem ihre Position, Lautstärke, Richtung und Gedämpftheit gespeichert sind. Die Soundszene kann dann mit den verschiedensten Abspiel-Setups genutzt werden, da sie keine festgelegte Konfiguration braucht. Dafür ist es extrem aufwändig objekt-basiertes spatial audio in Realtime zu berechnen. Inzwischen wird objekt-basiertes spatial audio immer beliebter, da die Geräte immer stärkere Prozessoren haben und damit die aufwändige Berechnung problemlos schaffen.

Das transform domain-based spatial audio wird auch als Ambisonics bezeichnet und beruht auf den zuvor beschriebenen Dimensionen der Lage der Tonquellen. Da die Richtungen nicht direkt in Kanälen abgespeichert werden wie beim Channel-basierten spatial audio, kann auch hier ein variables Abspielsystem verwendet werden.

% Paper: Spatial Audio Reproduction with Primary Ambient Extraction

% https://flypaper.soundfly.com/produce/wtf-vr-and-spatial-audio-mixing-360-audio/
% sehr guter Artikel, aber Information bisher alle aus dem Paper

\subsubsection{Quellen für Spatial Audio-Dateien}
360° Videos auf Youtube können bereits mit spatial Audio hochgeladen werden. Hier könnten die Audiospuren als 3D-Sounddateien heruntergeladen werden. Eine Plattform für den Vertrieb von Spatial Audio Dateien gibt es allerdings nicht.

\subsubsection{Erstellung und Bearbeitung von Spatial Audio}
Um Channel-basierten spatial audio content zu erstellen werden 8 Channels benötigt. Pro Achse (Vorne/Hinten, Links/Rechts, Oben/Unten) wird ein Stereo-Signal erzeugt. Hinzu kommt noch ein Stereo-Signal für den Teil der Töne, die aus keiner Richtung kommen, sondern direkt auf dem Zentrum sitzen. Das sind dann meistens Inhalte wie eine Hintergrundmusik.

Es gibt verschiedene Tools, um Spatial Audio Dateien zu erzeugen. Die meisten größeren Audiosoftwareprodukte haben zudem Plugins für 3D Audio. Beispiele für kostenfreie Tools sind die Spatial Workstation von Facebook oder das Ambisonic Toolkit für Reaper.
% https://facebook360.fb.com/spatial-workstation/
% http://www.ambisonictoolkit.net

In der Entwicklung von VR-Anwendungen wird der 3D Sound meist dadurch erzeugt, dass Soundquellen direkt als Objekte in den virtuellen 3D-Raum platziert werden. Die eingebundene Audiodateien sind dabei dann nur normale Mono- oder Stereodateien.
% https://developers.google.com/vr/ios/spatial-audio
%https://facebook.github.io/react-360/docs/audio.html

\subsection{Virtual Reality Spiele und Anwendungen}
Die Kombination aus den vorigen Inhalten ergibt VR-Anwendungen und Spiele. Inzwischen ist die Auswahl an verschiedenen Anwendungen extrem gewachsen und tut das auch weiterhin. Allerdings ist der hier nach wie vor der allergrößte Anteil an VR-Programmen Spiele.

\subsubsection{Quellen für Virtual Reality Anwendungen}
Auf jeder Plattform von VR-Geräten gibt es eigene Stores, um Virtual Reality-Anwendungen für die jeweilige Plattform zu finden. 
Bei HTC Vive und HTC Focus ist das der SteamVR Store \cite{ValveCorporation}, bei den Oculus Geräten der Oculus Store \cite{FacebookTechnologiesLLC} und Anwendungen für die Playstation VR sind im Playstation Store zu finden. \cite{SonyInteractiveEntertainmentEuropeLimited}

Da WebVR plattformunabhängig ist, gibt es dafür keinen ``Standard''-Store auf dem Gerät, hier ist es relativ schwierig, Apps zu finden. Eine Möglichkeit, mit allerdings sehr begrenzter Auswahl ist auf itch.io nach ``WebVR'' zu filtern. \cite{Itchcorp}

Außerdem gibt es noch wearvr.com, einen unabhängigen Virtual Reality App Store, der Sammlungen für Anwendungen für alle Plattformen haben. \cite{WEARVR}


\subsubsection{Erstellung von Virtual Reality Anwendungen}
Um Virtual Reality Anwendungen zu erstellen, gibt es zwei grundlegend verschiedene Ansätze: 

Zum einen können die Anwendungen für die verschiedenen Plattformen oder als WebVR Apps entwickelt werden. Dazu muss die Person die jeweilige Programmiersprache beherrschen und einiges an Zeit investieren, kann aber dafür eine Anwendung entwickeln, bei der alles so ist, wie es die entwickelnde Person sich wünscht.

Weitere Web-basierte Möglichkeiten VR-Applikationen zu entwickeln sind A-Frame, React360 oder Three.js. Genauere Erläuterungen dazu finden sich in \ref{subsec:webvr}.

Alternativ können Anwendungen auch nativ entwickelt werden, zum Beispiel als Android-Anwendung, die dann in der Google Cardboard-App Nutzern zur Verfügung gestellt werden kann.

Außerdem besteht die Möglichkeit die Virtual Reality-Apps auf die klassische Weise mit einer Game Engine zu entwickeln. Das kann entweder mit Unity und C\# geschehen oder der Unreal Engine mit C++ und Blueprints. Diese Game Engines können dann den Code für die jeweilige Plattform kompilieren und man erhält native Apps.
Genauere Ausführungen dazu sind wiederum in \ref{subsec:unity} beziehungsweise \ref{subsec:unreal} zu finden.

\bigskip

Die zweite Möglichkeit ist die Nutzung von Tools wie zum Beispiel InstaVR oder Vizor360. Mit diesen Programmen können auch Personen, die keinerlei Programmierkenntnisse haben einfache 360°-Anwendungen erstellen. Dazu werden einfach 360°-Fotos oder -Videos hochgeladen, mit dem Tool zu einer Tour verbunden und schon ist eine einfach Anwendung fertig. So könnte zum Beispiel eine kleine Tour über den Campus einer Uni erstellt werden, sodass mögliche zukünftige Studierende besser erfahren können, wie der Campus aussieht. \cite{Vizor2019} \cite{InstaVRInc}
Außerdem können auch noch Textelemente eingefügt werden, um zum Beispiel auf einem Foto zu markieren, welche Gebäude zu sehen sind, um dem Betrachter noch etwas mehr Kontext zu geben.

\subsection{3D 360°}
Um ein 3D Bild zu erzeugen müssen zwei leicht verschobene Bilder angezeigt werden, die um den Augenabstand eines Menschen horizontal verschoben sind. Soll also ein 3D Bild oder Video produziert werden, werden zwei Kameras um den Augenabstand versetzt auf ein Stativ montiert und und dadurch ein Bild pro Auge produziert.

Wird ein 360° Bild erstellt nimmt eine Kamera oder ein Rig mit mehreren Kameras wie in Grafik \ref{fig:3d-360} links zu sehen ein Bild der kompletten Umgebung auf, in dem sich dann der Betrachter umsehen kann.
Würde man nun also einfach zwei 360° Bilder um den Augenabstand versetzt aufnehmen hätte man das Problem, dass nur in einem Blickwinkel der Abstand stimmt, sodass ein 3D Bild entsteht. Ist der Betrachter um 90° gedreht, zeigen die beiden Bilder exakt das Gleiche und ist er um 180° gedreht, sind die beiden Bilder in die andere Richtung verschoben. So kann also kein 3D für 360° produziert werden.

Für 360° Fotos können die zwei Kameras auf ein drehbares Rig montiert werden, sodass jede Kamera aus den Fotos, die sie gemacht hat ein 360° Bild errechnet. Da allerdings durch die Drehung nicht durchgehend aufgenommen werden kann ist diese Technik nur für die Produktion von Fotos geeignet.
% Paper: Bourke: Synthetic stereoscopic panoramic images

Grundsätzlich gibt es zwei Ansätze, wie 3D 360° Videos produziert werden können. Für beide braucht man ein Setup von mindestens acht Kameras, um ein brauchbares Bild zu erzeugen.
Die erste Möglichkeit ist ein Rig-Aufbau wie in \ref{fig:3d-360} in der Mitte zu sehen. Es besteht aus acht Kameras, von denen jeweils zwei parallel zueinander ausgerichtet sind und die Paare wiederum jeweils 90° zueinander ausgerichtet sind.
Werden nun jeweils die Bilder der linken Kameras und die der rechten Kameras zusammengestitcht, entsteht ein 360° Bild pro Auge, das beim Drehen den Abstand der Augen behält.
Dieses Setup funktioniert sehr gut, wenn der Betrachter in eine der vier Richtungen schaut, in die die Kameras zeigen, dazwischen verschiebt sich der Augenabstand jedoch wieder, sodass hier Bildbereiche entstehen, die wiederum nicht korrekt in 3D angezeigt werden.
Zudem könnte das ständige Verschieben des Augenabstandes dazu führen, dass die Nutzer schneller Symptome von Cybersickness erleben.

Die zweite Möglichkeit die acht Kameras zu nutzen ist in \ref{fig:3d-360} dargestellt. Die acht Kameras werden sternförmig mit jeweils einer Rotation von 45° zueinander auf das Rig montiert. Um daraus dann das korrekte 3D 360°-Bild zu bekommen, werden die Kamerabilder jeweils in einen rechten und einen linken Bildbereich aufgeteilt. Dann werden alle linken Bildbereiche zum Bild für das linke Auge zusammengestitcht und alle rechten Bildbereiche zum Bild für das rechte Auge.

\begin{figure}[h]
  \includegraphics[width=\linewidth]{images/3d-360.eps}
  \caption{Aufbau von 360° Rigs.}
  \label{fig:3d-360}
\end{figure}


% https://www.youtube.com/watch?v=0OKBjkhdnOc
% Paper: Omnistereo: Panoramic stereo imaging


\subsection{360°-Inhalte in React360}
Die VR-App innerhalb von VRClassroom wurde mit React360 entwickelt. Nachfolgend wird darauf eingegangen, wie 360°-Inhalte in React360 eingebunden werden können.

\subsubsection{Fotos und Videos}
React360 ermöglicht sowohl die Nutzung von 180°- also auch 360°-Panoramas für Videos und für Fotos. Ebenfalls für beide Medientypen gilt, dass React360 mono und stereo Signale verarbeiten kann.

Die Fotopanoramas in 360° müssen entweder in equirectangularer Projektion oder im 1x6 Cubemap-Format vorliegen. Dabei besteht das Gesamtbild aus sechs unverzerrten Einzelbildern, die in der Reihenfolge rechts, links, oben, unten, vorne, hinten angeordnet sind.
In der equirectangularen Projektion kann das Bild auch stereo sein. Dazu müssen die zwei Bilder übereinander angeordnet sein, diese Anordnung ist als ``top/bottom''-Layout bekannt. (Cubemap-Panoramas können nur als mono-Signal verarbeitet werden.)

180°-Panoramen sind nur equirectangular möglich. Das Stereobild kann hierbei neben dem ``top/bottom''-Layout auch in ``left/right'', also nebeneinander vorliegen.

Bei den möglichen Formaten für Videos verhält es sich ganz ähnlich wie bei den Fotos: equirectangulare Panoramas können mono oder stereo sein. Sind sie 360°-Panoramen im ``top/bottom''-Layout, sind sie 180° im ``left/right''-Layout. 360° Panoramen können wiederum auch im Cubemap-Format sein, bei Videos allerdings in 3x2.

/// Hier noch genauer auf 3x2 (+ 1\% expansion) eingehen??

Wenn ein Foto stereo eingebunden werden soll, muss der Parameter ``format'' auf einen der folgenden Werte gesetzt werden:

\begin{itemize}
\item \textbf{2D}: [Defaultwert] Für Monobilder
\item \textbf{3DTB}: Stereobilder, bei denen das linke Bild oben ist und das rechte unten
\item \textbf{3DBT}: Stereobilder, bei denen das linke Bild unten ist und das rechte oben
\item \textbf{3DLR}: Stereobilder, bei denen das linke Bild links ist und das rechte rechts
\end{itemize}

Die Fotos und Videos können entweder von der Runtime oder aus der React360-App selbst gesetzt und gesteuert werden. Um sie im Panorama sehen zu können müssen sie als Hintergrundbild beziehungsweise -video gesetzt werden.

Um Videos abzuspielen muss zuerst ein player erstellt werden und der Pfad zur Video-Datei übergeben werden. Danach können auf dem Player die Steuerungsfunktionen zu den Videos aufgerufen werden. \cite{React360video}

\subsubsection{Sounds}
In React360 gibt es zwei grundlegende Arten Sounds einzubinden:

Zum Einen können Audiodateien als Hintergrundaudio oder Soundeffekt eingebunden werden. Hier wird auf dem AudioModule die Funktion playEnvironmental() mit dem Pfad und der gewünschten Lautstärke übergeben. Die Audiodateien werden dann als Loop abgespielt und erst beendet, wenn stopEnvironmental() aufgerufen wird. Soundeffekte werde mit .playOnShot() einmalig abgespielt.

Zum Anderen können Sounds in der 360°-Szene an einem bestimmten Ort installiert werden. Dazu muss die .createAudio()-Funktion aufgerufen werden. Sie erwartet einen ``handle'', den Pfad zur Audiodatei und ``is3d'' muss auf true gesetzt werden. Im Aufruf der play-Funktion wird dann die gewünschte Location übergeben, an der der Sound abgespielt werden soll. \cite{React360audio}

\subsubsection{3D-Modelle}
3D-Modell - in React360 als ``Entities'' bezeichnet - müssen an eine Location gerendert werden. Während Panoramabilder und -videos von überall als Hintergrundbild beziehungsweise -video gesetzt werden können, muss der Component, der die Entities beinhaltet in der auf eine Location gerendert werden.

Die Location des beinhaltenden Components ist das der Nullpunkt für die Entities im Component, sie werden dann also relativ dazu gesetzt.

Um Modell zu rendern muss nur eine Entity-Komponente erstellt werden und er Link zu dieser Entity übergeben werden. Mit einem transform-Attribut können sie dann noch skaliert, rotiert und verschoben werden.

3D-Modelle können als gltf2-Dateien (Dateiendung .gltf oder .glb) oder als OBJ-Dateien vorliegen. Bei OBJ-Modellen muss auch noch die zugehörige MTL-Datei übergeben werden, die die Materialdefinitionen enthält. \cite{React360models}

%\_____________________________________________________________________

\cleardoublepage
\section{Software Entwicklung von VR-Systemen}
Wie bereits erwähnt gibt es verschiedene Software-Produkte, die es auch dem Laien ermöglichen simple VR-Anwendungen wie eine 360°-Fotostrecke zu erstellen. Sollen allerdings komplexere VR-Anwendungen entstehen, müssen VR-Apps programmiert werden. Dafür gibt es zwei grundlegend verschiedene Möglichkeiten. Entweder können native VR-Apps für die verschiedenen VR-Plattformen entwickelt werden oder auf WebVR gesetzt werden, das im Browser läuft und somit mit allen Plattformen kompatibel ist.

\subsection{Native VR-Applikationen}
Um VR-Anwendungen, die nativ auf den VR-Headsets laufen, zu entwickeln gibt es momentan nur zwei verschiedene Möglichkeiten. Das sind die Game-Engines Unity und Unreal. Wie aus dem Namen schon erkennbar, wurden sie ursprünglich entworfen, um normale Computer und Konsolenspiele zu programmieren. Sie wurden aber weiterentwickelt, sodass sie sich inzwischen auch sehr gut zur Entwicklung von VR-Anwendungen eignen.

Ingesamt sind sich die beiden Engines relativ ähnlich, verwendet man Unity, muss man C\# programmieren und die Unreal Engine verwendet so genannte Blueprints, um einfachere Sachen visuell programmieren zu können mit einem Editor, in dem Nodes mit Links verbunden werden können um Funktionen zu nutzen. Darunterliegend wird in der Unreal Engine mit C++ gearbeitet.

\subsubsection{Unity}
\label{subsec:unity}
Unity: C\#
\subsubsection{Unreal Engine}
\label{subsec:unreal}
Unreal: C++
\subsection{WebVR - VR im Browser}
\label{subsec:webvr}

WebVR ist eine offene Spezifikation, die es erlaubt auch im Browser VR-Anwendungen zu nutzen.
Dazu wird die Display-Fläche in der Mitte halbiert und WebVR rendert die VR-Anwendungen je Auge in einem linsenförmigen Format, sodass sie mit einem VR-Headset genutzt werden können.

Um die VR-Ansicht zu aktivieren wird ein kleines Icon in der Ecke des Displays angezeigt, wenn die App geladen wird. Die WebVR-Ansicht gibt es nur im Landscape-Modus.
% https://webvr.info

WebVR gibt der App zusätzlich die Funktionen die Position und Ausrichtung, sowie die Bewegungsdaten des Nutzers, damit voll funktionsfähige entwickelt werden können.

\subsubsection{A-Frame}
% https://aframe.io/docs/0.8.0/introduction/
A-Frame ist ein Framework, das ursprünglich von Mozilla entwickelt wurde, mit dem Virtual Reality und Augmented Reality Apps entwickelt werden können.

A-Frame basiert auf HTML, ist aber trotzdem ein Komponenten-basiertes Framework, das eine erweiterbare deklarative Struktur zur Nutzung von Three.js bietet. Es kommt bereits mit vielen Core Komponenten, diese können aber selbstverständlich durch eigene Komponenten erweitert werden.
Es kann VR-Apps mit bis zu 90 FPS bauen und kommt mit einem visuellen 3D Inspector, der es einfach macht die VR-Szene zu überblicken.

Mit A-Frame können auch Apps entwickelt werden, die positional Tracking nutzen, also sechs Freiheitsgrade haben. Zudem werden die Controller der VR-Headsets unterstützt.
\subsubsection{React360}
React360 nutzt ebenso wie A-Frame Three.js für einen Teil des Renderings. React360 ist wie A-Frame komponenten-basiert und kommt mit einigen Core-Components, die durch eigene Komponenten erweitert werden können.
Um eine fertige React360-App zu bundeln wird Node.js genutzt.

React360 bietet mit der React 360 CLI Die Möglichkeit mit dem ``react-360 init ProjectName'' Kommando im Terminal den initialen Code für das VR-Projekt zu generieren. Es wird das ProjectName-Projekt erzeugt, das direkt in der nötigen Software-Struktur mit allen Libraries erstellt wird. Auch die Development Server werden hier direkt mitgeliefert.

Eine App besteht grundsätzlich erstmal aus drei Dateien. Das sind die index.html-Datei, in der der Javascript-Code eingebunden wird, die index.js-Datei, in der der eigentliche React360-Code ist, und die client.js, die die Runtime für die React360 App bildet. 
Die HTML-Datei wird nur dazu genutzt, den Javascript-Code einzubinden. 
In der client.js wird der React360 Code in den DOM eingebunden. Hier werden auch die Kompontenten auf Locations oder Surfaces gemounted.

React360 baut auf React Native auf. Es werden zum Beispiel das Konzept der Stylesheets übernommen, die ein einfaches Styling der Komponenten erlauben, das dem Styling bei HTML sehr ähnlich sieht, und nutzt zum App Bundling ebenfalls den Metro Bundler.
% https://facebook.github.io/react-360/docs/what-is.html

\subsubsection{Vergleich von A-Frame und React360}

% https://facebook.github.io/react-360/docs/what-is.html
Warum für React360 entschieden?
\subsubsection{Three.js}
Three.js ist ein Javascript Framework mit dem 3D Szene erstellt werden können. Um Objekte in die Szene zu setzen müssen sie dem Document Object Tree hinzugefügt werden. Auch die Kamera muss als Objekt manuell hinzugefügt werden und ein Field of View, das Seitenverhältnis und die ``near'' und ``far''-Werte mit übergeben werden. Diese Werte bezeichnen den Bereich der z-Werte, der gerendert wird. Liegt ein Objekt außerhalb dieses Intervalls, ist es nicht in der Szene zu sehen.

Three.js wird standardmäßig mit dem WebGL-Renderer gerendert, bietet aber auch andere Renderer als Fallback für inkompatible Browser.

Um VR-Apps mit Three.js zu bauen, muss zuerst WebVR eingebunden werden. Dann kann ein WebVR Button hinzugefügt werden, der dann die VR-Ansicht startet. Zudem muss der Animations-Loop angepasst werden. Für VR-Apps muss statt der ``requestAnimationFrame''-Funktion die ``setAnimationLoop''-Funktion genutzt werden.

Soll ein Objekt wie zum Beispiel ein Würfel in die Szene gesetzt werden, sind mehrere Schritte erforderlich. Zuerst muss mit THREE.BoxGeometry(1,1,1) eine Rechteckige Form und mit THREE.MeshBasicMaterial({color: 0x00ff00})ein Material erzeugt werden. Danach wird der Würfel erzeugt in dem das geometrische Objekt und das Material mit THREE.Mesh( geometry, material ) zusammengefügt werden. Zuletzt muss der erzeugte Würfel der Szene hinzugefügt werden. Damit ist Three.js eine sehr low-levelige Möglichkeit VR-Apps zu entwickeln. Dadurch stehen viele Möglichkeiten offen, es ist jedoch deutlich aufwändiger einfache VR-Szenen zu bauen. Andere VR-Frameworks wie A-Frame oder React360 nutzen im Hintergrund Three.js und bieten den Entwicklern einfache Funktionen, um Objekte in die VR-Szene zu setzen.

% https://threejs.org/docs/#manual/en/introduction/Creating-a-scene

\subsubsection{PlayCanvas}

Eine spannende Möglichkeit VR-Applikationen zu entwickeln ist die Nutzung von PlayCanvas. PlayCanvas ist eine Web-basierte Game-Engine, in der mit HTML und WebGL entwickelt werden kann. Das besondere ist, dass in Real-Time kollaborativ gearbeitet werden kann und da es keine Kompilierzeit gibt die Veränderungen auch direkt auf den Geräten angezeigt werden. PlayCanvas ist WebVR kompatibel und ist damit sehr gut geeignet VR-Anwendungen für Smartphone-gestützte VR-Headsets zu entwickeln. \cite{PlayCanvasLtd}

%\_____________________________________________________________________



\cleardoublepage
\section{Software Projekt: VRClassroom}
Aufbauend auf den Erkenntnissen aus ANDEREN KAPITELN wurde im Rahmen dieser Arbeit VRClassroom entwickelt. VRClassroom ist ein zweiteiliges System, das es Lehrkräften ermöglichen soll 360°-Fotos, -Videos und 3D-Modelle im Unterricht zeigen zu können, dabei die Anzeige auf allen Geräten steuern zu können und trotzdem den Überblick in der Klasse behalten zu können. 

VRClassroom bietet dafür die Lehrer-App, die auf einem Rechner läuft und die Schüler-App, die über den Browser der mobilen Geräte beziehungsweise VR-Headsets erreicht werden kann.

Die Lehrer-App ist eine Electron-App, die wiederum eine React-App für das Interface enthält und die Schüler-App startet. Die Schüler-App ist eine React360-Applikation. Der Code von VRClassroom ist also komplett in Javascript gehalten, nur unterschieden durch die verschiedenen Libraries Node.js, React.js, React360. Im Anschluss an diese Arbeit wird VRClassroom unter einer Creative Commons Lizenz veröffentlicht und so kann ein Entwickler schnell alle Teile des Codes verstehen ohne sich in mehrere Programmiersprachen einarbeiten zu müssen.

Grafik \ref{fig:electron}

\begin{figure}[ht]
  \centering
  \includegraphics[width=\textwidth]{images/electron.eps}
 \caption{Aufbau von Electron-Appliaktionen.}
  \label{fig:electron}
\end{figure}

\subsection{Nutzungsszenario und Anwendungsfokus}
VRClassroom wurde speziell für die Nutzung in der Schule entwickelt.
Dabei kann die Lehrkraft das VR-Erlebnis für die SchülerInnen führen und ihnen so komplexe 3-dimensionale Inhalte besser vermitteln und eine spannende neue Art des Lernens zu erleben. 
Außerdem wurde Funktionen wie etwa die Seitenleiste mit den verbundenen Geräten entwickelt, die es der Lehrkraft erleichtern sollen VRClassroom in einer Schulklasse einzusetzen und sicher zu stellen, dass alle Geräte verbunden sind und die Inhalte angezeigt werden.

Da viele Schulen noch überhaupt keine Ausrüstung an VR-Headsets haben, lag der Fokus besonders auf der Nutzung des VRClassroom Systems mit einem Smartphone in einem Google Cardboard. Allerdings wurde es bewusst so entwickelt, dass auch mit ``echten'' VR-Headsets das System problemlos weiter genutzt werden kann. So wird den Schulen ermöglicht mit einer sehr geringen Investition zu testen, ob es für sie in Frage kommt und können zu einem späteren Zeitpunkt zu einem elaborierteren Hardware-System wechseln ohne auf neue Software umsteigen zu müssen.

Weitere Einsatzszenarien für VRClassroom könnten Besprechungen im Arbeitsumfeld gehen, bei denen es sich um plastische Inhalte handelt wie zum Beispiel Architekturbüros, Designagenturen oder Landschaftsgärtner. Mit einem 360°-Foto oder -Video könnte der Ist-Zustand besprochen werden und anschließend die Entwürfe in 3D-Modellen vorgeführt werden. Dadurch könnten sich Kunden besser in die Entwürfe hineinversetzen und bewusster entscheiden, was sie letztendlich haben möchten.

\subsection{Anwender}
Wie bereits weiter oben beschrieben wurde VRClassroom speziell dafür entwickelt im Schulunterricht verwendet zu werden. Damit sind die Hauptanwendergruppe Lehrkräfte und Schülerinnen und Schüler.
Die Schülerinnen und Schüler als Anwender der Schüler-App können dabei im Alter von Grundschulkindern bis hin zu Erwachsenen reichen, die Spanne ist damit also sehr weit und somit ist auch die Vorerfahrung mit VR-Headsets und die individuelle Technikaffinität äußerst unterschiedlich.
Den Schülern soll es also möglichst leicht gemacht werden die Schüler-App von VRClassroom zu verwenden. Da fast keine Schulen über Mengen an VR-Headsets verfügen, die für eine gesamte Klasse ausreichen, muss also dafür Sorge getragen werden, dass die Schüler-App auf allen Geräten, aber besonders auf Smartphone-gestützten VR-Headsets läuft.

Bei den Lehrkräfte verhält es sich ganz ähnlich: Sie kommen aus verschiedensten Fachrichtungen, mit unterschiedlichsten Leveln an Erfahrung und Technikaffinität, außerdem hat die Umfrage gezeigt, dass mehr als die Hälfte teilnehmenden Lehrkräfte selbst noch nie ein VR-Headset ausprobiert haben. 

Dementsprechend lag der Fokus bei der Entwicklung besonders darauf, dass die Bedienung intuitiv und einfach ist. Außerdem soll den Lehrkräften mit der Anzeige der verbundenen Geräte mit dem Aktivitätsindikator eine Möglichkeit an die Hand gegeben werden, zu sehen, ob die Schüler-Geräte verbunden sind und alle Inhalte bekommen und anzeigen. Bewusst wurde das System so entworfen, dass die Lehrkräfte keine VR-Brille tragen, damit sie einen Überblick im Klassenzimmer behalten können und trotzdem sehen können, was die Schüler angezeigt bekommen.

\subsection{Grundstruktur}
Das VRClassroom System besteht aus zwei verschiedenen Applikationen: Zum einem der Schüler-App, die auf den VR-Systemen läuft mit denen die Schüler sehen können, was der Lehrer ihnen präsentiert, und die Lehrer-App, in der die Lehrkraft die verschiedenen Inhalte hineinladen, Markierungen auf die Inhalte setzen kann und und sehen kann, welche Schüler-Geräte aktuell verbunden sind.

Da viele Lehrer in ihrer Ausbildung oft nicht mit vielen neuen Technologien in Berührung gekommen sind, sondern es gewohnt sind mit den ``klassischen'' Medien zu arbeiten, lag der Augenmerk bei der Entwicklung darauf, dass das benötigte technische Verständnis möglichst niedrig ist und auch Personen, die sich selbst als nicht technikaffin bezeichnen würden, keinerlei Probleme bei der Nutzung haben. Gleiches gilt selbstverständlich auch auf der Seite der Schüler. Da diese aber hauptsächlich passiv agieren, standen hier die Lehrkräfte im Mittelpunkt der Aufmerksamkeit.

Grafik \ref{fig:architecture}

\begin{figure}[ht]
  \includegraphics[width=\textwidth]{images/architecture1.eps}
 \caption{Aufbau des VRClassroom Systems.}
  \label{fig:architecture}
\end{figure}

\subsection{Lehrer-Applikation}
Die Lehrer-Applikation besteht aus einer Electron-App, die in zwei logische Teile zerlegt ist. Das ist zum einen der main-Prozess, der es erlaubt Zugriffe auf das File-System des Rechners zu machen und für rechenaufwändige Hintergrundprozesse genutzt wird, und zum Anderen der render-Prozess. Der render-Prozess ist der Teil des Programms, das die Lehrkraft letztendlich auf ihrem Bildschirm sieht.

Die Lehrer-App enthält genau genommen zwei render-Prozesse: Die teacher-App, in der die Lehrkraft alle verbundenen Geräte sehen kann und verschiedene Inhalte hineinladen kann, und die student-App, die als iFrame in die teacher-App eingebunden ist und auf den Geräten der Schülern läuft.

Die teacher-App enthält außerdem noch den QR-Code Generator, der in einem zweiten Fenster geladen wird.

Um eine Liste der verbundenen Geräte zu halten und Veränderungen der Inhalte auf die Schüler-Geräte zu synchronisieren, startet die teacher-App einen Websocket-Server, mit dem sich alle Schüler-Geräte verbinden. 

\subsubsection{Verbundene Geräte}
Wie in der Grafik zu sehen hält die teacher-App eine Liste mit allen verbundenen Geräte dieser Session. Sind die Geräte gerade aktiv, werden sie mit einem grünen Icon dargestellt, sind sie inaktiv, mit einem Roten.

Das soll der Lehrkraft erleichtern zu überprüfen, ob die Schüler den gezeigten Stoff verfolgen oder sich anderweitig beschäftigen.

Haben die Schüler bereits einen Namen eingegeben, wird dieser in der Liste angezeigt. Ist dies nicht der Fall wird aus dem user-agent versucht möglichst genau zu schließen, um welches Gerät es sich handelt, sodass der Lehrer zumindest einschränken kann, um welchen Schüler bzw welche Schülerin es sich handeln könnte. 

NEU: Bei Modellen und Videos der loading-Status der einzelnen Geräte

\bigskip

Ein User-Agent kann zum Beispiel wie folgt aussehen: 
\begin{framed}
Mozilla/5.0 (iPhone; CPU iPhone OS 5\_0 like Mac OS X) AppleWebKit/534.46 (KHTML, like Gecko) Version/5.1 Mobile/9A334 Safari/7534.48.3
\end{framed}

Daraus lässt sich schließen, dass es sich um eine iPhone handelt, welches Safari benutzt, um die Schüler-App zu laden. Die Oculus Geräte hingegen geben in ihrem User-Agent an, den Oculus Browser zu verwenden und sind so auch gut von den anderen verbundenen Geräten zu unterscheiden. Da aber hier nicht ersichtlich wird welches Oculus-Gerät es genau ist wird nur ``Oculus device'' angegeben und nicht genauer spezifiziert, ob es sich dabei um eine Go, Quest oder Rift handelt.
 %hier Grafik connected clients einfügen% 

\subsubsection{Hineinladen von Inhalten}
Um Inhalte in VRClassroom hineinzuladen kann mit ``STRG'' + ``o'' beziehungsweise Cmd+o oder in der Menüleiste über ``Datei'' und ``Öffnen...'' der FileBrowser geöffnet werden, in dem dann die gewünschte Datei ausgewählt wird. Ist eine Datei ausgewählt wird sie sofort in die App geladen und auf allen verbundenen Geräten angezeigt.
Wurde die VRClassroom App zum ersten Mal installiert wird ein Ordner ``.vrclassroom'' im Dateisystem angelegt, in den die verwendeten Dateien hineingeladen werden. Von dort aus werden sie dann auch für den Zugriff durch die verbundenen Schüler-Geräte freigegeben.

Bereits früher verwendete Dateien sind unter ``Datei'' und ``Zuletzt geöffnet'' zu finden und können so bequem wieder verwendet werden ohne sie lange im Dateisystem suchen zu müssen. Neu geöffnete Dateien werden beim Öffnen in den ``.vrclassroom''-Ordner im Dateisystem geladen und sind ab diesem Moment auch in der Liste der bereits geöffneten Dateien zu finden.

Drittens können auch noch die URLs von Google Streeview-Panoramen eingegeben werden, sodass dann allen verbundenen Geräten das verlinke Panorama angezeigt wird. Dazu muss ``Datei'' und ``StreetView...'' ausgewählt werden und in das Pop-Up die URL zum gewünschten Panorama eingegeben werden. Handelt es sich um eine korrekte URL wird dann das StreetView Bild geladen und angezeigt wie ein normales 360°-Foto. Für die Nutzer der Schüler-App ist kein Unterschied zu normalen 360°-Fotos erkennbar.
 
\subsubsection{Controls}
Wie in Grafik \ref{fig:controls} zu sehen sind die Controls, mit denen Aktionen des aktuell geladenen Inhalts ausgelöst werden können durch einen Overlay über dem iFrame der React360-App dargestellt. 

Für jeden der drei Typen an Inhalten, 360°-Fotos, -Videos und 3D-Modelle unterscheiden sich die Bedienelemente und Funktionen, das Setzen einer Markierung ist allerdings bei allen Medientypen gegeben.

\begin{figure}
  \includegraphics[width=0.9\linewidth]{images/controls-overlay.png}
  \caption{Die Kontrollleiste als Overlay über den iFrame der React360-App.}
  \label{fig:controls}
\end{figure}

\paragraph{Marker}
\label{subsec:marker}

\begin{SCfigure}
  \centering
  \caption{Die leuchtende Farbe und das Drehen der Marker erhöht die Erkennbarkeit.}
  		\includegraphics[width=0.4\textwidth]{images/marker.png}
	\label{fig:marker}
\end{SCfigure}

Soll ein Marker gesetzt werden, muss zuerst in den ``Markierung setzen''-Modus gewechselt werden, indem in der Control Bar auf ``Marker setzen'' geklickt wird. Danach kann an beliebiger Stelle eine Markierung gesetzt werden.

Um die 3D-Koordinaten des Markers zu bekommen wird mit Hilfe von Mouseposition und Fenstergröße ein Strahl berechnet, der aus der 2D-Koordinate der Mouseposition ausgeht. 
Bei 360°-Fotos und -Videos wird die Entfernung zur Kamera auf einen festen Wert gesetzt, der so gewählt wurde, dass er gerade noch im Zylinder der Welt liegt. Mit der Entfernung zur Kamera lässt sich die 3D-Koordinate dann leicht berechnen.

Um Markierungen auf 3D-Modellen setzen zu können muss ein Schnittpunkt des 3D-Modells mit dem virtuellen Strahl berechnet werden. 
(Soll das wirklich rein?) In React360 gibt es momentan keine Möglichkeit auf diese Weise auf die in der 3D-Welt platzierten 3D-Modelle zuzugreifen, sodass

Ist der Schnittpunkt des Strahls mit dem 3D-Modell berechnet, wird derjenige Schnittpunkt als Koordinate verwendet, der am nächsten zur Kamera ist. An dieser Stelle wird dann der Marker platziert. Zudem werden die Marker invers zur Nähe zur Kameraposition skaliert, sodass sie immer in der gleichen Größe dargestellt werden, egal wo am Modell sie gesetzt sind.

Das verwendete Modell erinnert wie in Grafik \ref{fig:marker} zu sehen an Stecknadeln, sodass Nutzern die Bedeutung direkt verständlich ist. Um die Markierungen leichter erkennbar zu machen sind sie in einer leuchtenden Farbe und drehen sich, sodass die Aufmerksamkeit direkt auf die markierte Stelle gelenkt wird.



\paragraph{Photo Controls}
Ist ein Foto geladen beinhaltet die Kontrollzeile nur den Dateinamen des 360°-Fotos und die Buttons zum Setzen von Markierungen, die wie bereits beschrieben bei allen Medientypen gegeben sind. Grafik \ref{fig:photocontrols} zeigt die Kontrollleiste für 360°-Fotos.

\begin{figure}[h]
  \includegraphics[width=\linewidth]{images/photocontrols.png}
  \caption{Die Kontrolleiste für 360°-Photos.}
  \label{fig:photocontrols}
\end{figure}


\paragraph{Video Controls}
Wird ein Video gezeigt, hat die Lehrkraft mehrere unterschiedliche Funktionen in der Kontrollleiste: 
Wie in Grafik \ref{fig:videocontrols} zu sehen sind die Elemente ähnlich wie bei bekannten Videoplayern wie Quicktime oder Netflix gehalten. Ganz links ein kombinierter Play/Pause-Button, der für alle verbundenen Geräte synchronisiert das Abspielen beziehungsweise Pausieren des Videos auslöst, daneben die aktuelle Abspielzeit, gefolgt von einem Slider, der grafisch die aktuelle Position im Video darstellt. Außerdem kann mit dem Slider zu anderen Zeitpunkten im Video gesprungen werden.
Rechts daneben wird die Gesamtdauer des Videos angezeigt. Der letzte Video-spezifische Button ist der Sound-Button, der im aktiven Zustand auf alle Geräten den Ton des Videos abspielt. Der Sound-Button ist standardmäßig deaktiviert. 

Zudem sind wiederum die Buttons für das Setzen von Markierungen vorhanden. Sie sind während dem Abspielen des Videos deaktiviert und können nur genutzt werden, wenn das Video pausiert ist. Wird das Video dann wieder weiter abgespielt, werden alle gesetzten Markierungen automatisch zurückgesetzt.

\begin{figure}[h]
  \includegraphics[width=\linewidth]{images/videocontrols.png}
  \caption{Die Kontrolleiste für 360°-Videos.}
  \label{fig:videocontrols}
\end{figure}

\paragraph{Model Controls}
Wie in Grafik \ref{fig:modelcontrols} zu sehen ist auch in der Kontrollleiste bei 3D-Modellen ein Slider vorhanden. Dieser kann sowohl zum Drehen des Modells als auch zum Skalieren benutzt werden. Dazu sind links vom Slider die Buttons ``Drehen'' und ``Skalieren'' mit denen zwischen den zwei Funktionalitäten des Sliders gewechselt werden kann. Der aktive Modus wird durch die blaue Farbe dargestellt.

Auch auf 3D-Modellen können Markierungen gesetzt werden. Dabei ist zu beachten, dass nur auf dem Modell eine Markierung gesetzt werden kann. Wird außerhalb des 3D-Modells geklickt, wird keine Markierung gesetzt.

\begin{figure}[h]
  \includegraphics[width=\linewidth]{images/modelcontrols.png}
  \caption{Die Kontrolleiste für 3D-Modelle.}
  \label{fig:modelcontrols}
\end{figure}
 
\subsection{Kommunikation zwischen Lehrer-App und Schüler-App}
Wie in Grafik XXX zu erkenne kommunizieren die Lehrer-App und die Schüler-App über eine Websocket-Verbindung. Beim Start der VRClassroom-App auf dem Computer der Lehrkraft werden sowohl die Lehrer-App, die Schüler-App und ein Asset-Server gestartet. Über den Asset-Server sind die Inhalte erreichbar, die die Lehrer-App an die Schüler-App schickt.

Bei jeder inhaltlichen Veränderung in der Lehrer-App wird eine Nachricht an alle verbundenen Geräte geschickt, sodass diese dann die Anzeige aktualisieren können.
Damit alle Geräte zu jedem Zeitpunkt über die gleichen Informationen verfügen und auch Geräte, die nach Beginn der Session hinzustoßen alle Information zur Anzeige haben, ist jede Nachricht in der Form wie in YYY zu sehen gestaltet.

Eine Nachricht enthält immer die Felder ``mediatype'', ``url'', ``markers'',``playing'',``playbackPosition'',``rotation'',``scaleFactor'' und``muted''.
Das Feld ``mediatype'' kann die Werte ``photo'',``video'' und``model'' enthalten und ist dafür zuständig, dass die Schüler-App weiß, welcher Medientyp angezeigt wird und damit auch, welche Controls angezeigt werde sollen. 
Im URL-Feld enthält die URL des aktuell anzuzeigenden Inhalts. Dieses Feld wird nur verändert, wenn ein neuer Medientyp hineingeladen wird. Geschieht dies wird auch das Array der ??marker''-Positionen zurückgesetzt. Das Array enthält die Positionen der Markierungen in Form von Tripeln innerhalb des Arrays.

Die Felder ``playing'',``playbackPosition'' und ``muted'' werden nur beim Anzeigen von 360°-Videos verwendet. ``playing'' gibt an, ob das betreffende Video in diesem Moment abgespielt oder pausiert werden soll, ``playbackPosition'' gibt die Stelle im Video an, an der es abgespielt werden soll. Werden Videos abgespielt schickt die Lehrer-App jede Sekunde eine Nachricht mit der aktuellen Abspielposition an alle Geräte und diejenigen, bei denen die Abspielposition um mehr als eine Zeit von PPPPP versetzt ist, springen dann an die mitgeschickte Position, um von dort weiter abzuspielen.
Das ``muted''-Feld gibt an, ob der Ton des Videos an den Geräten abgespielt werden soll oder nicht.

Nachrichten von Seite der Schüler-Geräte sind viel seltener als anders herum. Gibt der Schüler seinen Namen ein wird dieser als Nachricht an die Lehrer-App geschickt. Außerdem senden die Geräte eine Nachricht, wenn sich der visibility-Status ändern, also zum Beispiel der Browser geschlossen wird. Außerdem geben die Schüler-Geräte Rückmeldung an die Lehrer-App, solange sie Inhalte laden und wiederum, wenn sie die Inhalte fertig geladen beziehungsweise Videos genug vorgeladen haben. Das wird dann mit dem orangenen Status-Indikator neben dem Geräte-Namen in der Lehrer-App angezeigt. Wechselt hier die Farbe wieder auf grün, ist das Gerät bereit die Inhalte anzuzeigen.

\subsection{QR-Code Fenster}
Das QR-Code Fenster ist ein zweites Browserfenster, das aus dem main-Prozess der Electron App auf dem Lehrer-Computer gestartet wird. Es zeigt, wie in \ref{fig:qrcode} zu sehen, einen QR-Code, den die SchülerInnen mit ihren Smartphones scannen können, um bequem die URL zu laden, auf der sie die Schüler-Applikation erreichen können. Der QR-Code wird dynamisch beim Öffnen der App generiert.

Für Geräte, die keine Kamera haben oder wenn das Scannen des QR-Codes fehlschlägt, wird zudem unterhalb des QR-Codes die URL angezeigt, unter der die Schüler-Applikation zu erreichen ist. 

Der QR-Code wird in diesem extra Fenster generiert und angezeigt, damit die Lehrkraft dieses Fenster auf einem Beamer anzeigen kann, um den SchülerInnen den Zugang zur Schüler-App ohne umständliches URL-Abtippen zu ermöglichen.

\begin{figure}
  \includegraphics[width=0.8\linewidth]{images/QR-Code.png}
  \caption{Qr-Code Fenster mit Link zur Schüler-App.}
  \label{fig:qrcode}
\end{figure}

\subsection{Schüler-App}
In der VRClassroom App nehmen die Schüler eine passive Rolle ein und können selbst nicht in der 3D-Welt navigieren. 

Die WebApp für die Schüler ist eine React360-App, sie stellt eine 3D-Welt da, in deren Mitte sich die Kamera, also der Viewport der Geräte, befindet. Sie stellt die in der Lehrer-App hineingeladenen 360°-Fotos und Videos in einer Kugel um den Viewport da, sodass die Nutzer sich in alle Richtungen umsehen können. 3D-Modelle werden in kurzer Entfernung vor dem Viewport angezeigt, als befänden sich sich im Raum vor der Person.

Grafik \ref{fig:react360}

\begin{figure}[ht]
  \includegraphics[width=\textwidth]{images/react360.eps}
 \caption{Struktur der React360 App von VRClassroom.}
  \label{fig:react360}
\end{figure}

%% Screenshot VRClassroom App?

\subsubsection{Eingabe des Namens}
Läd ein Gerät zum ersten Mal die VRClassroom-App wird dem Nutzer neben der Begrüßung, die immer angezeigt wird bevor Inhalte hineingeladen werden, eine Tastatur im VR-Raum angezeigt. Damit können die Schüler ihren Namen eingeben, der dann in der Liste der verbundenen Geräte angezeigt wird. Wird ein Smartphone als VR-Headset genutzt, muss die Tastatur mittels der Toucheingabe auf dem Display gemacht werden, bevor die WebVR Ansicht geladen wird. VR-Headsets, die einen Controller haben, können den Namen mit dem Controller auf der Tastatur tippen. 

Damit die Schüler nicht jedes Mal wieder ihren Namen eingeben müssen, wird der eingegebene Name im Browser des Schüler-Gerätes gespeichert und beim erneuten Laden der VRClassroom Applikation direkt wieder an die Lehrer-App übermittelt.

%% Screenshot VRClassroom App mit Tastatur + Greeting

\subsubsection{Anzeigen von 360°-Fotos}
Um 360°-Fotos in der 3D-Szene anzuzeigen lädt die React360-App das Foto von dem Link, den die Teacher-App geschickt hat und setzt sobald das gesamte Foto geladen wurde das Bild als Hintergrund der Szene. Gleichzeitig wird bei erfolgreichem Ladens des Fotos eine Nachricht an die Teacher-App geschickt, um dem Status des Geräts von ``loading'' wieder auf ``active'' zu setzen.
Das 360°-Bild wird dazu von innen an die Kugel in der die React360-Szene ist projiziert und ergibt so durch seine equirectangulare Projektion ein unverzerrtes 360°-Bild.

Das Bild wird angezeigt bis VRClassroom von der Lehrkraft beendet wird oder etwas anderes in die App hineingeladen wird.

\subsubsection{Abspielen von 360°-Videos}
Sendet die Teacher-App die Nachricht, dass ein Video angezeigt werden soll, startet die React360-App sofort mit dem Laden des Videos und lädt soviel von dem Video wie es geht. 
Ist eine Zeit des Videos vorgeladen, sodass React360 davon ausgeht, dass das Video ruckelfrei abgespielt werden kann, sendet wird wiederum die Nachricht, dass das Gerät aus dem ``loading''-Status wieder auf ``active'' gesetzt wird.
Ähnlich wie bei 360°-Fotos wird das 360°-Video dann als Hintergrund-Video der Szene gesetzt, um als 360°-Video abgespielt werden zu können

Die React360-App spielt das Video nicht automatisch ab, sobald genug geladen ist, sondern wartet auf das Signal der Teacher-App, um das Abspielen zu starten. Alle Funktionen werden erst ausgeführt wenn das Signal der Teacher-App kommt, die Funktion auszuführen. Zu den Funktionen zählen: Play, Pause, Springen zu einer anderen Stelle im Video und den Ton des Videos abspielen.

Falls ein Schüler-Gerät sich verspätet verbindet oder ein Paket verloren gegangen ist, sendet die Teacher-App jede Sekunde eine neue Nachricht in der immer der Link zum aktuellen Video und die aktuelle Abspielposition enthalten ist. Unterscheidet sie sich mehr als eine Sekunde von der Abspielposition in der React360-App wird zu der Abspielposition, die in der Nachricht steht gesprungen.

\subsubsection{Abspielen von Ton in Videos}
Da die meisten Browser das automatische Abspielen von Videos mit Ton verbieten, werden 360°-Videos von React360 standardmäßig beim erstellen des Videoplayer-Komponenten ``muted'' auf ``true'' gesetzt. Die Hersteller wollen dadurch die Ablenkungen, die beim surfen auf den Nutzer zukommen, abmildern. Erst wenn der Nutzer ein ``user gesture click'' also einen Klick auf der Website gemacht hat, darf die Tonspur automatisiert abgespielt werden. Wird versucht ein Video mit Ton abzuspielen, ohne dass ein Klick gemacht wurde, wird der Videoplayer blockiert und kann nicht mehr abspielen. \cite{Decker2017}

Damit React360 in einer single-threaded Umgebung wie einem Webbrowser flüssig ablaufen kann und nicht durch ``blocking behavior'' irgendeiner Art das Rendern unterbrochen wird, ist eine React360-App in zwei Teile aufgeteilt: Die React-Applikation und den Code, der die React Komponenten in 3D Elemente auf dem Bildschirm umwandelt. Die App selbst läuft in einem Webworker, einem anderen Prozess als der des Hauptbrowserfensters. \cite{FacebookInc.2018}

Das führt dazu, das die React360-Elemente nicht als html-Elemente gelten und ein Click-Event auf einem VRButton nicht als Interaktion zählt, um die Erlaubnis zu haben Ton abzuspielen.

Um dieses Hindernis zu umgehen ist nun ein durchsichtiger, Bildschirm-füllender Button über die React360-App gelegt, der bei einem Klick verschwindet, um die Erlaubnis vom Browser zu bekommen Ton abzuspielen. Wurde der Button am Schüler-Gerät geklickt wird ein flag gesetzt, dass die App Ton abspielen darf. Ob dann tatsächlich Ton beim Video abgespielt wird, kann der Lehrer aus der teacher-App einstellen. Auch dort ist das Abspielen von Ton an den Schüler-Geräten standardmäßig erst einmal abgestellt, damit der Lehrer entscheiden kann, ob er nur den Ton aus dem Rechner über Boxen für Alle abspielen möchte oder aus jedem Schüler-Gerät einzeln der Ton kommen soll.

\subsubsection{Anzeigen von 3D-Modellen}
Wird die Nachricht empfangen, dass der ``mediatype'' auf ``model'' gesetzt wurde, wird als Hintergrund der Standardhintergrund gesetzt, wie er auch angezeigt wird, bevor Medien von der Lehrkraft in die App geladen wurden. Das Laden und Darstellen der 3D-Modelle passiert in der ModelView. Hier wird zuerst analysiert, ob es sich um eine gltf-Datei oder eine .obj-Datei handelt. Valide gltf-Dateien haben die Endung .gltf oder .glb, ein Container-Dateiformat, das alle Texturen und Styling-Dateien enthält. Handelt es sich um eine .obj-Datei wird versucht eine gleichnamige .mtl-Datei zu laden. Diese enhält alle Informationen über Texturen und Materialien, die .obj-Datei ist allein das Modell.

Im Anschluss wird die Datei geladen und angezeigt. Damit die Modelle zu erkennen sind und nicht schwarz gerendert werden wird zudem eine Punktlichtquelle installiert, die das Modell von oben rechts beleuchtet. Außerdem wird das Modell mit einem leichten Ambientlight versehen, damit auch Teile des Modells, die sonst im Schatten liegen, erkennbar werden.

Auf die ähnliche Weise werden die Marker gerendert, die von der Lehrkraft gesetzt werden können, wie bereits in ref{subsec:marker} beschrieben. Die Marker haben keine Punktlichtquelle, sondern nur ein Ambientlight. Sie werden zudem nicht an eine feste Position, sondern an die von der Lehrkraft ausgewählt Stelle in der VR-Szene gerendert und ihre Größe wird zudem in Abhängigkeit ihrer Entfernung zur Kamera skaliert.

\subsubsection{WebVR Polyfill}
WebVR ist eine Javascript-API, um Virtual Reality-Inhalte im Browser anzuzeigen. \cite{WebVR} 

Wie in Grafik \ref{fig:WebVRfig}a links zu sehen wird auf einer Web-App, die mit WebVR angesehen werden kann ein Icon in der unteren rechten Ecke des Fensters zu angezeigt, mit dem dann die WebVR-Ansicht geladen werden kann.

Wie Grafik \ref{fig:WebVRfig}b zeigt teilt WebVR dafür den Bildschirm in zwei Bilder für die Linsen in einem VR-Headset wie zum Beispiel dem Google Cardboard auf. Die zwei Bilder sind dabei nicht Bildschirm-füllend, sondern in einer annähernd ovalen Form, die von einem schwarzen Rand umgeben wird, sodass sie gut auf die Linsen passen. Durch die Krümmung der Linsen wird daraus dann eine drei dimensionale 360°-Welt, in der der Nutzer sich umsehen kann.

\begin{figure}[h!]
  \centering
  \begin{subfigure}[b]{0.4\linewidth}
    \includegraphics[width=\linewidth]{images/WebVR1.png}
    \subcaption{``View in VR'' Button in VRClassroom.}
  \end{subfigure}
  \begin{subfigure}[b]{0.4\linewidth}
    \includegraphics[width=\linewidth]{images/WebVR2.png}
    \subcaption{VR-Szene mit WebVR Ansicht.}
  \end{subfigure}
  \caption{VR-Szene mit und ohne WebVR Ansicht.}
  \label{fig:WebVRfig}
\end{figure}

Bisher ist weder in Chrome noch in Safari WebVR standardmäßig unterstützt. In Chrome auf Android kann es durch setzen der ``WebVR''- und ``Gamepad-Extension''-flags auf ``enabled'' eingeschaltet werden.  \cite{Jones2017}
In Safari ist es bisher gar nicht möglich WebVR zu nutzen.

Da aber VRClassroom auf allen mobilen Geräten funktionieren soll und möglichst auch keine Einstellungen auf der Nutzerseite benötigen soll, wurde ein Polyfill benutzt, um die WebVR Funktionen für die VRClassroom-App auf allen Geräten nutzen zu können. 
Dafür 

%______________________________________________________________________
\cleardoublepage

\section{Nutzerstudie und Evaluation}

\subsection{Online-Umfrage}
Um die Meinung möglichst vieler verschiedener Lehrkräfte zum entwickelten VRClassroom System zu erfahren, wurde eine Online-Umfrage durchgeführt, bei der die Teilnehmer ihre Meinung zu VRClassroom anzugeben, nachdem sie ein erklärendes Video gesehen haben.
Dafür wurde ein Video produziert, welches die Idee und Nutzung von VRClassroom erklärt und die einzelnen Funktionen zeigt, die die App anbietet. Im Anschluss haben die teilnehmenden Lehrkräfte den kurzen Fragebogen ausgefüllt.

Alle Fragen waren optional gestellt, konnten also wenn gewünscht unbeantwortet bleiben, sodass davon ausgegangen werde kann, dass die gegebenen Antworten alle der Wahrheit entsprechen und nicht einfach eine der möglichen Antworten gewählt wurde, weil eine nötig war. 
Die Fragen wurden als Aussagen formuliert, zu denen mit Hilfe einer unzentrierten Lickert-Skala geantwortet werden konnte. Es wurde eine unzentrierte Skala verwendet um zu aussagekräftigeren Antworten zu ermuntern.

Insgesamt haben 59 Personen an der Umfrage teilgenommen, allerdings wurden nicht alle Fragen von allen Teilnehmer beantwortet. Die Prozentangaben aus den Grafiken beziehen sich dabei immer auf die  Antworten für die jeweilige Frage.

Die Umfrage war an Lehrkräfte aller Schulen und Altersgruppen gerichtet, um eine möglichst diverse Meinungsbasis zu erhalten.

\subsubsection{Gesamteindruck VRClassroom}
Allgemein waren Antworten der Studienteilnehmer positiv. 83\% der Teilnehmer gaben an, dass sie VRClassroom als intuitiv und leicht bedienbar bewerten würden. 
Besonders überzeugt waren sie von der Wirkung auf die Schüler. Grafik \ref{fig:survey1} zeigt, dass die überragende Mehrheit von 92\% der Meinung ist, dass die Schüler an Unterricht mit VRClassroom Spaß hätte und auch für sich selbst erwarten 78\%, dass sie Spaß hätten. Etwas weniger Teilnehmer mit 66\% sind zudem der Meinung, dass VRClassroom dazu beitragen würde, dass ihre Schüler drei dimensionale Inhalte besser verstehen würden als ohne.

\begin{figure}[ht]
  \includegraphics[width=\textwidth]{images/survey1.eps}
 \caption{Ich brauche noch einen sinnvollen Text.}
  \label{fig:survey1}
\end{figure}

Der Hauptkritikpunkt an VRClassroom war die technische Ausstattung, die zur Nutzung benötigt wird. Dabei ging es zum Einen darum, dass nicht alle Schüler, besonders in den unteren Klassenstufen und der Grundschule, ein Smartphones besitzen. Andererseits befürchten Viele, dass die Schule nicht die nötigen Vorraussetzungen bieten, um VRClassroom einsetzen zu können.
Dabei sehen sie vor Allem das fehlende oder schlecht verfügbare WLAN in ihrer Schule, aber auch, dass die Schule keine VR-Headsets besitzt und dafür auch kein Budget zur Verfügung steht.

Passend für die Problematik der nicht vorhandenen Infrastruktur in den Schulen und das fehlende Budget für die Aufrüstung wurde VRClassroom entwickelt. Wird davon ausgegangen, dass ein Rechner zur Verfügung steht sind die Anschaffungskosten für einen Satz für eine gesamte Klasse mit circa 30 Euro zu veranschlagen. Nachbauten für Google Cardboards sind ab etwas über 1 Euro zu bekommen. 
Falls kein Netzwerk in der Klasse zur Verfügung stehen kann auch mit einem Router ein lokales Netzwerk für VRClassroom eröffnet werden. Dafür müsste dann noch zusätzlich ein Router bereitgestellt werden, wofür noch einmal etwa 30 Euro eingerechnet werden müssen. Das ganze System könnte also mit einem finanziellen Aufwand von nur 60 Euro realisiert werden.

Ein weiterer Kritikpunkt an VRClassroom ist, dass es keine fest integrierten Inhalte hat. Einige Lehrkräfte wünschen sich ein System, das bereits passende Inhalte mitbringt, die dann speziell dafür angepasst sind und auch den Ansprüchen für den Unterricht entsprechen. 
Da VRClassroom als offenes System entwickelt wurde, können beliebige Inhalte verwendet werden. Sowohl selbsterstellte oder Inhalte von anderen Produzenten. Sollte VRClassroom in der Zukunft weiterentwickelt werden, könnte ein Geschäftsmodell darin bestehen, passende Inhalte für den Schulunterricht zu erstellen.

Zudem wurde kritisiert, dass Google Cardboards nicht mit einer Brille genutzt werden können. Das lässt sich allerdings nur dadurch ändern, dass man ein anderes Headset verwendet. Da VRClassroom WebVR nutzt und damit auf allen VR-Headsets läuft, ist dies allerdings nicht wirklich als Problem von VRClassroom zu werten.

\bigskip

Als besonders vorteilhaft schätzen die Teilnehmer an VRClassroom ein, dass es die Schülerinnen und Schüler motiviert. Außerdem finden sie es sehr gut, dass die Aufmerksamkeit auf die gezeigten Inhalte gelenkt wird und die Schüler kaum Ablenkungen preisgegeben sind. 
Einige äußerten zudem, dass sie es sehr schätzen das dass System so gestaltet ist, dass nur die geringen Anschaffungskosten von Google Cardboards anfallen würden und das ganze System keinen hohen finanziellen Anspruch hat.

Zudem lobten sie die einfach wirkende Handhabung und die Kompatibilität verschiedenster VR-Plattformen.
Besonders gefiel zudem die Möglichkeit Orte virtuell zu Erkunden und die korrekte Darstellung räumlicher Inhalte sowie die Darstellung von Inhalten, die sonst schwer darzustellen sind.

\bigskip

Auf die Frage nach Funktionen, die sich die Teilnehmer für VRClassroom wünschen würden, wurde besonders oft angemerkt, dass die Lehrkräfte die Marker mit einem Text oder sogar Links versehen können. 
Einen einfachen Text an die Marker zu binden, könnte für manche Unterrichtsinhalte sicherlich eine Bereicherung sein, die in der Zukunft bei einer Weiterentwicklung von VRClassroom realisiert werden könnte.
Das öffnen von Links dagegen würde die VR-Welt verlassen und dazu führen, dass die URL in einem neuen Browserfenster geöffnet werden würde. Um dann weiter zu agieren, müsste das Smartphone aus dem VR-Headset entfernt werden und bräche damit den kompletten VR-Aufbau.

Zudem wünschten sich die Teilnehmer eine Funktion, um überwachen zu können, dass die Schüler die gezeigten Inhalte auch ansehen. Diese ist bereits inplementiert und zeigt zudem noch an, ob die Inhalte geladen sind und die Geräte abspielbereit. Auch die Offline-Funktion ist in der aktuellen Version von VRClassroom bis auf das Laden von StreetView-Panoramen möglich. Die Struktur von VRClassroom wurde speziell so gewählt, dass eine Nutzung nur in einem lokalen Netzwerk funktioniert.

Ein weiterer Funktionswunsch war die schnelle Skalierung der 3D-Modelle auf Originalgröße, um den Schülern einen guten Eindruck zu vermitteln, wie das Objekt in echt aussieht. Darauf wird im folgenden in  \ref{subsec:modelscaling} eingegangen.


% überhaupt reinnehmen?
% gewünschte Inhalte:
%- Mathematik: Vektoren, Ebenen und Schnitte von Körpern
%- Kernkraftwerk und andere Einrichtungen, die man nicht besuchen kann im Rahmen einer Exkursion
%- Menge an Modellen, auch mit didaktischer Erläuterung
%- Körper, Organe, biologische Prozesse
%- Moleküle, chemische Reaktionen auf Teilchenebene
%- NuT
%- Kunst: Architketur

das müsste sich ändern:
- technische Ausstattung und betreuung an den Schulen
- Inhalte müssen existieren
- Schulungen für Lehrer
- Infrastrktur





\subsubsection{Medien in VRClassroom}
Grafik \ref{fig:survey2} zeigt die Meinung der Befragten zu den verschiedenen Medientypen. Im Vorfeld wurde erwartet, dass besonders 360°-Videos und 3D-Modelle gut bei den Lehrkräften ankommen. Entgegen der Erwartungen schnitten die 360°-Videos deutlich schlechter ab als die anderen beiden Medientypen. Ein Drittel der Befragten schätzten dabei die Nutzuung von 360°-Videos in VRClassroom als nicht hilfreich zur Vermittlung von Lehrstoff ein. Mit 87\% Zustimmung schnitten die 3D-Modelle nach Einschätzung der befragten Lehrkräfte am Besten ab, um Inhalte zu vermitteln.

\begin{figure}[ht]
  \includegraphics[width=\textwidth]{images/survey2.eps}
 \caption{Einschätzung zu den verschiedenen Medientypen in VRClassroom.}
  \label{fig:survey2}
\end{figure}

Wie Grafik \ref{fig:survey3} zu sehen sind die Einschätzungen zu Markern auf den unterschiedlichen Medientypen fast exakt genauso wie die Meinungen zu den Medientypen selbst: 82\% stuften Marker auf 360°-Fotos als hilfreich ein, auf 360°-Videos waren es nur 61\% und bei den 3D-Modellen waren es 83\%.

\begin{figure}[ht]
  \includegraphics[width=\textwidth]{images/survey3.eps}
 \caption{Einschätzung zu Markern auf den verschiedenen Medientypen in VRClassroom..}
  \label{fig:survey3}
\end{figure}


\subsubsection{Alters- und Fächergruppen für VRClassroom}
Anschließend wurden die Lehrkräfte dazu befragt für welche Fächergruppen VRClassroom einen Mehrwert bringen könnte.
Die Fächer wurden zu Gruppen zusammengefasst, um die Studie möglichst unabhängig von Schultypen zu machen.

\begin{figure}[ht]
  \includegraphics[width=\textwidth]{images/survey4.eps}
 \caption{Mehrwert von VRClassroom für verschiedene Fächergruppen.}
  \label{fig:survey4}
\end{figure}

Im Vorfeld wurde erwartet, dass besonders die naturwissenschaftlichen Fächer und Sprachen Zuspruch erhalten würden. 
Dazu passen wurde VRClassroom für die Fächergruppe der naturwissenschaftlichen Fächer mit 89\% Zustimmung als sinnvoll erachtet. Ebenfalls hohe Zustimmungswerte gab es für die gesellschaftswissenschaftlichen Fächer und Kunst/Musik, was im ersten Moment überraschend war.

Allerdings erklärten viele Teilnehmer in der Folgefrage nach Lehrstoff, den sie sich gut vorstellen können, dass sie besonders Geschichtliche Themen sowie Plastiken und Architektonische Modelle neben den offensichtlicheren naturwissenschaftlichen Inhalten sähen. Zu diesen zählen: chemikalische Experimente, physikalische Kräfte, Moleküle und geometrische Körper.

Interessant war zudem der Vorschlag Bewegungsabläufe aus dem Sportunterricht im Vorfeld in VR zu besprechen, damit die Schüler besser verstehen, wie der optimale Ablauf ist.

Entgegen der Erwartung glauben viele Teilnehmer, dass im Sprachunterricht VRClassroom keinen Mehrwert bringt. Nur 39\% sehen einen Vorteil. Es wurde erwartet, dass Lehrkräfte der sprachlichen Fächer es spannend finden mit ihren Schülern Orte aus dem Unterricht zu erkunden. Diese Einschätzung wurde in der Studie aber widerlegt wie Grafik \ref{fig:survey4} deutlich macht.

\bigskip

In Frage 18 wurde abgefragt, welche Klassenstufen für die Nutzung von VRClassroom für geeignet halten. Wie erwartet halten 59\% und damit die meisten Studienteilnehmer Grundschüler für noch nicht geeignet. Erst an Schulen der Sekundarstufe wird die Software als geeignet eingestuft. Dabei steigen die Zustimmungszahlen mit dem Alter der Schüler von 73\% bei der Unterstufe, über 82\% bei der Mittelstufe auf 89\% in der Oberstufe. Lediglich für Erwachsene sinkt sie wieder leicht auf 88\%.

Grafik \ref{fig:survey5} zeigt die genauen Prozentwerte zu diesem gut erkennbaren Trend.

\begin{figure}[ht]
  \includegraphics[width=\textwidth]{images/survey5.eps}
 \caption{Eignung der Klassenstufen für VRClassroom.}
  \label{fig:survey5}
\end{figure}


\subsubsection{Net Promotor Score}

\begin{figure}[ht]
  \includegraphics[width=\textwidth]{images/survey6.eps}
 \caption{Net Promotor Score von VRClassrooms.}
  \label{fig:survey6}
\end{figure}

Der Net Promotor Score, kurz NPS, ist bei Tech-Konzernen weit verbreitet und versucht darzustellen wie zufrieden und damit loyal die Nutzer eines Produkts sind.

Dabei sollen die Befragten auf einer Skala von 0 bis 10 angeben, wie wahrscheinlich sie das Produkt an einen Kollegen weiterempfehlen würden.
0 steht dabei für sehr unwahrscheinlich und 10 steht für extrem wahrscheinlich.
Aus den Zahlenwerten kann dann ein Wert berechnen, der die Loyalität der Kunden wiederspiegeln soll.

Außerdem werden die Werte in drei Gruppen eingeteilt. Personen, die mit einem Wert von 0 bis 5 angeben gelten als Detraktoren, also Personen, die ihren Kollegen davon abraten würden das Produkt zu verwenden. Bei Werten von 6-8 gelten die Personen als Indifferente, die das Produkt okay finden, allerdings auch nicht positiv Anderen gegenüber davon berichten würden. Die dritte Gruppe sind die Promotoren, die auf die Frage mit 9 oder 10 geantwortet haben. Promotoren werden ihren Kollegen positiv von ihren Erfahrungen mit dem Produkt berichten und dadurch dafür sorgen, dass das Produkt neue Kunden gewinnt und bekannter wird.

(Zahlen nochmal nachschauen)

% Quelle!

In der Studie zeigte sich, dass 56\% der Teilnehmer Detraktoren wären, also die Software ihren Kollegen nicht empfehlen würden, wäre VRClassroom ein marktreifes Produkt. 28\% sind indifferent und nur 16\% sind von VRClassroom überzeugt und würden auch ihre Kollegen ermuntern, die Software auszuprobieren. Grafik \ref{fig:survey6} veranschaulicht den NPS der Online-Evaluation von VRClassroom.

\subsubsection{Einschätzung der Zukunft von VR und VRClassroom}
Trotz der insgesamt sehr positiven Bewertungen von VRClassroom in der Studie zeigt Grafik \ref{fig:survey7}, dass nur 42\% glauben, dass Virtual Reality zukünftig eine große Rolle im Schulunterricht spielen wird.
Trotzdem können sich 65\% vorstellen VRClassroom in ihrem Unterricht zu verwenden.

\begin{figure}[ht]
  \includegraphics[width=\textwidth]{images/survey7.eps}
 \caption{Einschätzungen zur Zukunft von VR und VRClassroom.}
  \label{fig:survey7}
\end{figure}

Grafik \ref{fig:survey8} zeigt, dass die meisten Lehrkräfte VRClassroom nur sporadisch einsetzen würden und nur wenige monatlich oder öfter darauf zurückgreifen würden. 

\begin{figure}[ht]
  \includegraphics[width=\textwidth]{images/survey8.eps}
 \caption{Vorraussichtliche Einsätze von VRClassroom im eigenen Unterricht.}
  \label{fig:survey8}
\end{figure}

Zuletzt wurde nach Szenarien (auch außerhalb des Unterrichts) gefragt, in denen die Teilnehmer VRClassroom sehen könnten. 
Dabei sahen die Teilnehmer hauptsächlich Szenarien, die im Zusammenhang mit Orten stehen. Von virtuellen Museumsbesuchen, interessanten Orten, die erkundet werden können über Tourismus im Generellen zu Vorbereitung von Exkursionen.

Besonders interessante Ideen war der Einsatz in der Fahrschule um die Situation im Auto möglichst realistisch nachzuempfinden und die Begleitung von Personen an ihrem ersten Arbeitstag. So kann der Arbeitsalltag einer Person und das Umfeld in der Arbeitsstelle im Rahmen einer Orientierungsfindung besser eingeschätzt werden.

Ein drittes Szenario war die Anwendung in Architekturbüros, die 360°-Fotos des Ist-Zustands und 3D-Modelle der geplanten Bauten präsentieren können.


\subsubsection{Demografie und persönliche Angaben}
Am Ende der Befragung wurden noch einige personenbezogene Fragen gestellt.
Es wurde nach dem Geschlecht, dem Alter (innerhalb von Altersgruppen) und der Schulart, an der die Person unterrichtet gefragt.
Zudem wurden die Teilnehmer gefragt, ob sie sich selbst als technikaffin bezeichnen würden und ob sie schon einmal eine VR-Brille benutzt haben.
Als Folgefrage sollte dann noch angegeben werden, welche VR-Brille schon einmal ausprobiert wurde. 
Insgesamt war die Teilnehmermenge in allen Aspekten sehr ausgewogen: Beim Geschlecht, dem Alter und den Schule, an denen die Personen unterrichten, waren die Teilnehmer sehr divers verteilt, sodass davon auszugehen ist, dass die Antworten der Studie die natürliche Verteilung der Gesellschaft gut abbilden.

Insgesamt haben an der Befragung 57 Lehrkräfte teilgenommen, wobei das Geschlechterverhältnis fast ausgewogen war.
48\% der teilnehmenden Lehrkräfte gaben an weiblich zu sein und 52\% männlich. 4 Personen haben die Frage nicht beantwortet. Die Altersverteilung war ebenfalls sehr ausgewogen, es nahmen aus allen Altersgruppen von unter 30 bis über 60 Lehrer an der Studie teil.

\begin{figure}[ht]
  \includegraphics[width=\textwidth]{images/survey9.eps}
 \caption{Demografische und persönliche Angaben.}
  \label{fig:survey9}
\end{figure}

Die überwiegende Mehrheit mit 91 bezeichnet sich selbst als technikaffin, wie Grafik \ref{fig:survey9} zeigt. Allerdings haben nur 43\% der Teilnehmer schon einmal eine VR-Brille ausprobiert. Darin liegt vermutlich auch einer der Hauptgründe für das Misstrauen der Teilnehmer daran, dass Virtual Reality zukünftig eine große Rolle im Unterricht spielen wird.

An den Antworten auf die Frage der Schule lässt sich ableiten, dass auch hier aus vielen verschiedenen Schultypen Lehrer geantwortet haben. Es waren damit Lehrkräfte für alle Altersgruppen von Grundschulkindern bis Erwachsenen Schülern an Berufs- oder Fachoberschulen eingebunden. 

\subsubsection{Ergebnis und Schlüsse aus der Evaluation}
Die Umfrage hat gezeigt, dass viele Lehrkräfte noch keine Erfahrung mit Virtual Reality generell und auch im Unterricht haben. Das Interesse dazu ist jedoch durchaus vorhanden. Wie bereits beschrieben fürchten jedoch viele der Teilnehmer, dass ein System wie VRClassroom aufgrund von schlechter, veralteter oder gar komplett fehlender Infrastruktur scheitern würde.

Das bestätigt die Idee hinter der Struktur von VRClassroom im aktuellen Zustand: Denn das System wurde so entwickelt, dass es bis auf die Funktion  Google StreetView Panoramas zu laden komplett ohne Verbindung zum Internet funktioniert. Es wird lediglich ein lokales Netzwerk benötigt, über das die Geräte mit dem Lehrer-Computer kommunizieren.
Auch die Entwicklung der Software als WebVR-App im Browser wird dadurch bestätigt. Denn so kann durch eine sehr geringe Investition in einen Satz Google Cardboards mit den eigenen Geräten der Schülerinnen und Schüler VRClassroom benutzt werden. Wird dann zu einem späteren Zeitpunkt in  echte VR-Headsets investiert, kann die bestehende Software weiter genutzt werden und es wird keine neue Eingewöhnung benötigt.


\subsection{VRClassroom Studie in Schulen}
Da das System speziell für die Nutzung im Schulunterricht entwickelt wurde, sollte auch die Nutzerstudie in diesem Szenario durchgeführt werden. Dafür wurden Lehrkräfte angefragt, die dann einen Teil ihrer Unterrichtszeit für einen Test der Software verwenden wollten und im Anschluss bereit waren einen kurzen Fragebogen dazu zu beantworten.

Insgesamt hatten sich 15 Lehrkräfte bereit erklärt mit ihrer Klasse das VRClassroom System auszuprobieren und im Anschluss einen Fragebogen auszufüllen. 

\bigskip

Für den Test sollte allen SchülerInnen ein Cardboard zur Verfügung gestellt werden, das die Kinder mir ihrem eigenen Smartphone als VR-Brille genutzt sollten. 
Der Rechner, auf dem das Programm installiert war, sollte den Lehrkräften bereitgestellt und den Lehrkräften im Vorfeld eine kurze Einführung in die Software gegeben werden. Dies ist trotz Absage der Studie durch ein Video geschehen. Das Video wurde zudem für die Online-Umfrage benutzt. 

\bigskip

Für die Durchführung der Studie war ein Zeitrahmen von 15 bis 20 Minuten veranschlagt. In dieser Zeit sollte die Lehrkraft die App starten, sich alle Schüler-Geräte damit verbinden und die Lehrkraft die eigentlichen Inhalte präsentieren.

Damit alle Studienteilnehmer, Lehrer und Schüler gleichermaßen, die gleichen Erlebnisse haben und sich zu allen Funktionen des Systems eine Meinung bilden können, hatte jede Lehrkraft ein vollständiges Set an Inhalten mit jeweils mindestens einem 360°-Fotos, einem 360°-Video und einem 3D-Modell.

\subsubsection{Freigabe Kulturministerium}
Da alle Studien, die an Schulen gemacht werden, einer Freigabe des Kultusministeriums bedürfen, wurde im Vorfeld der Studie beim Kultusministerium eine solche Freigabe beantragt. Für den Antrag werden alle Fragebögen und der gesamte Ablauf der Studie an das Kultusministerium zur Prüfung vorgelegt. Das Kultusministerium hat die Durchführung von Studien beschränkt, um den Datenschutz der Schülerinnen und Schüler zu sichern. Da allerdings in der Studie nur die Lehrkräfte befragt werden und die Fragebögen komplett anonym gehalten sind, sind die Daten der Schüler zu keinem Zeitpunkt in Gefahr.

Leider wurde der Antrag nie beantwortet, sodass die gesamte Studie kurzfristig abgesagt werden musste. Es hatten sich bereits 15 Lehrkräfte bereit erklärt mit ihrer Klasse das VRClassroom System auszuprobieren und im Anschluss den Fragebogen zu beantworten, die sehr an dem System interessiert waren. Da das VRClassroom-System speziell für den Einsatz im Unterricht entwickelt wurde, wäre eine Nutzerstudie im Unterricht sehr sinnvoll gewesen, um abschätzen zu können, ob es im aktuellen Zustand einen guten Mehrwert bietet oder noch Anpassungen braucht um gut im Unterrichtsablauf zu funktionieren.



%______________________________________________________________________
\cleardoublepage

\section{Ausblick}
Allen Prognosen nach wird VR (Quelle dazu?) in den kommenden Jahren eine immer größere Rolle einnehmen, von Ausbildung, Beruf bis in die Freizeit hinein, wird es immer mehr VR-Geräte und -Programme geben. 
Im Bildungsbereich bietet VRClassroom bereits jetzt eine Möglichkeit für Lehrkräfte ohne großen finanziellen Aufwand und ohne viel Vorwissen zu benötigen, VR-Inhalte in ihren Unterricht einfließen zu lassen. Im Folgenden wird darauf eingegangen, wie die Zukunft von Virtual Reality aussehen kann beziehungsweise was an nächsten Entwicklungsstufen passieren muss, damit VR den prognostizierten Durchbruch auch haben wird.
Außerdem werden bestehende Probleme von VRClassroom besprochen und wie das System in der Zukunft weiterentwickelt werden könnte.

\subsection{Zukunft von VR}
VR wird sich noch weiter verbreiten, da Geräte günstiger werden und mehr Inhalte da sind
-> Artikel mit Prognosen?

Trotz dieser steigenden Zahlen und positive Prognosen ist davon auszugehen, dass es trotzdem noch eine Weile dauern wird, bis Virtual Reality den Weg in die Schulen finden wird. Denn wie auch in den Ergebnissen der Studie zu sehen sind, glauben nur 38\%, dass Virtual Reality - in welcher Form auch immer - eine große Rolle im Unterricht spielen wird. Außerdem haben selbst von den teilnehmenden Lehrkräften bis dato nur 43\% jemals selber eine VR-Brille ausprobiert. Erst wenn ein übermäßiger Teil der Lehrkräfte selbst Erfahrungen sammeln konnten und genug Unterrichts-geeignetes Material zur Verfügung steht, besteht die Chance, dass Virtual Reality im Unterrichtsgeschehen einbezogen werden wird. VRClassroom soll dabei einen Möglichkeit bieten, mit den Schülerinnen und Schülern erste Erfahrungen mit Unterrichtsmaterialien zu sammeln.

Nochmal Artikel von davor aufgreifen?
Was muss sich ändern, damit es sich noch mehr verbreitet?
 -> standalone Geräte, müssen mehr Rechenleistung, bessere Displays und weniger Gewicht haben
 -> besseres Tracking?
 -> mehr Anwendungen abseits von Games
 
 Geräteweiterentwicklungen
 - Gestenerkennung ohne Controller?
 - Gazeinput wie zB EyeVR Paper?

\subsection{Probleme von VRClassroom}
In Tests haben sich noch ein paar Probleme gezeigt, die das VRClassroom System in seinem aktuellen Zustand noch hat, die beim ``echten'' Einsatz im Schulunterricht das Erlebnis stören könnten.
Diese Problem konnten aber leider nicht im Rahmen dieser Arbeit gelöst werden, das sie nicht wirklich Probleme der entwickelten Software sind, sondern technischen Grenzen von Geräten, die sich in der Zukunft aller Voraussicht nach weiter verschieben werden, sodass sie dann kein Problem mehr darstellen.

\subsubsection{Anzeigen großer Dateien}
Eines der größten Probleme, die VRClassroom momentan hat, ist die Anzeige von komplexen 3D-Modellen oder 360°-Videos mit einer großen Dateigröße. 360°-Videos sind besonders problematisch, wenn sie eine extrem hohe Auflösung (>4096) und eine hohe Framerate haben. 

Für diese Probleme gibt es allerdings momentan nicht wirklich eine Lösung, da die Daten über das WLAN übertragen werden müssen und bei 25-30 Geräten, die verfügbare Bandbreite zwischen allen Geräten geteilt werden muss und das einfach nicht genug pro Gerät hergibt, um so hohe Datenmengen schnell genug zu übertragen. 

(Hier vielleicht noch Vorrechnen mit einem geläufigen Router einfügen?)

Zudem war bei Tests vereinzelt zu sehen, dass bei extrem komplexen Modellen WebVR crasht und ein Neuladen der Schüler-App herbeigeführt wird, sodass das Gerät aus dem Cardboard genommen und die WebVR-Ansicht neu aktiviert werden muss.

\subsubsection{Synchronisierung}
Ein weiteres Problem bei VRClassroom ist es, alle Veränderungen exakt synchron auf allen Geräten darzustellen. Besonders beim Abspielen von 360°-Videos fällt dieses Problem auf. 

Der Websocket-Server broadcastet die Nachricht über den veränderten Inhalt an alle Clients und da die Nachricht nur eine kurze JSON-Datei ist, wird sie auch instantan auf allen Geräten empfangen. Trotzdem kommt es zu geringen Verschiebungen der Anzeigeaktualisierung bei den einzelnen Geräten.
Der Ton der Videos ist dann um einen Bruchteil einer Sekunde verschoben, was zu unangenehmen Vermischungen der Sounds der verschiedenen Geräte und Halleffekten führt. 

Das kann dadurch umgangen werden, dass nur der Sound aus der Teacher-App abgespielt wird, die Schüler Ohrstöpsel tragen oder sie die Abspiellautstärke auf ihren Geräten weit herunterdrehen, dass es nicht durch den ganzen Raum schallt. Eine echte Lösung dafür gibt es allerdings nicht.

\subsection{Mögliche Weiterentwicklungen an VRClassroom}
VRClassroom ist ein vollfunktionsfähiges System, dass laut Umfrageergebnissen bereits bei einigen Lehrkräften das Interesse geweckt hat, es in ihrem Unterricht einzusetzen. Trotzdem sind in der Entwicklung und bei der Online-Befragung noch mögliche Weiterentwicklungen von VRClassroom aufgefallen, die zukünftig noch integriert werden könnten.

\subsubsection{Hosten von VRClassroom Online statt Verbindung mit Lehrer-Rechner}
Eine mögliche Weiterentwicklung von VRClassroom könnte sein das ganze VRClassroom System nicht lokal auf dem Rechner der Lehrkraft laufen zu lassen, mit dem sich dann alle VR-Geräte verbinden, sondern auch eine Version anzubieten, die Online gehostet ist.

Die Entscheidung VRClassroom so zu entwickeln, dass es lokal läuft wurde allerdings ganz bewusste gefällt: Durch Vorgespräche mit den Lehrkräften, die an der Studie teilnehmen wollten und aus eigener Erfahrung wurde schnell klar, dass jede Schule unterschiedlich gut mit Netzwerk versorgt ist. Es gibt Schulen, die für Schüler gar kein WLAN anbieten oder dieses stark beschränken, sodass direkte Verbindungen zwischen den Geräten im Netzwerk unterdrückt werden, oder andere Restriktionen in ihrem Netz haben.
Um diese Probleme zu umgehen, wurde dann die Entscheidung gefällt, VRClassroom so zu entwickeln, dass es unabhängig vom Internet funktionieren kann. Einzig die Streetview-Funktion fällt dann weg. So kann dann einfach ein lokales Netzwerk eröffnet werden mit dem sich alle Geräte verbinden und VRClassroom problemlos genutzt werden. Dies könnte zum Beispiel mit einem Aufbau gelöst werden, bei dem der Rechner auf dem VRClassroom installiert ist und ein vorkonfigurierter Router immer gemeinsam genutzt werden und sobald Strom angesteckt wird der Router sein lokales Netzwerk aufmacht.

Ein weiteres Argument für die aktuelle Implementierung ist das schon zuvor in (HIER REF) besprochene Problem des Ladens von komplexen Modellen oder hochaufgelösten Videos, also großen Datenmengen. Laden alle Geräte lokal vom Lehrercomputer wird nicht die gesamte Internetverbindung der Schule lahmgelegt und auch die Übertragungsrate ist höher als beim Laden aus dem Internet. 

Wird VRClassroom nur in einem lokalen Netzwerk benutzt fällt allerdings wie bereits oben erwähnt die Funktionen zum Anzeigen von Google Streetview Fotos weg, denn dafür wird die Verbindung zum Internet benötigt. 
Zudem könnte es für technisch weniger verzierte Lehrkräfte schwieriger sein nachzuvollziehen, wo das Problem liegt, wenn etwa ein Schüler Gerät sich nicht verbinden kann, weil es nicht im Netzwerk ist, in dem VRClassroom läuft. 
Diese Punkte würden wiederum eher dafür sprechen die Struktur zu ändern, sodass VRClassroom online gehostet wird. 

Für den Rahmen der Arbeit ist die Entscheidung dafür gefallen, VRClassroom über den Rechner der Lehrkraft zu hosten, aber eine Online-Version wäre defintiv eine interessante Weiterentwicklung.

\subsubsection{Skalierung von 3D-Modellen auf echte Größe}
Um den Schülern zu vermitteln wie groß beziehungsweise klein die Dinge in echt sind, die gerade besprochen werden, könnte eine Funktion in der Lehrer-App eingefügt werden, die die 3D-Modelle auf Originalgröße skaliert. Dazu müsste selbstverständlich alle 3D-Modelle in Originalgröße vorliegen, was nur sehr selten der Fall ist, da diese Funktion für die meisten Verwendungszwecke nicht benötigt wird und daher einfach unbeachtet bleibt.

Um die gezeigten Inhalte gut zu begreifen und auch in der echten Welt einschätzen zu können, würden Schüler allerdings sehr davon profitieren diesen Einblick zu haben. Denn erst wenn man direkt davor steht, kann man begreifen wie groß der Eiffelturm ist oder wie klein eine extrem giftige Spinne wie die schwarze Witwe in Wahrheit ist.

Wie schon erwähnt müssen dazu zum einen die Modelle in der richtigen Größe vorliegen, aber zum anderen müsste auch die VR-Classroom-App angepasst werden, um diese Funktion sinnvoll zu erfüllen. Wird das Modell auf Originalgröße skaliert, die kleiner ist als die Größe, in der man sie genauer betrachten würde ist das kein Problem und könnte direkt erfüllt werden. Problematisch wird die Skalierung wenn sie extrem viel größer wird als das Modell normal angesehen wird, wie zum Beispiel beim Betrachten eines Gebäudes. Wird das Modell in der App extrem groß skaliert, wird es ab einer gewissen Größe nicht mehr vollständig angezeigt und Teile abgeschnitten. Das passiert, da React360 darauf ausgelegt ist, dass die Kamera in einer Art Kugel ist, in der sie sehen kann. Das dient dazu den Rendering-Aufwand nicht zu extrem werden zu lassen. (QUELLE dafür?) Alles was die Kugel durchbricht wird nur bis zum Rand der Kugel gerendert und alles was außerhalb liegt abgeschnitten. 

\subsubsection{Navigation von 3D-Modellen aus Schüler-App}
Momentan können Schüler keinen Einfluss darauf nehmen, welchen Teil eines 3D-Modells sie sehen können und welchen nicht, da nur aus der Teacher-App gesteuert werden kann wie das Modell skaliert und gedreht wird. 
Eine mögliche Erweiterung von VRClassroom könnte sein, dass die Schüler die 3D-Modelle selbstständig drehen und skalieren können beziehungsweise zu anderen Punkten am Modell springen können. 


\subsubsection{Ausfragemodus: Ausgewählter Schüler setzt Markierung}
Eine mögliche Weiterentwicklung von VRClassroom wäre eine Art ``Ausfragemodus'', bei dem die Lehrkraft aus der Liste der verbundenen Geräte eins auswählen kann, das dann eine Markierung setzen kann. Das ausgewählte Gerät hätte dann die Möglichkeit einmalig eine Markierungen zu setzen.

Dafür wäre es notwendig für die Schüler-Geräte eine Möglichkeit zu geben einen Punkt auszuwählen, an dem die Markierung gesetzt werden soll. Bei der aktuellen Implementierung mit einem Smartphone in einem Google Cardboard ist das noch nicht möglich. Dazu müsste mit dem Touch-Event vom Cardboard an der aktuellen Gaze-Position des Nutzers an dieser Stelle ein Marker gesetzt werden. Mit einem Headset, das bereits einen zugehörigen Controller hat, wäre das einfacher zu lösen, da lediglich die Position des ``Rays'' des Controllers abgefragt werden müsste. 

Da die Markierungen auch in der Teacher App über das iFrame mit der Student-App gesetzt werden ist die Kommunikation der Markerposition an die Teacher-App bereits implementiert und müsste nur leicht abgeändert werden. Möglicherweise wäre es gut die von Schülern gesetzten Marker noch in einer anderen Farbe darzustellen wie die der Lehrkraft, um für andere Schüler leichter erkennbar zu machen, welche Marker von welcher Person kommen. 

Momentan können Schüler nicht beeinflussen wie sie 3D-Modelle sehen, sondern nur den von der Lehrkraft ausgewählten Blickwinkel. Um auch auf 3D-Modellen sinnvoll Markierungen setzen zu können wäre es dann auch sehr sinnvoll den Schülern die Möglichkeit zu geben den Blickwinkel des 3D-Modells zu verändern. 

%______________________________________________________________________

\cleardoublepage
\fancyhead[LE,RO,LO,RE]{} % Keine Kopfzeile mehr oben auf jeder Seite
\section*{Inhalt der beigelegten CD}
Die beigelegte CD enthält folgende Dateien:

\begin{itemize}
\item Sourcecode von VRClassroom
\item Kompilierte VRClassroom App für MacOS, Linux und Windows
\item VRClassroom Video, das als Grundlage der Online-Umfrage diente
\item PDF der Thesis
\end{itemize}

Das Github-Repository, in dem der Sourcecode von VRClassroom sowie die Thesis sind ist zudem unter XXXXX erreichbar.
Das für die Umfragen produzierte Video zu VRClassroom ist auf Youtube unter https://youtu.be/b60PQ3bqjk0 zu erreichen.

%______________________________________________________________________

\cleardoublepage

\bibliographystyle{IEEEtran}
\bibliography{literature}

\end{document}
