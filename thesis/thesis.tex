\documentclass[11pt,a4paper,twoside]{article}

\usepackage[T1]{fontenc} % sonst geht \hyphenation nicht mit Umlauten
\usepackage[latin1]{inputenc} % man kann schreiben äöüß, statt "a"o"u"s
%\usepackage[utf8]{inputenc} % wie oben, aber UTF-8 als Encoding statt ISO-8859-1 (latin1)
\usepackage[ngerman,english]{babel} % deutsche Trennregeln, "Inhaltsverzeichnis" etc.
%\usepackage{ngerman} % Alternative zum Babel-Paket oben
\usepackage{mathptmx} % Times-Roman-Schrift (auch für mathematische Formeln)
\usepackage{framed}
\usepackage{longtable}
\usepackage{tabu}


% Zum Setzen von URLs
\usepackage{color}
\definecolor{darkred}{rgb}{.25,0,0}
\definecolor{darkgreen}{rgb}{0,.2,0}
\definecolor{darkmagenta}{rgb}{.2,0,.2}
\definecolor{darkcyan}{rgb}{0,.15,.15}
\usepackage[plainpages=false,bookmarks=true,bookmarksopen=true,colorlinks=true,
  linkcolor=darkred,citecolor=darkgreen,filecolor=darkmagenta,
  menucolor=darkred,urlcolor=darkcyan]{hyperref}

% pdflatex: Bilder in den Formaten .jpeg, .png und .pdf
% latex: Bilder im .eps-Format
\usepackage{graphicx}
\usepackage{subcaption}
\usepackage{sidecap}

\usepackage{fancyhdr} % Positionierung der Seitenzahlen
\fancyhead[LE,RO,LO,RE]{}
\fancyfoot[CE,CO,RE,LO]{}
\fancyfoot[LE,RO]{\Roman{page}}
\renewcommand{\headrulewidth}{0pt}
\setlength{\headheight}{13.6pt} % behebt headheight Warning

% Korrektes Format für Nummerierung von Abbildungen (figure) und
% Tabellen (table): <Kapitelnummer>.<Abbildungsnummer>
\makeatletter
\@addtoreset{figure}{section}
\renewcommand{\thefigure}{\thesection.\arabic{figure}}
\@addtoreset{table}{section}
\renewcommand{\thetable}{\thesection.\arabic{table}}
\makeatother

\sloppy % Damit LaTeX nicht so viel über "overfull hbox" u.Ä. meckert

% Ränder
\addtolength{\topmargin}{-16mm}
\setlength{\oddsidemargin}{25mm}
\setlength{\evensidemargin}{35mm}
\addtolength{\oddsidemargin}{-1in}
\addtolength{\evensidemargin}{-1in}
\setlength{\textwidth}{15cm}
\addtolength{\textheight}{34mm}
%______________________________________________________________________

\begin{document}

\pagestyle{empty} % Vorerst keine Seitenzahlen
\pagenumbering{alph} % Unsichtbare alphabetische Nummerierung

\begin{center}
\textsc{Ludwig-Maximilians-Universität München}\\
Department ``Institut für Informatik''\\
Lehr- und Forschungseinheit Medieninformatik\\
Prof.\ Dr.\ Heinrich Hußmann

\vspace{5cm}
{\large\textbf{Masterarbeit}}\vspace{.5cm}

{\LARGE Entwicklung eines Systems zur Nutzung von VR-Brillen im Unterricht}\vspace{1cm}

{\large Veronika Fuchsberger}\\\href{mailto:veronika.fuchsberger@campus.lmu.de}{veronika.fuchsberger@campus.lmu.de}

\end{center}
\vfill

\begin{tabular}{ll}
Bearbeitungszeitraum: & 01. 08. 2018 bis 30. 01. 2018\\
Betreuer: & Christoph Krichenbauer\\
Verantw. Hochschullehrer: & Prof. Heinrich Hußmann
\end{tabular}
%______________________________________________________________________

\clearpage
\section*{Zusammenfassung}
In der folgenden Arbeit wurden aktuelle VR-Headsets verglichen und nach Preissegmenten und Anwendungsfunktionen eingeordnet. Zudem wurde bisher existierende Software, die Virtual Reality für den Schulunterricht zugänglich macht diskutiert und bewertet und auf die Challenges eingegangen, die Virtual Reality-Systeme generell und im Bezug auf die Nutzung im Schulunterricht momentan haben.
Weiterhin wurde in der Arbeit auf die verschiedenen Inhalte für VR-Systeme eingegangen, welche Quellen es für diese Inhalte gibt und wie sie bearbeitet und selbst erstellt werden können.

Anschließend wurde basierend auf den zuvor erörterten Anforderungen an eine Software, die Lehrkräfte im Unterricht nutzen können, um 360°-Inhalte zu zeigen, VRClassroom entwickelt. Ein zwei-teiliges System, mit dem der Lehrer in eine App auf seinem Computer Inhalte hineinladen kann, die dann in einer Web-App auf VR-Brillen gezeigt werden. Die Lehrkräfte führen dabei das komplette VR-Erlebnis für die Schüler und können 360°-Fotos und -Videos hineinladen, Markierungen setzen und das Video synchronisiert für alle abspielen und pausieren. Außerdem können auch 3D-Modelle in die App geladen werden, die die Lehrkraft dann je nach Bedarf skalieren und drehen kann und an interessanten Stellen Markierungen setzen kann. Um zu sehen, welche Geräte verbunden sind, wird in der Lehrer-App ein Liste mit allen Geräten mit einem Aktivitätsindikator angezeigt. 

Anschließend wurde eine Online-Befragung mit Lehrkräften durchgeführt, bei der die Teilnehmer ein Video gezeigt wurde, das die Nutzung von VRClassroom erklärt, zu dem sie nachfolgend einige Fragen beantworteten. Die Mehrheit der Teilnehmer war dem System gegenüber sehr aufgeschlossen und konnte sich gut vorstellen VRClassroom zukünftig im Unterricht zu verwenden, um 360°-Inhalte zu zeigen.

Abschließend wurde diskutiert wie Zukunft von Virtual Reality im Schulunterricht aussehen könnte und wie VRClassroom weiterentwickelt werden könnte, um die Bedürfnisse von Lehrkräfte noch besser zu erfüllen beziehungsweise welche weiteren Funktionen das System noch interessanter machen würden.

\selectlanguage{english}
\section*{Abstract}

Short abstract of the work, maximum of 250 words.

\selectlanguage{ngerman}
\clearpage


\vfill % Sorgt dafür, dass das Folgende an das Seitenende rutscht

\noindent Ich erkläre hiermit, dass ich die vorliegende Arbeit
selbstständig angefertigt, alle Zitate als solche kenntlich gemacht
sowie alle benutzten Quellen und Hilfsmittel angegeben habe.

\bigskip\noindent München, \today

\vspace{4ex}\noindent\makebox[7cm]{\dotfill}

%______________________________________________________________________

\cleardoublepage
\pagestyle{fancy}
\pagenumbering{roman} % Römische Seitenzahlen
\setcounter{page}{1}

% Inhaltsverzeichnis erzeugen
\tableofcontents

%Abbildungsverzeichnis erzeugen - normalerweise nicht nötig
%\cleardoublepage
%\listoffigures
%______________________________________________________________________

\cleardoublepage

% Arabische Seitenzahlen
\pagenumbering{arabic}
\setcounter{page}{1}
% Geändertes Format für Seitenränder, arabische Seitenzahlen
\fancyhead[LE,RO]{\rightmark}
\fancyhead[LO,RE]{\leftmark}
\fancyfoot[LE,RO]{\thepage}

\section{Einleitung}
VR-Headsets immer mehr verbeitet, es gibt immer mehr Content
Umfrage, wie viele Leute schon VR-Headsets benutzt haben?
Statistiken zu:
Medien im Unterricht
Erfolg neuer Lehrkonzepte bzgl neue Medien


%______________________________________________________________________

% Der Befehl \cleardoublepage erscheint nur vor \section, nicht vor
% den "kleineren" Gliederungsbefehlen wie \subsection!
\cleardoublepage % Neue rechte Seite anfangen
\section{Existierende VR-Hardware-Systeme}
Es gibt inzwischen eine große Zahl verschiedener VR-Hardware-Systeme, die sich in drei Gruppen mit unterschiedlichen Anwendungsszenarien unterteilen: Die Computer-gestützten VR-Systeme, die Stand-alone VR-Systeme und die Smartphone-gestützten VR-Systeme.

\subsection{Merkmale von VR-Systemen}
Um die verschiedenen VR-Hardware-Systeme einordnen zu können, gibt es einige Merkmale, auf die ein Käufer achten sollte. Denn die verschiedenen Headsets sind für unterschiedliche Anwendungsszenarien entwickelt worden, sodass ein potenzieller Käufer zuerst entscheiden sollte wie er das VR-Headset einsetzen möchte.

Diese Merkmale können dazu herangezogen werden: Degrees of Freedom (DoF), die Displaygröße und -auflösung, die Frequenz des Displays, Field of View, Rechenleistung und Gewicht des Headsets, die verwendete Tracking-Methode, die mitgelieferten Controller beziehungsweise mögliche Input-Methoden, das Gewicht des Headsets und der Verkaufspreis.

\subsubsection{Degrees of Freedom}
Das Konzept der Degrees of Freedom, kurz DoF, entspringt ursprünglich der Mechanik. Wie Gans in seiner Arbeit ``Engineering dynamics: From the lagrangian to simulation'' erläutert, hat ein Körper grundlegend sechs Freiheitsgrade. Die Bewegungen in x-Richtung, y-Richtung und z-Richtung. Hinzu kommen dann noch die Rotationen um die jeweiligen Achsen. Kurz beschreibt er Freiheitsgrade als die minimale Anzahl an Variablen um ein System zu spezifizieren. \cite{Gans2013}

Freiheitsgrade wurden zudem viel genutzt um die Bewegungen von maritimen Fahrzeugen zu benennen: Die Bewegung in x-Richtung wird ``surge'' genannt, in y-Richtung ``sway'' und in z-Richtung ``heave''. Die Rotation um die x-Achse heißt ``roll'', um die y-Achse ``pitch'' und um die z-Achse ``yaw''. Diese Bezeichnungen beziehen sich dabei auf ein Schiff, das in x-Richtung ausgerichtet ist. \cite{Fossen1995}

Wie in Grafik \ref{fig:dof} zu erkennen, sind die Degrees of Freedom für VR-Headsets aus diesen Definitionen abgeleitet. Unterstützt ein System nur die Rotationsbewegungen um die drei Achsen wird es als ein 3DoF-System bezeichnet, kann es auch die Bewegungen im Raum verarbeiten ist es ein 6DoF-System. Um ein 6DoF VR-Gerät zu entwickeln, wird also positional Tracking der Nutzer benötigt. 

\begin{figure}[ht]
  \centering
  \includegraphics[width=\textwidth]{images/dof.eps}
 \caption{Degrees of Freedom eine Virtual Reality Headsets.}
  \label{fig:dof}
\end{figure}

\subsubsection{Display, Frequenz und Field of View}
Bei Displays von VR-Headsets gibt es zwei grundsätzliche Ansätze: Ein durchgehendes Display, das das Bild für beide Augen generiert und zwei kleinere unabhängige Displays. 
Auf ein einzelnes großes Display setzt eigentlich nur Sony mit ihrer Playstation VR, alle anderen Hersteller von VR-Headsets, die fest integrierte Displays haben, arbeiten mit getrennten Display für die beiden Augen.

Bei Smartphone-gestützten VR-Headsets hängt die Größe des Displays, sowie die Auflösung und damit auch das letztendliche Field of View vom eingelegten Smartphone ab. Da in aktuelle Smartphones allerdings sehr hochaufgelöste Displays eingebaut werden, stehen diese den fest integrierten nicht oder nur minimal nach. Nutzt man beispielsweise ein Galaxy S9+ hat man ein Display mit 2960x1440 Pixel Auflösung und 530 ppi, was einer Auflösung von 1480x1440 pro Auge entspricht, was damit höher aufgelöst ist als alle fest eingebauten Displays (abgesehen von der Oculus Quest, die erst im Laufe des Jahres auf den Markt kommen soll).

In den Tabellen \ref{tab:headsets1} und \ref{tab:headsets2} sind die Angaben zur Auflösung der Displays auf die Auflösung pro Auge zu verstehen.

Der Screen Door Effect und der Pixel Fill Factor beziehen sich auf den nicht leuchtenden Abstand zwischen zwei Pixeln. Der Screen Door Effect bezeichnet den Zustand, wenn diese Abstände deutlich als Gitter über dem Inhalt erkennbar werden. Der Pixel Fill Factor beschreibt die Dichte der Pixel zueinander. Ist der Pixel Fill Factor hoch, wird der Screen Door Effect schwächer oder sogar ganz verschwinden.
Da bei VR-Headsets das Display extrem nah am Auge ist, wurden bei den ersten Geräten von Oculus und HTC starke Screen Door Effekte wahrgenommen. Aber mit den Versionen, die dann letztendlich in den Handel kamen, sind diese Probleme großteils ausgeräumt worden.
% Paper: A Review Paper on Oculus Rift-A Virtual Reality Headset

Die Arbeit von XXX et al. schließt aus ihren Nachforschungen, dass das menschliche visuelle System schon ab 13ms komplette verschiedene Bilder wahrnehmen und damit unterscheiden kann. Das entspräche einer benötigten Framerate von knapp 80Hz. Die Forschungen zeigen allerdings auch, dass kleinere Veränderungen noch deutlich schneller, mit bis zu 500Hz wahrgenommen werden können. 
Die VR-Headsets sind dabei mit ihren zwischen 60 und 120Hz noch ein ganzes Stück davon entfernt, allerdings ist zu vermuten, dass nicht 500 Hz benötigt sind, um das natürliche Sehen komplett auszunutzen, da die Studie reinen Flackern von Lichtern getestet hat und keine konkreteren Inhalte.
% https://www.nature.com/articles/srep07861 (500Hz Flicker)
% Detecting Meaning in Rapid Serial Visual Representation ({RSVP}) at 13 ms Per Picture (Paper)

Wie Lin et al. in ``Effects of field of view on presence, enjoyment, memory, and simulator sickness in a virtual environment'' ausführen haben Menschen ein aus beiden Augen zusammengesetztes Sichtfeld von 180°. Ein weiteres Sichtfeld als 180° wäre also nicht sinnvoll, da es nicht mehr wahrnehmbar wäre.
Das ultimative Ziel für VR-Headsets wird also diesen Wert anstreben, mit dem momentan 100°-110° sind sie davon allerdings noch etwas entfernt. 
Augenabstand IPD.
Wie aber Lin et al. ebenso bemerkten sie jedoch in ihrer Studie, dass ein weiteres Field of View als 140° kaum noch positivere Effekte im Bezug auf Immersionsgefühl und Spaß (Enjoyment) hatten. Zusätzlich fanden sie heraus, dass das field of view mit dem Auftreten von Cybersickness korreliert. Je weiter also das Field of View ist, desto mehr litten die Studienteilnehmer an den Symptomen.

\subsubsection{Rechenleistung}
Generell lässt sich sagen, je höher die Rechenleistung, desto aufwändigere VR-Szenen können dargestellt werden. Deshalb können (zumindest momentan) stand-alone Geräte und Smartphone-gestützte Geräte nicht mit den Computer-gestützten konkurrieren, was die Kombination aus Display-Auflösung, Bildwiederholungsrate und Field of View angeht.
Da die Rechenleistung bei Computer- und Smartphone-gestützten VR-Headsets allerdings vom benutzen Rechner beziehungsweise Smartphone abhängt, ist die Rechenleistung nur bei den stand-alone Geräten ein Merkmal, das berücksichtigt werden muss.

Fast alle hier aufgeführten Geräte haben dabei den gleichen Prozessor eingebaut, den Qualcomm Snapdragon 835, nur die Oculus Go hat einen Qualcomm Snapdragon 821. Damit sind die anderen Geräte etwa 30\% schneller und verbrauchen dabei ungefähr 40\% weniger Energie als die Oculus Go.

Quellen:
https://vrodo.de/lenovo-mirage-solo-test-besser-als-oculus-go/
https://vrodo.de/oculus-quest-vs-oculus-go-vergleich-das-sind-die-unterschiede/
https://www.vive.com/cn/product/vive-focus-en/


\subsubsection{Inputmethoden}
Wie in Kapitel XXX (hier Ref einfügen) beschrieben, gibt es zwei grundsätzliche Input-Methoden für VR-Headsets, die momentan verwendet werden: Input über Controller mit verschiedenen Buttons und einem ``Raycaster'' in der VR-Welt und das Auslösen von Touch-Events auf dem Display des VR-Headsets. 

Da nur solche Headsets Touch-Events auf dem Display registrieren, die ein Smartphone als Display nutzen, sind es ebenso nur diese, die Input durch Touch-Interaktionen mit dem Display als Input erlauben. Das sind vor Allem Google Cardboards, die eine Art ``Arm'' nutzen um auf das Display zu drücken oder einige Nachbauten des ursprünglichen Google Cardboards, die Magneten verwenden um ein Touch-Event auf dem Display auszulösen.
(Diese Technik noch genauer erläutern?) Bei beiden Möglichkeiten ist es allerdings nur möglich an einer bestimmten Stelle ein Touch-Event auszulösen, sie müssen also in Verbindung mit einem Art Gaze-Punkt verbunden werden, um dem Nutzer sinnvollen Input zu erlauben.

Bisher gibt es keine Geräte, die mit Voice-Input oder Gaze Input kontrolliert werden können. Allerdings wäre das sehr wünschenswert, da damit die Nutzung von VR-Systemen auch für Menschen mit körperlichen Behinderungen möglich wird.

\subsubsection{Positional Tracking}
Um für ein VR-Headsets 6 DoF zu bekommen, muss die Position im Raum getrackt werden. Positional Tracking wird also generell nur von 6 DoF Headsets genutzt.

Momentan gibt es dazu 2 verschiedene Ansätze: Die eine Möglichkeit ist es Tracker im Raum aufzustellen, die dann die Position des Headsets und gegebenenfalls der Controller im Raum tracken. Die Computer-gestützten VR-Headsets nutzen alle diese Weise, das so genannte outside-Tracking, um die Brillen zu tracken.

Die zweite Möglichkeit ist das inside-out Tracking. Für das Inside-out Tracking müssen keinerlei Sensoren im Raum platziert werden, es werden nur Sensoren verwendet, die in das Headset integriert sind. Dafür nutzt die Lenovo Mirage Solo beispielsweise das WorldSense-System von Google, das die zwei in das Headset integrierten Kameras zur Feststellung der Position nutzt.
% https://developers.google.com/vr/discover/worldsense

Die Oculus Quest dagegen soll mit dem von Facebook selbst entwickelten ``Insight''-Tracking kommen. Dafür sind vier Kameras an den vorderen vier Ecken des Displays angebracht, die die Position im Raum anhand von Objekten im Raum bestimmen, die die Kameras erkennen. Das Insight-Tracking soll zusätzlich auch die Position der beiden Controller Tracken, sodass auch diese 6 Degrees of Freedom unterstützen.
% https://www.roadtovr.com/oculus-quest-hands-specs-tech-details-oculus-connect-5/

\subsubsection{Gewicht}
Die Spanne zwischen den Gewichten der einzelnen Virtual Reality-Headsets reicht von 610g im Maximum zu nur knapp 100g im Minimum. Die Entwicklung der Geräte geht zu leichteren Modellen, allerdings ist bei diesen neu vorgestellten Modellen, die ohne Kabel funktionieren wiederum das Gewicht des Akkus ein neues Problem. Die Hersteller versuchen das doch noch recht hohe Gewicht durch gute Riemensysteme auszugleichen, aber dennoch kann das Gewicht bei längerer Nutzung unangenehm sein.

Die Playstation VR hat ein kleines Gegengewicht im Bügel, sodass sich das Gewicht etwas besser verteilt.

Das leichteste aller VR-Headsets ist das Google Cardboard mit einem eingelegten Smartphone, es hat dann ein Gesamtgewicht von etwa 270g, wenn man die 96g des Cardboards mit einem durchschnittlichen aktuellen Smartphone addiert. Allerdings sind dadurch auch keine besonders angenehmen Polsterungen oder Riemen eingebaut.
Bei allen Geräten, die in Tabelle \ref{tab:headsets3} aufgeführt sind, das Gewicht immer ohne das Smartphone ist.

\subsubsection{Preis}
\label{subsec:price}
Wie in \ref{sec:challenges} erläutert sieht die Autorin des Artikels ``5 Major Challenges For The VR Industry'' den Preis als eines der Hautprobleme, dass sich VR noch nicht überall durchgesetzt hat.
Da aber besonders die Smartphone-gestützten VR-Systeme sehr günstig zu bekommen sind, gilt dieses Argument eigentlich nicht wirklich.

Trotzdem wird von Experten (QUELLE) erwartet, dass die Preise für VR-System weiter sinken werden, sodass auch die hochwertigen Lösungen erschwinglicher für alle Nutzergruppen werden.

Die Preise in den Tabellen \ref{tab:headsets1}, \ref{tab:headsets2} und \ref{tab:headsets3} sind in Dollar, wie sie auf dem amerikanischen Markt zu kaufen sind. Darin sind keine Umsatzsteuern enthalten.

Preis PlaystationVR ist ein Bundle-Preis, bei dem 2 Spiele enthalten sind.

\subsection{Computer-gestützte VR-Hardware}
Die erste Gruppe sind die Computer-gestützten Hardware-Systeme: Sie sind mit einem Kabel mit dem Rechner verbunden, der die Rechenleistung für die eigentlich Brille übernimmt. Sie eignen sich besonders für extrem rechenaufwändige Anwendungen, aber schränken durch ihre Kabel die Bewegungsfreiheit des Nutzers ein.

\begin{table}[]
\begin{tabular}{p{0.14\linewidth}|p{0.07\linewidth}|p{0.14\linewidth}|p{0.18\linewidth}|p{0.18\linewidth}|p{0.08\linewidth}|p{0.06\linewidth}}
\textbf{VR-Gerät} & \textbf{DoF} & \textbf{Display} & \textbf{Input} & \textbf{Tracking} & \textbf{Gewicht} & \textbf{Preis} \\ \hline
Oculus Rift & 6 DoF & 1080x1200 \newline 90Hz \newline 100° & Oculus Touch, Xbox One Controller & 2 optische Sensoren im Raum & 380-470g &  349\$ \\
Playstation VR & 6 DoF & 960 x 1080 \newline120Hz \newline 100° & PlayStation Move Controller
 & Playstation Camera & 610g & 349 \$ \\
HTC Vive & 6 DoF & 1080x1200  \newline 90Hz \newline 110° & SteamVR-Tracking Controller & 2 Laser-Sensoren im Raum & 555g & 499\$
\end{tabular}
  \caption{Vergleich der Computer-gestützten VR-Headsets.}
  ~\label{tab:headsets1}
\end{table}

% https://www.tomshardware.com/reviews/vive-rift-playstation-vr-comparison,4513-6.html
% Trackingsysteme und Controller Vergleich

% Vergleich: https://www.vrnerds.de/vr-brillen-vergleich/


\subsubsection{Oculus Rift}
Die Oculus Rift ist die erste von Oculus herausgebrachte VR-Brille. Sie wird durch einen Windows-Computer bespielt und wird durch zwei Kameras, die auf dem Schreibtisch um den Bildschirm des Computer platziert werden können, optisch getrackt. 

Mit der Oculus Rift kommen die zwei Oculus Touch Controller, diese haben jeweils zwei Action-Buttons, einen Menü-Button und einen Thumbstick sowie einen rückseitigen Button, der für den Zeigefinger gedacht ist. Die Buttons erkennen alle, ob ein Finger auflegt, was mehr verschieden Aktionen als nur die Events des Klickens der Buttons erlaubt.

Der IPD ist verstellbar zwischen 58 und 72mm, der Linsendurchmesser liegt bei circa 50mm. 

Die Oculus Rift ist mit bis zu 470g deutlich leichter als die beiden anderen Kabel-gebundenen VR-Brillen. Auch die Controller mit 160g merkbar leichter als die der HTC Vive.

Für ein vollständiges Setup wird ein Windows-Rechner (mit Bildschirm, Mouse und Keyboard), die Oculus Rift Brille, die Oculus Touch Controller und die zwei Sensoren benötigt. 

% https://www.vrbound.com/headsets/oculus/rift
% https://www.oculus.com/rift/

\subsubsection{HTC Vive}
Die HTC Vive hat ein sehr ähnliches Display verbaut wie die Oculus Rift. Sie haben die gleiche Auflösung und Bildwiederholungsrate von 90Hz, allein das Field of View ist mit 110° etwas weiter als bei anderen VR-Headsets. Im Vive-Headset lässt sich die Pupillendistanz und der Objektivabstand einstellen.

Für ein vollständiges Setup wird ein Windows-Rechner (mit Bildschirm, Mouse und Keyboard), die HTC Vive Brille, die HTC Vive Controller und die zwei Laser-Sensoren, genannt ``Lighthouses'' benötigt. Die Lighthouses müssen an diagonalen Enden des Spielfelds überhalb des Aktionsbereich aufgestellt werden, was die initiale Installation etwas aufwändiger macht. Durch die Lasertechnologie in den Lighthouses statt einer optischen Erkennung ist das mögliche Spielfeld der HTC Vive deutlich größer als das der anderen beiden VR-Systeme.

Um das Tracking perfekt einzustellen muss zu Beginn der ersten Nutzung das System kalibriert und der Spielbereich virtuell markiert werden.

Die Controller haben ein klickbares Trackpad, Menü- und System-Buttons auf der Vorderseite, einen Abzugsknopf für den Zeigefinger auf der Rückseite und Greifknöpfe an den Seiten des Controllers.

Die HTC Vive hat zusätzlich zu den Sensoren auch noch eine Kamera eingebaut, um gegebenenfalls im Weg stehende Gegenstände zu erkennen, um Unfälle zu verhindern.

Preislich liegt die Vive mit 499\$ am oberen Ende des Spektrums der VR-Headsets.

% https://www.vive.com/de/product/?gclid=EAIaIQobChMIgvauiYfA3wIVBcYYCh3oUwkIEAAYASAAEgLADfD_BwE#vive-spec

\subsubsection{Playstation VR}
Die Playstation VR ist dafür entwickelt mit der Playstation 4 Konsole benutzt zu werden, sie wurde also spezielle für den Anwendungsfall des Gamings entwickelt. Sie unterstützt sogar das Spielen mit zwei Spielern, wobei ein Spieler die Playstation VR nutzt und der zweite Spieler das Bild auf dem Fernseher sieht. Beide Nutzen die Playstation Move Controller.

Die Controller haben die gewohnten Playstation-Buttons und ihre Position im Raum wird getrackt.

Wie in Tabelle \ref{tab:headsets1} zu sehen ist die Auflösung der Playstation VR deutlich geringer als die der anderen beiden Headsets. Dafür hat sie eine extrem hohe Bildwiederholrate von 120Hz, was gerade für schnelle Spiele einen Vorteil bietet. Allerdings ist sie mit 610g recht schwer, sodass ein langes Tragen nicht sehr angenehm ist.

Zum Tracking der Position im Raum nutzt die Playstation VR die bereits existierende Playstation Camera. Darin sind 2 Kameras verbaut, die beide Tiefenwahrnehmung haben. Sie tracken optisch sowohl die Position des Headsets als auch die der Playstation Move Controller.

Für ein vollständiges Setup wird ein Playstation 4 Konsole, die Playstation VR Brille, die Playstation Move Controller und die Playstation Camera benötigt. 

% https://www.vrbound.com/headsets/sony/playstation-vr
% https://www.playstation.com/de-de/explore/playstation-vr/tech-specs/


\subsection{Stand-alone VR-Hardware}
Die zweite Gruppe sind die Stand-alone Hardware-Systeme wie etwa die Oculus Go, Oculus Quest oder Google Daydream. Es handelt sich hierbei um vollumfängliche Systeme, die ohne weiteres Equipment auskommen und auch keinen Computer benötigen. Da sie mit einem Akku betrieben werden, ist die Betriebsdauer allerdings eingeschränkt und auch die Rechenleistung ist deutlich geringer als die der Computer-gestützten Systeme.

% https://upload-magazin.de/blog/25071-standalone-vr-headsets/
% Vergleich: https://www.vrnerds.de/vr-brillen-vergleich/

\begin{table}[]
\begin{tabular}{p{0.14\linewidth}|p{0.08\linewidth}|p{0.14\linewidth}|p{0.15\linewidth}|p{0.15\linewidth}|p{0.1\linewidth}|p{0.08\linewidth}}
\textbf{VR-Gerät} & \textbf{DoF} & \textbf{Display} & \textbf{Input} & \textbf{Tracking} & \textbf{Gewicht} & \textbf{Preis} \\ \hline
Oculus Go & 3 DoF & 1280x1440  \newline 60-72Hz \newline 100° & & & 468g & 199\$ \\
Oculus Quest & 6 DoF & 1600x1440 \newline OLED \newline 72Hz \newline 100° & Oculus Touch Controller &  & & 399\$ \\
Lenovo Mirage Solo & & & & & 645g & \\
HTC Vive Focus & & & & & &
\end{tabular}
  \caption{Vergleich der Stand-alone VR-Headsets.}
  ~\label{tab:headsets2}
\end{table}

\subsubsection{Oculus Go}
Oculus Go: https://www.vrbound.com/headsets/oculus/go
Sensoren: Orientational Tracking
has no IPD adjustment

\subsubsection{Oculus Quest}
Oculus Quest: https://uploadvr.com/oculus-quest-specs-price-release-date/
has an IPD adjustment
Oculus says Quest?s weight isn?t locked in just yet, but presently it?s about 100 grams heavier than the Rift, which would put it around 570 grams

\subsubsection{Lenovo Mirage Solo}
Mirage Solo: https://www.lenovo.com/us/en/virtual-reality-and-smart-devices/virtual-and-augmented-reality/lenovo-mirage-solo/Mirage-Solo/p/ZZIRZRHVR01
Preis 400\$
General Usage: 2.5 hours
Dimensions: 204 mm x 269.5 mm x 179.86 mm
Sensoren: P-Sensor, Gyroscope, Accelerometer, Magnetometer
Controller: 3DoF Daydream Motion Controller

https://www.theverge.com/2018/5/4/17318648/lenovo-mirage-solo-google-daydream-standalone-vr-headset-review
keine eingebauten Lautsprecher

\subsubsection{HTC Vive Focus}

\subsection{Smartphone gestützte VR-Hardware}
Die dritte Gruppe bilden die Smartphone-gestützten VR-Systeme. Damit sind alle Systeme gemeint, bei denen das Smartphone integriert wird, um die VR-Inhalte zu zeigen.
Wie in Tabelle \ref{tab:headsets3} zu sehen sind damit die Merkmale für das Display, die Rechenleistung und das Tracking nicht beinhaltet, da diese abhängig vom eingelegten Smartphone sind. Diese liegen preislich auf einem sehr niedrigen bis mittlerem Niveau und bilden so eine gute Möglichkeit für alle Leute in die Welt der VR-Systeme einzutauchen, ohne direkt mehrere Hundert Euro ausgeben zu müssen.

\begin{table}[]
\begin{tabular}{p{0.19\textwidth}|p{0.07\textwidth}|p{0.15\textwidth}|p{0.26\textwidth}|p{0.08\textwidth}|p{0.06\textwidth}}
\textbf{VR-Gerät} & \textbf{DoF} & \textbf{Field of View} & \textbf{Input} & \textbf{Gewicht} & \textbf{Preis} \\ \hline
Google Daydream View & 3 DoF & 100° &  Daydream controller & 261g & 99\$ \\
Samsung Gear VR & 3 DoF & 101° & Gear VR Controller & 345g & 130\$ \\
Google Cardboard & 3 DoF &  90° & Hebel, der Touch-Event auf Display auslöst & 96g & 15\$
\end{tabular}
  \caption{Vergleich der Smartphone-gestützten VR-Headsets.}
  ~\label{tab:headsets3}
\end{table}

\subsubsection{Google Cardboard}
% Google Cardboard: https://store.google.com/product/google_cardboard
In das Google Cardboard können Smartphones mit Displaygrößen zwischen 4 und 6 Zoll eingelegt werden. Um die Google Cardboard App zu nutzen, müssen die Geräten dann mindestens Android 4.1 oder iOS 8.0 oder höher haben.

Es kann kein Controller verbunden werden. Es kann lediglich mit einem Hebel ein Touch-Event auf dem Display ausgelöst werden. In der Google Cardboard App wieder die Interaktion beispielsweise so ausgelöst, dass der Mittelpunkt des Displays ``geklickt'' wird, wenn das Touch-Event ausgelöst wird. So ist ein Input, wenn auch etwas umständlich, möglich. 

Das Field of View der Google Cardboards kann je nach eingelegtem Smartphone bis zu 90° breit sein.

Durch die einfache Konstruktion und den günstigen Preis ist das Google Cardboard das wohl am weitesten verbreitetste VR-Headset. Zudem gibt es davon diverse Arten von Nachbauten, die teilweise statt dem Hebel, der das Touch-Event auslöst, schwache Magneten an der Seite des Cardboards bewegen um wiederum ein Touch-Event auszulösen.

\subsubsection{Samsung Gear VR}
% Samsung Gear VR: https://www.samsung.com/de/wearables/gear-vr-r324/SM-R324NZAADBT/?cid=de_ppc_google_im-wearables-gearvr-q3restructured_20180720_samsungvr-broad&tmcampid=7&tmad=c&tmplaceref=c_DE_IMECOM_Warm_Brand_GearVR_Broad&tmclickref=b_%2Bsamsung%20%2Bvr&gclid=EAIaIQobChMIpLnFnuvC3wIVCKQYCh0XVweYEAAYASAAEgLilfD_BwE

Die Samsung Gear VR Brille funktioniert nur mit den Galaxy Geräten ab Version 6 oder neuer und benötigt mindestens Android Lollipop 5. Die Geräte werden auf dem USB-C-Stecker der Brille gesteckt, um die Verbindung herzustellen.
Wird ein Gerät zum ersten Mal verbunden werden Anweisungen angezeigt, die Oculus App herunterzuladen und einen Account zu erstellen beziehungsweise zu verbinden. Im Anschluss wird beim erneuten Verbinden mit dem Gerät die Oculus-App geöffnet um dort dann VR-Apps zu nutzen.

Da das Display vom Smartphone abhängig ist, kann hier nicht mit den anderen Geräten verglichen werden, aber das Field of View kann je nach eingelegtem Geräte bis zu 101° weit sein.

Im die Samsung Gear VR kommt mit einem Controller, der ein Touchpad und ``Zurück''-, ``Home''- und Lautstärke-Buttons hat. Auch den Controller der GearVR kann am Band des Headsets verstaut werden.

Die Gear VR ist nicht geeignet mit Brille genutzt zu werden. Dafür kann mit einem Rad an der Oberseite der Fokus eingestellt werden, sodass sie von Nutzern mit Sehschwächen trotzdem getragen werden kann.

Mit einem Preis von knapp 130\$ ist die GearVR die teuerste der drei Smartphone-gestützten VR-Brillen und schon nahe am Preis einer Oculus Go, die ohne ein extra Smartphone auskommt. Außerdem ist sie mit 345g ohne eingelegtem Smartphone eines der schwersten Headsets.

\subsubsection{Google Daydream View}
% Google Daydream View: https://store.google.com/product/google_daydream_view_specs
Das Google Daydream View Headset funktioniert ebenfalls nicht mit allen Smartphones, sondern nur Android-Smartphones: das Google Pixel 2 und 3, das Galaxy 8 und 9, das Zenfone AR von Asos und das LG V30 werden auf der Herstellerwebseite als besonders geeignet aufgeführt. Grundsätzlich muss ein Gerät die Google Daydream App installiert haben innerhalb derer dann VR Apps gesucht und mit dem Controller verbunden und interagiert werden kann. 

Google Daydream View kann den aktuellen Bildschirminhalt auf einen Chromecast streamen, sodass Andere sehen können, was gerade passiert.

Der Controller hat oben ein Touchpad, sowie einen Apps und Home-Button. An der linken Seite sind außerdem zwei Lautstärketasten platziert. Da allerdings Daydream View selbst keine Lautsprecher hat werden damit die integrierten Smartphone Lautsprecher beziehungsweise die verbundenen Kopfhörer gesteuert.

Der Strap mit dem die Brille am Kopf befestigt wird ist so gestaltet, dass man am hinteren Ende den Controller einstecken kann, sodass er nicht verloren gehen kann.

% ---------------------------------------------------------------------------------------------------------------------------

\cleardoublepage % Neue rechte Seite anfangen

\section{Aktuelle Einsatzgebiete von VR}
medizin
Gaming
Forschung
verschiedene Therapien zB Angsttherapien

\subsection{3D in VR}
WebVR macht das schon.
Dadurch muss die Szene doppelt gerendert werden.
Wie entsteht 3d generell?
Augenabstand
\subsection{Eingabemethoden in VR}
Controller
Voice Control
EyeVR GazeInput
Gesteninput

Input schon bei Geräten besprochen?

\subsection{VR-Systeme im Bildungsbereich}
\label{subsec:VReducation}
Hier kommen noch mehr Sachen aus den Papern rein!
Paper: Virtual reality for collaborative e-learning 2008

\subsubsection{ClassVR}
Ein Beispiel für ein VR-System im Bildungsbereich, das einen ganz ähnlich Ansatz wie das im Rahmen dieser Arbeit entwickelte System, ist ClassVR. 

ClassVR benötigt allerdings die ClassVR Headsets, und kann nicht mit anderen Geräten benutzt werden. Zudem wird eine Einrichtungsservice auf der Webseite angeboten, was vermuten lässt, dass die initiale Installation von ClassVR und das Setup der Geräte relativ komplex ist.

Die ClassVR Headsets können sowohl für VR, als auch AR genutzt werden. Die Headsets laufen auf Android, 
Die Geräte haben keinen Controller, sondern werden mit einfachen einhändigen Gesten und Kopfbewegungen gesteuert. Außerdem kann auch die Kontrolle durch die Lehrkraft übernommen werden, sodass alle verbundenen Geräte die Inhalte anzeigen, die die Lehrkraft auswählt.
Das interessante an ClassVR ist, dass die Headsets in Boxen gekauft werden, in denen sie gelagert und geladen werden können. Außerdem bietet die Firma auch fertige Inhaltssets an, die dann mit den Headsets genutzt werden können. Momentan sind die Inhalte aber ausschließlich für den Britischen Markt ausgelegt und an dortige Lernpläne angepasst und nur in englischer Sprache erhältlich.

Ein Set aus 8 Headsets in einem Koffer, in dem die Geräte geladen werden können, gibt es ab 2249£. Für einen Satz für eine Klasse mit 24 Kindern liegt der Preis also bei 6747£, was etwa einem Preis von 7650 Euro entspricht. Alternativ kann auch ein Schrank mit 30 Headsets erworben werden, dieser liegt preislich bei 7500£. 
Hinzu kommt der Preis für das Abo für das ClassVR Portal, mit dem die Geräte gesteuert und Inhalte ausgewählt werden können. Das sind jährlich noch einmal 299£
% https://www.lgfl.net/learning-resources/summary-page/classvr-pricing.aspx
% http://www.inclusive.co.uk/inclusive-classvr

Die Lehrkraft kann eine Playlist mit den verfügbaren Inhalten anlegen, die dann auf die Schüler-Headsets geladen wird. Zudem können auch eigene 360°-Fotos und -Videos hochgeladen werden.
Es können 360°-Fotos und -Videos sowie 3D-Modelle angesehen werden. 3D-Modelle werden mit Karten aktiviert, auf denen ein Code ist, der dann das Anzeigen der Modelle aktiviert, sobald die Kamera ihn erkennt. Die Modelle werden dann in das Bild des Klassenzimmers, das die Kamera liefert integriert. Die 3D-Modelle sind relativ simple Modelle mit einem niedrigen Polycount, sodass sie einigermaßen schnell geladen werden können. Sie werden in einer festen Größe angezeigt und können nicht skaliert werden.
Sind die Inhalte auf die Geräte geladen, kann in der Lehrer Preview gesehen werden, welche Geräte verbunden sind. Außerdem kann verfolgt werden, welche Bereich der Inhalt die Schüler in diesem Moment sehen und ein Punkt gesetzt werden, zu dem dann die Aufmerksamkeit der Schüler gelenkt wird. Die Geräte senden zudem einen Lifestream ihres Bildausschnitts an das ClassVR System, der von der Lehrkraft für jedes Gerät verfolgt werden kann. 
Die Kommunikation zwischen der Lehrer-App und den ClassVR-Headsets funktioniert über WLAN. Das ist auch einer der Punkte, die das System in der Praxis problematisch machen könnten. Werden für alle Schüler Inhalte über das internet übertragen und alle Schüler-Headsets streamen wiederum den Bildschirminhalt ins Internet, muss die Internetverbindung der Schule extrem gut sein. Denn die Datenmengen für 360°-Inhalte sind groß.
% http://www.classvr.com

\subsubsection{Google Expeditions}

\subsubsection{Senselab.io}
% https://senselab.io/en/home-en

\subsubsection{DiscoveryVR}
% https://www.discoveryvr.com
Nur Videos
Kostenlos
Apps für Andoird, iOS, Daydream, GearVR, Oculus und HTC vive

\subsubsection{Victory VR}


\subsubsection{Evtl}
https://unimersiv.com

https://www.victoryvr.biz/education/
nur Naturwissenschaften

%_______________________________________________________________________________________________________________________
\cleardoublepage % Neue rechte Seite anfangen
\section{Aktuelle Challenges an VR-Systemen}
\label{sec:challenges} 
In ihrem Artikel ``5 Major Challenges For The VR Industry'' von März 2018 sieht Wolwort den Preis, fehlende Inhalte für VR-Systeme, fehlende Businessmodelle, die unklaren Folgen für die Gesundheit der Nutzer und das Image als ``gimmick'' von Virtual Reality als Grund dafür, dass VR bisher noch nicht den prophezeiten Durchbruch hat. \cite{Wolwort2018}

Wie bereits in \ref{subsec:price} diskutiert ist der Preis eigentlich kein valides Argument mehr, seitdem es Smartphone-gestützte VR-Headsets gibt. Sie bieten die Möglichkeit für jeden einen Blick in die Virtual Reality Szene zu werfen.

Der zweite Problempunkt der fehlenden Inhalte ist nach wie vor akut: Abseits von der Welt der Spiele gibt es nach wie vor wenig Anwendungen, die für VR-Headsets entwickelt wurden. Besonders im Bildungsbereich sieht die Autorin eine Zukunft mit Virtual Reality-Apps, die eine komplett andere Wahrnehmung der Inhalte bieten. 
Besonders bei Simulator-Trainings wird bereits seit langer Zeit auf VR gesetzt (QUELLE), diese sind aber meist nicht auf konventionellen VR-Headsets nutzbar. Eine neuere Applikation für VR in der Bildung sind Google Expiditions, wie in \ref{subsec:VReducation} genauer erläutert.
Facebook hat außerdem die Facebook Spaces App entwickelt, die es den Teilnehmern eines Treffens erlauben sich mit ihren Freunden auszutauschen, Fotos zu teilen, Videotelefonate zu führen und zusammen in VR zu malen. Das lässt vermuten, dass sie auch an VR Meetingräumen arbeiten, die zukünftig die Zahl der notwendigen Dienstreisen der Mitarbeiter verringern könnten.
% Quelle: https://www.facebook.com/spaces

Drittens sieht Wolfort das fehlen von Business-Modellen, mit denen Unternehmen rentabel VR-Software anbieten können. Das Problem wird noch dadurch verschärft, dass es keinen technologischen Standard gibt, sodass native Anwendungen für jede Plattform einzeln entwickelt werden muss.
Wird die im Rahmen dieser Arbeit entwickelte Software VRClassroom genutzt, könnte ein Business-Modell darin bestehen passende Inhalte für den Unterricht anzubieten wie zum Beispiel ``Das Kolosseum''.

Die für die Autorin wohl kritischste Challenge für Virtual Reality sind die unerforschte mögliche Gesundheitsrisiken und besonders darin, dass sie noch nicht erforscht sind. Bekannt und erforscht ist bisher nur die Virtual Reality- oder Cybersickness, auf die in \ref{subsec:cybersickness} genauer eingegangen wird, die allerdings nur temporär existiert. Langfristige Risiken und Folgen starker Nutzung sind noch nicht erforscht worden. Im Gegenteil werden in der Forschung immer öfter VR-Systeme eingesetzt um Verhalten zu verstehen oder therapieren. Das spricht dafür, dass die Forscher darin keinerlei Gefahr sehen.

Als letztes sagt Wolfort, VR werde momentan nur als ``gimmick'' wahrgenommen. Das ist zu Teilen auch verständlich, da bisher nur wenige Anwendungen abseits von Spielen verfügbar sind. Wie aber bereits oberhalb beschrieben und auch im Rahmen dieser Arbeit entwickelt, gibt es nun immer mehr Apps, die einen Usecase abseits von Games haben. Diese Challenge wird also momentan gemeistert. \cite{Wolwort2018}

\subsection{Virtual Reality Sickness}
\label{subsec:cybersickness} 

Da in der Vergangenheit die Begriffe immer wieder vermischt wurden und dadurch oft unklar sind, werden im Folgenden die Begriffe ``motion sickness'',
``simulator sickness'', ``virtual reality sickness'' und ``cybersickness'' noch einmal differenziert und für diese Arbeit definiert.

\paragraph{Motion Sickness}
\paragraph{Simulator Sickness}
\paragraph{Virtual Reality Sickness}
\paragraph{Cybersickness}

2. Symptome

3. Umfrage

4. Mögliche Lösungen (Artikel dazu)

https://vrodo.de/virtual-reality-laut-umfrage-haben-60-prozent-probleme-mit-vr-uebelkeit/
Umfrage auf reddit: https://www.strawpoll.me/12015620/r
Unterschied zu motion sickness
Symptome
Lösungen?

Paper: Psychometric evaluation of the Simulator Sickness Questionnaire as a measure of cybersickness

https://www.ncbi.nlm.nih.gov/pubmed/6847562
https://www.twentymilliseconds.com/post/all-about-motion-sickness/
http://fortune.com/2018/02/06/virtual-reality-motion-sickness/
https://www.iflscience.com/technology/turns-out-answer-virtual-reality-sickness-right-front-your-face/


\subsection{Mindestalter zur Nutzung von VR-Systemen}
Fast alle Hersteller geben in ihren Nutzungsbedingungen oder Sicherheitsanweisungen ein Mindestalter für die Benutzung ihrer VR-Systeme an. Wie in Tabelle~\ref{tab:tableage} zu sehen, sind sich die Hersteller sehr einig, dass VR-Headsets nicht für kleine Kinder geeignet sind und frühestens für Jugendliche in Frage kommen.
\bigskip

Oculus weist konkret darauf hin, dass eine Nutzung ihrer Geräte unter 13 Jahren ihren Nutzungsbedingungen widerspricht und diese erst für diese Altersgruppe entwickelt sind.  ``The Services are intended solely for users who are aged 13 or older. Any registration for, or use of, the Services by anyone under the age of 13 is unauthorised, unlicensed and in breach of these Terms.'' Es werden allerdings keine genauen Gründe für diese Altersrestriktion angegeben.
  ~\cite{FacebookTechnologiesLLC2018}

Samsung geht dabei noch einen Schritt weiter und warnt vor einer Nutzung unter 13 Jahren, da sich jüngere Kinder in einer ``critical period in visual development''  ~\cite{SAMSUNG} befinden. Zudem sollen auch Kinder über 13 Jahren nur unter Aufsicht einer erwachsenen Person die Gear VR benutzen und dabei darauf achten regelmäßig Pausen zu machen. Eine lange Nutzung soll generell vermieden werden und die Kinder sollen während und nach der Nutzung beobachtet werden, ob sich ihre Fähigkeiten in der Hand-Augen-Koordination, Balance oder Multi-Tasking verschlechtern.

Außerdem wird eine Liste an Symptomen aufgeführt bei deren Anzeichen eine Nutzung sofort unterbrochen werden soll. Das sind: ``seizures, loss of awareness, eye strain, eye or muscle twitching, involuntary movements, altered, blurred, or double vision or other visual abnormalities, dizziness, disorientation, impaired balance, impaired hand-eye coordination, excessive sweating, increased salivation, nausea, lightheadedness, discomfort or pain in the head or eyes, drowsiness, fatigue, or any symptoms similar to motion sickness'', also verschiedenste Probleme beim Sehen und an den Augen sowie Probleme der Konzentration, Koordination und Balance.   ~\cite{SAMSUNG}

HTC dagegen gibt für die Nutzung der HTC Vive beziehungsweise Vive Solo kein genaues Mindestalter an. Sie geben allerdings an, dass das Gerät nicht dafür ausgelegt ist von kleinen Kindern genutzt zu werden. Sie warnen davor, dass Kinder Kleinteile verschlucken könnten oder sich und Andere auf anderem Wege damit verletzen können. Für ältere Kinder empfehlen sie die Aufsicht einer erwachsenen Person und dass die Nutzungszeit nicht zu lang ist. \cite{HTC2016}

Zudem wird für die Nutzung der HTC Vive ein HTC Account benötigt, der laut HTC erst ab 14 Jahren erlaubt ist. \cite{HTCCorporation}


In ihren FAQs gibt Sony an, dass man zur Nutzung ihrer Playstation VR Konsole mindestens zwölf Jahre oder älter sein sollte. Weitere Angaben oder Gründe dieses Mindestalter sind auch hier nicht zu finden. \cite{SonyEntertainmentLLC2017}
\bigskip

Gegenüber all der Warnungen der Geräte-Hersteller gibt Martin Banks, Professor of Optometry, Vision Science, Psychologie, and Neuroscience an der University of California in Berkeley in einem Interview im Frühling 2016 an, dass er ``no concrete evidence that a child of a certain age was somehow adversely affected by wearing a VR headset,'' [keine konkreten Beweise, dass ein Kind in einem gewissen Alter durch das Tragen von VR-Brillen negativ beeinflusst wurde] gefunden hat. Er ist überzeugt, dass die Hersteller der VR-Headsets die Nutzung durch Kinder ausschließen, um sicher sein zu können, dass nicht später bekannt werdende Probleme bei Kindern, die VR-Headsets nutzen, ihnen angelastet werden können. 

Weiter gibt er an, dass die Angst, dass die Entwicklung des Auges negativ beeinflusst wird im Gegensatz zur Nutzung von Büchern oder Smartphones viel unproblematischer ist, das durch die in die VR-Brillen eingebauten Optiken das Auge gar nicht auf eine so nahe Sache fokussiert, sondern auf weiter entfernte und somit keine Schäden der Augen nach sich zieht. 

Banks sieht als Gefahren lediglich die gleichen, die auch für Erwachsene bestehen: Das sind hauptsächlich Virtual Reality Sickness, auch bekannt als Cybersickness, und die Gefahr mit Personen oder Gegenständen im Raum zu kollidieren, während das VR-Headset getragen wird.  Ansonsten bewertet er die Nutzung der VR-Brillen von Kindern unproblematisch. \cite{Hill2016}

\bigskip

Momentan gibt es keine veröffentlichten Forschungsarbeiten zu den Gefahren für Kinder bei der Nutzung von VR-Brillen, dagegen sind viele Arbeiten zu finden, die VR-Systeme in der Therapie von verhaltensauffälligen, lernverzögerten oder behinderten Kindern erfolgreich einsetzen. Es gibt beispielsweise Arbeiten, die .... (HIER NOCH GUTE BEISPIELE RAUSSUCHEN)


\begin{table}[]
\begin{tabular}{p{0.2\linewidth}|p{0.2\linewidth}|p{0.5\linewidth}}
\textbf{VR-Gerät} & \textbf{Mindestalter} & \textbf{Weitere Angaben} \\ \hline
Oculus Rift \newline Oculus Go \newline  Oculus Quest & 13 Jahre & keine  \\
HTC Vive \newline HTC Vive Solo & keine genaue Altersangabe & HTC Account erst ab 14 Jahren  \\
Google Daydream & 13 Jahre & keine  \\
Samsung Gear VR & 13 Jahre & nur unter Aufsicht eines Erwachsenen  \newline regelmäßig Pausen machen \newline Warnung vor einer Vielzahl an Symptomen aus dem Bereich Koordination, Balance und Sehen \\
Playstation VR & 12 Jahre & keine  \\
Google Cardboard & keine Angabe & nur unter Aufsicht eines Erwachsenen
\end{tabular}
  \caption{Übersicht der verschiedenen VR-Headsets mit ihren jeweiligen Nutzungsmindestaltern}~\label{tab:tableage}
\end{table}




\cleardoublepage % Neue rechte Seite anfangen
\section{360°-Inhalte}
Neben der verwendeten Hardware spielen die verfügbaren Inhalte für das Virtual Reality Erlebnis eine erhebliche Rolle. Nur wenn die gezeigten Inhalte richtig in der 360°-Umgebung dargestellt werden bringt die Darstellung Nutzung von VR-Headsets die gewünschte Immersion.

Für 360°-Inhalte gibt es keine speziellen Dateiformate, die Inhalte werden in den gewohnten Formaten gespeichert. Grundsätzlich gibt es fünf verschiedene Medientypen von 360°-Inhalten: 360°-Fotos, -Videos, 3D-Modelle und spatial Audio, auch bekannt als 3D Audio. 

\subsection{Projektionen von 360°-Fotos und -Videos}
Um 360°-Bildmaterial in 2D Bildformaten zu speichern, werden sie auf verschiedene Arten ``zerschnitten'' oder verzerrt, um dann zur richtigen Darstellung wieder zusammengesetzt werden zu können. Das Panorama wird dazu auf ein zwei-dimensionales Bild gemappt, um problemlos in den bekannten Formaten speicherbar zu sein. Die Information, wie das Panorama gemappt wurde, wird dann in den Metadaten der Datei gespeichert und 360°-fähige Software kann mit diesen Informationen die Panoramas dann wiederum richtig darstellen.

Die Projektionen wurden ursprünglich dazu entwickelt Weltkarten drucken zu können, auf denen die gesamte Erdkugel zu erkennen ist. Sie wurden als ``map projections'' bekannt. Daraus haben sich die heutigen ``image projections'' der Panoramafotografie entwickelt. (Quelle map projections?)

Im Folgenden werden die bekanntesten und am weitesten in der 360°-Szene verbreiteten Projektionen erläutert und verglichen:

\subsubsection{Equirectangulare Projektion}
Verbildlichen könnte man sich die Projektion etwa damit, dass eine Kugel in einem Zylinder sitzt, der den gleichen Durchmesser wie Kugel hat und dann die Pole packt und immer weiter nach außen zieht, bis die Kugel die Form des Zylinders angenommen hat.

Bei der equirectangularen Projektion werden die Gradzahlen von Longitude und Latitude auf die x und y-Werte in einem Grid gemappt. Ein Bildpunkt an (53°/ 90°) wird dann im projizierten Bild an (53,100) dargestellt. Es wird keine zusätzliche Skalierung oder Transformation vorgenommen.
Dadurch werden Äquator-nahe Bildbereiche wenig verzerrt und die Pol-nahen Regionen extrem in horizontaler Richtung verzerrt, sodass die Pole der sphere auf die Breite des Äquators gestrecht werden. 
Vertikale Linien bleiben unverzerrt erhalten, horizontale Linien werden bis auf den Äquator verzerrrt. Dadurch bleiben zudem keine Winkel erhalten.

Die equirectangulare Projektion hat dadurch den Nachteil, dass die Bildelemente an den Polen eine deutlich höhere Auflösung haben als die Bildbereiche am Äquator, was meist diejenigen sind, die interessanter für den Betrachter sind. Außerdem ist durch die starke Verzerrung das Bild an den Rändern schwer zu erkennen.

Trotz diesen Nachteilen ist die equirectangulare Projektion die am weitesten verbreitete Projektion in der 360°-Szene. Das liegt vermutlich daran, dass sie durch das einfache Mapping am leichtesten zu verarbeiten und wieder zu einer Kugel zusammensetzbar ist. 

Die Online-Video-Plattform Vimeo nutzt für 360°-Videos die equirectangular Projektion und auch Youtube hat bis 2017 diese Projektion benutzt bis sie auf ihren eigens entwickelten Standard EAC gewechselt haben.
% https://vimeo.com/blog/post/terms-you-need-to-know-to-create-360-video
% https://youtube-eng.googleblog.com/2017/03/improving-vr-videos.html

\subsubsection{Zylindrische Projektion}
Die zylindrische Projektion hat einen änhlichen Ansatz wie die Equirectangulare. Auch hier kann man sich die Kugel in einem stehen Zylinder vorstellen, der die Kugel am Äquator berührt. Dann wird die Oberfläche der Kugel aus dem Mittelpunkt auf den Zylinder projiziert. Dadurch bleiben vertikale Linien und der Äquator unverzerrt, alle anderen Linien werden allerdings verzerrt. 

Durch diese Projektion werden Bildbereiche die Nahe den Polen extrem in vertikaler Richtung verzerrt, sodass sie viel größer erscheinen, als sie eigentlich sind. Daher ist ein vertikales Field of View, das höher als 120° ist, für zylindrische Projektion nicht zu empfehlen.

\subsubsection{Mercator-Projektion}
Die Mercator-Projektion ist die wohl berühmteste Projektion für Karten, die seit ihrer Entwicklung 1569 für Weltkarten genutzt wird und auch heute noch besonders in der Nautik eine große Rolle spielt. 

Eine Mercator-Projektion sieht letztendlich sehr ähnlich wie eine zylindrische Projektion aus, die Berechnung ist aber ungleich viel komplexer. Der große Vorteil einer Mercator-Projektion ist, dass sie winkelttreu ist. So kann also eine Route zwischen zwei Punkten gezeichnet werden und die Gradzahl, die sich auf der Karte abläsen lässt ist exakt die, die man dann auf dem Kompass verfolgen muss, um ans Ziel zu kommen. Dadurch bewirkte die Mercator-Projektion einen Durchbruch in der Kartografie und Seefahrt. Auch heute sind viele Karten noch in der Mercator-Projektion abgebildet.

Problematisch an dieser Projektion ist wie auch bei der zylindrischen Projektion die extreme Verzerrung von Bereichen, die weit vom Äquator entfernt sind. Sie kann auch keine komplette 180° vertikal-Projektion produzieren, sondern nur bis 85° nördlich und südlich, und deckt damit einen Bereich von 170° an. Diese extreme Verzerrung führt beispielsweise dazu, dass Grönland größer als der ganze afrikanische Kontinent wirkt, obwohl es in der Realität nur etwa einem Vierzehntel der Fläche entspricht.

Varianten der Mercator-Projektion sind bis heute der de-facto Standard für digitale Karten. Auch Google Maps hat lange eine Abwandlung der Mercator Projektion, die so genannte ``Web Mercator Projection'' verwendet. Mit einem Tweet im August 2018 haben sie jedoch ihren Wechsel zu einer Kugeldarstellung angekündigt. Bisher ist diese neue 3D Globe Ansicht allerdings nur auf dem Desktop ausgerollt und nicht in den mobilen Apps zu sehen.
% https://twitter.com/googlemaps/status/1025130620471656449?ref_src=twsrc%5Etfw%7Ctwcamp%5Etweetembed%7Ctwterm%5E1025130620471656449&ref_url=http%3A%2F%2Fgeoawesomeness.com%2Fthe-earth-is-not-flat-google-maps-confirms-with-globe-mode%2F
 
\subsubsection{Rectilinear-Projektion}
Die Rectilinear-Projektion ist anders als die vorigen Projektionen keine zylindrische Projektion, sondern lässt sich verbildlichen indem man eine Kugel auf eine Fläche legt und einen Teil der Kugel vom Mittelpunkt der Kugel auf die Ebene projiziert. Ein einzelnes rectilienares Bild stellt also kein komplettes 360°-Panorama dar. Um ein komplettes Panorama zu erhalten werden dann einfach mehrere rectilineare Projektionen zusammen gesetzt.

Ein großer Vorteil dieser Projektion liegt darin, dass gerade Linien erhalten bleiben. Eine starke Verzerrung in dieser Projektion ist in den Ecken des Bildes zu finden und wird umso stärker, je weiter das Field of View ist. Will man also ein möglichst unverzerrtes Panorama, sollten mehrere rectilienare Bilder mit einem kleineren Field of View zusammengesetzt werden.

Bekannt unter dem Begriff Cubemap Projektion oder Skybox aus der Gaming-Szene ist die Kombination mehrer rectilinearer Bilder. Ein bekanntes Beispiel, bei dem Cubemap-Bilder verwendet werden, sind die Google Streetview-Panoramas. 

\subsubsection{Stereographische Projektion}
Die Stereographische Projektion hat den gleichen Ansatz wie die rectilineare Projektion. Der Unterschied ist, dass nicht aus dem Mittelpunkt der Kugel projiziert wird, sondern aus dem Punkt an der Oberfläche der Kugel, der exakt gegenüber dem Punkt liegt, der die Ebene berührt, auf die projiziert wird. 

Auch mit der stereographischen Projektion ist keine volles 360°-Panorama möglich, sondern nur bis 330°. Empfehlenswert ist es allerdings ein Panorama aus drei 120°-Projektionen zusammenzusetzen. 

Stereographische Projektionen erzeugen immer ein Rundes Bild, das einer Fish-Eye Projektion ähnlich sieht.

\subsubsection{Pannini/ Vedutismo-Projektion}
Die Vedutismo oder Pannini-Projektion ist ebenfalls eine zylindrische Projektion und hat damit dem Vorteil, dass alle vertikalen Linien unverzerrt bleiben. Mit dieser Projektion werden jedoch zusätzlich auch noch alle Linien, die durch das Projektionszentrum gehen ebenfalls als gerade Linien dargestellt. Die Perspektive mit einem zentralen Fluchtpunkt wird dadurch also für einen weiten Blickwinkel möglich.

Die Pannini-Projektion lässt sich als eine rectilineare Projektion einer Kugel auf einen Zylinder vorstellen. Das Projektionszentrum kann dabei überall auf der Sichtachse liegen mit der Distanz d zur Zylinder-Achse. Ist d also d=0, wird eine normale rectilineare Projektion erzielt, geht d gegen Unendlich entsteht eine zylindrische Projektion. Durch die Variation von d werden jeweils unterschiedliche Projektion erreicht. 
Die Pannini-Projektion entspricht dabei d=1.

Eine einzelne Pannini-Projektion ist allerdings für maximal 150° in beiden Richtungen möglich, sodass ein volles Panorama wiederum durch mehrere zusammengesetzt werden muss, ganz ähnlich wie bei der rectilinearen Projektion.

\subsubsection{Equi-Angular Cubemap (EAC)}
Wie bereits erwähnt hat Youtube vor einigen Jahren ihren eigenen Standard für 360°-Bildprojektionen Equi-Angular Cubemap entwickelt. Der Name Cubemap verrät bereits, dass auch dieses Bild wieder aus mehreren Projektionen zusammengesetzt wird. Hierbei werden allerdings die einzelnen Projektionen nicht aus rectilinearen Projektionen zusammengesetzt.

Die Teile der EAC-Cubemap sind sehr ähnlich wir rectilineare Projektionen mit dem Unterschied, dass Bildabschnitte in Äquatornähe genauso hoch aufgelöst werden wie Bildbereiche nahe den Polen. Die Pixeldichte bleibt also an allen Bildbereichen gleich, sodass alle Bereiche des 360°-Bildes eine gleiche Auflösung haben. Letztendlich führt das dann zu einer besseren Videoqualität mit gleicher Auflösung.

% https://blog.google/products/google-ar-vr/bringing-pixels-front-and-center-vr-video/
% Quelle:  https://youtube-eng.googleblog.com/2017/03/improving-vr-videos.html

\subsubsection{Fish-Eye Projektion}
Die Fish-Eye Projektion lässt sich wie die Spiegelung in einer Silberkugel vorstellen. Sie kann eine Szene mit bis zu 180° in beiden Dimensionen darstellen und erzeugt damit ein rundes Bild. Bildbereiche nahe dem Zentrum werden dann vergrößert wahrgenommen, während Bildbereiche nahe den Rändern stark verzerrt werden. Die Fish-Eye Projektion ist meist nicht als End-Projektion sondern eher als Output einer Kamera zu finden, das dann noch weiter verarbeitet wird.
% https://www.cambridgeincolour.com/tutorials/image-projections.htm

Viele 360° Kameras, wie zum Beispiel auch die Rico Theta haben zwei Linsen, die jeweils ein 180°-Fish-Eye Panorama erzeugen. Diese können dann in der Nachbearbeitung zu vollen 360°-Panoramas in einer anderen Projektion zusammengesetzt werden. Eine volles Panorama ist mit einer Fish-Eye Projektion nicht darstellbar und auch das zusammensetzen mehrerer Fish-Eye Panoramen wie bei rectilinearen Projektionen ist nicht möglich
% https://theta360.com/en/about/theta/technology.html

\subsection{360°-Fotos}
Bei 360°-Fotos handelt es sich um Bilddateien, die wenn sie mit Programmen angesehen werden, die 360°-Inhalte korrekt anzeigen können, ein volles Panorama um den Nutzer bilden, in dem er sich dann umsehen kann. 

Die wahrscheinlich bekanntesten Anwendungen, die 360°-Bilder anzeigen sind Google Streetview und 360°-Fotos auf Facebook, die der Nutzer einfach durch bewegen des Smartphones erkunden kann. (Quellen!)

\subsubsection{Quellen für 360°-Fotos}
Es gibt bisher keine Plattform, die sich darauf spezialisiert hat 360°-Fotos, die in der Lehre genutzt werden können, anzubieten. 

Die verbreiteten Anbieter für Stockphotos wie zum Beispiel Shutterstock oder iStockphoto bieten zwar bereits 360°-Fotos an, aber die meisten dieser Stockphotos zeigen keine relevanten Inhalte, sondern nur eine schöne Landschaft oder einen Raum. Sie sind also für zur Nutzung im Unterricht nicht geeignet. Zudem sind diese Bilder oft teuer oder mit einem Abo beim jeweiligen Anbieter verbunden.

Die aktuell beste Quelle für 360°-Fotos ist die Foto-Sharing-Plattform Flickr, auf der die Urheber das Herunterladen ihrer Bilder erlauben und auch die Nutzungslizenz dafür angeben können. Eine Suche nach ``equirectangular'' und der Creative Commons Lizenz ist dort einfach möglich und man wird schnell fündig. 
Flickr hat zudem die Funktionalität die equirectigularen 360°-Fotos richtig darzustellen, sodass die Nutzer sich im Bild umsehen können, indem sie den Viewport verschieben.

Ein engagierter Lehrer hat sogar eine Gruppe gegründet, die speziell dafür gedacht ist, dass Lehrkräfte aus aller Welt ihre 360°-Fotos teilen können, damit Andere sie auch in ihrem Unterricht nutzen können.

% Flickr-Group: https://www.flickr.com/groups/360images4schools/
% Flickr Search equirectangluar + Creative Commons: % https://www.flickr.com/search/?text=equirectangular&license=2%2C3%2C4%2C5%2C6%2C9

Eine weitere Möglichkeit an 360°-Fotos zu kommen, die größtenteils ebenfalls von Hobbi-Fotografen stammen, ist die Google Photo Sphere Community, eine Gruppe auf Google Plus. Allerdings ist hier der Download nicht immer erlaubt und die Rechte für die Bilder auch nicht immer angegeben.
% Google Photo Sphere Community: https://plus.google.com/communities/115970110085205516914/stream/abbc1e71-8239-4ab0-9de3-3f6429e7681f

Außerdem gibt es noch die Möglichkeit 360°-Fotos bei 360cities oder Airpano zu lizensieren. 
360cities ist eine Plattform auf der professionelle 360°-Fotographen ihre Panoramen hochladen und zum Verkauf anbieten können. Dabei handelt es sich oft um sehr gute Bilder, es ist aber relativ teuer, sodass sich das für die Lizenzierung für eine einzelne Lehrkraft nicht wirklich anbietet.

Airpano bietet speziell Bilder von Orten auf der Welt an wie zum Beispiel Städten oder geschichtlichen Orten aus der Luftperspektive an. Die Lizenzierung dieser Bilder ist mit ca 1000 Euro pro Jahr extrem teuer, sodass das für die Nutzung im Unterricht eher nicht in Frage kommt.

% www.360cities.net
% https://www.airpano.com

\subsubsection{Erstellung und Bearbeitung von 360°-Fotos}
Es gibt grundlegend zwei verschiedene Arten 360°-Bilder zu erstellen:  Die eine Möglichkeit, die für den Nutzer deutlich einfacher ist, ist die Verwendung einer 360°-Kamera wie zum Beispiel die Vuze 360, 360 Rize, Rico Theta, Samsung Gear 360, oder Panono 360 Kamera. Diese Kameras haben mehrere Linsen eingebaut und generieren aus den Einzelbildern ein fertiges 360°-Foto, das dann direkt weiterverwendet werden kann.

% https://vuze.camera/vr-software/
% https://www.360rize.com/vr/
% https://theta360.com/de/
% https://www.samsung.com/de/wearables/gear-360-r210/
% https://www.panono.com/en/home/

Die zweite Möglichkeit ist mehrere normale Kameras auf ein Rack, also ein Stativ für mehrere Kameras, zu installieren, bei dem jede Kamera einen anderen Bildabschnitt abdeckt und die Bilder dann nachträglich manuell zusammenzusetzen. 
Den Vorgang des Zusammensetzens der Bilder nennt man Stitching. Erstellt man 360°-Fotos durch zusammenstitchen mehrerer ``normaler'' Kamerabilder ist es wichtig, dass dann die richtigen Metadaten zum Bild hinzugefügt werden, damit Software, die 360°-Bilder anzeigen kann, weiß, dass es sich um ein 360°-Bild handelt und welche Projektion es hat.

360°-Bilder, die direkt von einer 360°-Kamera kommen, haben bereits die passenden Metadaten und sie müssen nicht manuell hinzugefügt werden.

Metadaten wichtig: https://www.panotwins.de/technical/how-to-add-mandatory-photo-sphere-meta-data-to-an-equirectangular-image/
% http://u88.n24.queensu.ca/~bogdan/#m_workspace
https://developers.google.com/streetview/spherical-metadata

\subsection{360°-Videos}
Aufhänger?
https://filmpuls.info/360-videos-zukunft/

Vimeo:
https://vimeo.com/blog/post/introducing-vimeo-360

 https://vimeo.com/blog/post/terms-you-need-to-know-to-create-360-video
 Vimeo Videos in equirectingular
 
 https://youtube-eng.googleblog.com/2017/03/improving-vr-videos.html
 YT früher equirectangular, jetzt EAC

\subsubsection{Quellen für 360°-Videos}
Eine gute Quelle für 360°-Videos is genau wie bei 360°-Fotos Vimeo. Wie bereits oben beschrieben, können die Eigentümer der Videos angeben unter welcher Lizenz sie das Video veröffentlichen und es gegebenenfalls zum Download freigeben. In den meisten Fällen sind sie frei zur privaten Nutzung und nur die kommerzielle Weiterverwendung untersagt.

Auch bei Youtube gibt es viele interessante 360°-Videos. Insgesamt ist es aber nicht so einfach direkt auf der Plattform die Videos zu finden, die interessant sind, da oft bei der Such innerhalb der Plattform normale Videos hineingespült werden. 
Youtube lässt den Download der Videos nicht zu, allerdings stehen Apps auf fast allen Geräten und Plattformen zur Verfügung: die Youtube VR-App gibt es im Oculus und SteamVR-Store, die iOS und Android Youtube App kann die 360° Videos ebenfalls für das Abspielen in VR-Headsets anzeigen.
Da bei Youtube immer ersichtlich ist, wer der Urheber des Videos ist, besteht hier allerdings die Möglichkeit in Kontakt zu treten, um zu Erfragen, ob das Video für die Nutzung im Unterricht bereitgestellt werden könnte.

Auch bei Facebook gibt es 360°-Videos. Wie bei fast allen anderen Videoplattformen sind sie auch hier nicht zum Download freigegeben. Sie sind mit der Facebook App auf den mobilen Geräten, der Facebook 360 App im Oculus Store und im Browser anzusehen. Im VR-Modus mit Bilder für jedes Auge einzeln kann man die Videos allerdings nur in der Facebook 360 App ansehen.

Die BBC hat sogar eine App veröffentlicht, die exklusiv VR-Inhalte für ihre Leser bereitstellt und speziell für die Nutzung mit dem Google Cardboard entwickelt wurde. Darin zeigen sie von gefilmten 360°-Reportage bis hin zu 360°-Animationsfilmen Geschichten, die speziell für das 360°-Medium entwickelt wurden.
% https://www.theguardian.com/technology/ng-interactive/2016/nov/10/virtual-reality-by-the-guardian

Auch die New York Times hat einige Beiträge seit 2016 in 360°. Die Videos können im Browser oder in der New York Times-App angesehen werden. Der Player unterstützt allerdings keine VR-Headsets, die Smartphones können also nur genutzt werden um sich in der Szene umzusehen indem sie in der Hand gehalten und bewegt werden.
Ein Download ist auch hier nicht vorgesehen.
% https://www.nytimes.com/2016/11/01/nytnow/the-daily-360-videos.html

Eine schöne Quelle deutschsprachiger 360°-Videos gibt es vom ZDF, leider sind diese allerdings ebenfalls nicht zum Herunterladen, sondern nur über die ZDF VR-App, die es für Android, iOS und GearVR gibt, oder im Browser anschaubar.
% https://vr.zdf.de

Lizenzierbare 360°-Videos gibt es wie auch Fotos bei 360cities und Airpano. Diese sind aber eher gedacht um in Filme eingebaut zu werden, als einzeln verwendet zu werden. Sie sind also eher vergleichbar zu Stockphotos und damit wirklich nicht im Bildungsbereich verwendbar.
% www.360cities.net und https://www.airpano.com

In die gleiche Kategorie fallen dabei die Videos von Videoblocks.com, die ebenfalls einige schöne 360°-Videos haben, die aber keinen wirklichen Inhalt haben, sondern eher als Bildmaterial für die Weiterverarbeitung gedacht sind. Alle drei Anbieter sind zudem nicht besonders günstig.
% https://de.videoblocks.com/videos/footage/360-files

\subsubsection{Erstellung und Bearbeitung von 360°-Videos}
https://de.wikipedia.org/wiki/360-Grad-Video

Entweder direkt 360 Kamera oder mehrere und dann stitchen (stitchen besser bei statischen Inhalten)

\subsection{3D-Modelle}
3D Modelle können einen Blick auf Dinge gewähren, die so nicht so einfach angesehen werden. Beispielsweise können damit Molekülstrukturen oder menschliche Organe gezeigt und erforscht werden, wie sie wirklich aussehen. Da es bereits eine große Szene für 3D-Modelle gibt, ist es nicht schwierig für die verschiedensten Themengebiete passende Modelle zu finden. Auch für den Bildungsbereich gibt es dort einiges zu finden, was man im Unterricht verwenden kann.

\subsubsection{Quellen für 3D-Modellen}
3D-Modelle sind die einzige Art von Inhalten für 360°, für die es Webseiten gibt, über die man Modelle suche und herunterladen kann. Dabei kann man nach Output-Formaten, Polycount, Lizenzen und Preisen Filtern, sodass man für alle Anwendungsbereiche die passenden Modelle finden kann.

Anbieter gibt es dafür viele. Im Laufe der Entstehung dieser Arbeit sind unter Anderen folgende Webseiten genutzt worden:

\begin{itemize}
      \item Turbosquid.com
      \item cgtrader.com
      \item Sketchfab.com
      \item free3d.com
   \end{itemize}

Da für Computerspiele, neuere Animationsfilme und für 3D-Druck bereits seit Jahren viele 3D-Modelle benötigt werden, hat sich hier anders als bei den anderen Inhalten bereits eine Szene gebildet, in der Modelle erstellt und dann verkauft oder verschenkt werden.
   
\subsubsection{Erstellung und Bearbeitung von 3D-Modellen}
Das Erstellen von 3D-Modellen ist anders als bei den anderen Inhalten deutlich aufwändiger. Man muss sich dafür zuerst in eines der 3D-Modellierungsprogramme hineinarbeiten und auch das Erstellen der Modelle ist sehr zeitaufwändig. 

Es gibt viele verschiedene 3D-Modellierungsprogramme, die auch alle etwas unterschiedlich zu benutzen sind:
Beliebte Programme sind Autodesk 3ds Max, Blender, Cinema 4D, Paint 3D. Das einfachste dieser Programme ist wohl Paint3D, das allerdings auch den geringsten Funktionsumfang hat und nur auf Windows-Rechnern mit Windows 10 benutzbar ist.

Blender ist als gratis Open-Source-Software das wahrscheinlichsten verbreiteste Programm überhaupt, da es so den Nutzern die Möglichkeit bietet in die Welt der 3D-Modellierung hineinzuschnuppern.

Da man mit der Maus in all diesen Programmen sowohl den Viewport steuert, als auch Funktionen auslöst, ist es ratsam eine gute Mouse, am besten sogar eine 3D-Maus zu verwenden.

Da es so viele verschiedene Programme gibt, gibt es ebenso viele Output-Formate für die fertigen Modelle. Jede Software hat eigene Speicherformate, aber die meisten können auch in andere exportieren. Den Export zu OBJ-Dateien, unterstützen dabei alle Programme und ist damit eine gute Speicherform, um 3D-Modelle zu veröffentlichen.

\subsection{360° Sound/ Spatial Audio}
https://flypaper.soundfly.com/produce/wtf-vr-and-spatial-audio-mixing-360-audio/
https://www.youtube.com/watch?v=mcUwiYwgBHw
https://medium.com/@superavi/goodbye-stereo-hello-360º-sound-6a9d64fbd1f

https://facebook.github.io/react-360/docs/audio.html

https://developers.google.com/vr/ios/spatial-audio
Audioquellen im Raum platzieren (Mono-Soundquelle)
oder als ambisonic background sound (zB Regen/ Meeresrauschen etc)

https://creator.oculus.com/learn/spatial-audio/
Was ist spatial audio?
Content creation

https://sonicscoop.com/2018/02/05/audio-mixing-for-vr-the-beginners-guide-to-spatial-audio-3d-sound-and-ambisonics/

http://www.bbc.co.uk/guides/zrn66yc

https://www.vrtonung.de/spatial-audio-support-for-360-video-platform/

Paper: Def von spatial Audio

\subsubsection{Quellen für Spatial Audio-Dateien}


\subsubsection{Erstellung und Bearbeitung von Spatial Audio}
Hier muss wieder zwischen den verschiedenen Typen von Sounds Unterschieden werden, die spatial Audio erzeugen:

Werden in einer VR-Anwendung Geräusche nur an einem bestimmten Ort in der Szene platziert handelt es sich in der Regel um Mono-, manchmal Stereo-Audiodateien, diese können in allen Audio-Bearbeitungsprogrammen bearbeitet und mit allen Audiorecordern erstellt werden.

- Ambisonic
- Object-based spatial audio (Dolby Atmos)

\subsection{Virtual Reality Spiele und Anwendungen}
Die Kombination aus den vorigen Inhalten ergibt VR-Anwendungen und Spiele. Inzwischen ist die Auswahl an verschiedenen Anwendungen extrem gewachsen und tut das auch weiterhin. Allerdings ist der hier nach wie vor der allergrößte Anteil an VR-Programmen Spiele.

\subsubsection{Quellen für Virtual Reality Anwendungen}
Auf jeder Plattform von VR-Geräten gibt es eigene Stores, um Virtual Reality-Anwendungen für die jeweilige Plattform zu finden. 
Bei HTC Vive und HTC Focus ist das der SteamVR Store \cite{ValveCorporation}, bei den Oculus Geräten der Oculus Store \cite{FacebookTechnologiesLLC} und Anwendungen für die Playstation VR sind im Playstation Store zu finden. \cite{SonyInteractiveEntertainmentEuropeLimited}

Da WebVR plattformunabhängig ist, gibt es dafür keinen ``Standard''-Store auf dem Gerät, hier ist es relativ schwierig, Apps zu finden. Eine Möglichkeit, mit allerdings sehr begrenzter Auswahl ist auf itch.io nach ``WebVR'' zu filtern. \cite{Itchcorp}

Außerdem gibt es noch wearvr.com, einen unabhängigen Virtual Reality App Store, der Sammlungen für Anwendungen für alle Plattformen haben. \cite{WEARVR}


\subsubsection{Erstellung von Virtual Reality Anwendungen}
Um Virtual Reality Anwendungen zu erstellen, gibt es zwei grundlegend verschiedene Ansätze: 

Zum einen können die Anwendungen für die verschiedenen Plattformen oder als WebVR Apps entwickelt werden. Dazu muss die Person die jeweilige Programmiersprache beherrschen und einiges an Zeit investieren, kann aber dafür eine Anwendung entwickeln, bei der alles so ist, wie es die entwickelnde Person sich wünscht.

Eine spannende Möglichkeit VR-Applikationen oder -Games zu entwickeln ist die Nutzung von PlayCanvas. PlayCanvas ist eine Web-basierte Game-Engine, in der mit HTML und WebGL entwickelt werden kann. Das besondere ist, dass in Real-Time kollaborativ gearbeitet werden kann und da es keine Kompilierzeit gibt die Veränderungen auch direkt auf den Geräten angezeigt werden. PlayCanvas ist WebVR kompatibel und ist damit sehr gut geeignet VR-Anwendungen für Smartphone-gestützte VR-Headsets zu entwickeln. \cite{PlayCanvasLtd}

Weitere Web-basierte Möglichkeiten VR-Applikationen zu entwickeln sind A-Frame, React360 oder Three.js. Genauere Erläuterungen dazu finden sich in \ref{subsec:webvr}.

Alternativ können Anwendungen auch nativ entwickelt werden, zum Beispiel als Android-Anwendung, die dann in der Google Cardboard-App Nutzern zur Verfügung gestellt werden kann.

Außerdem besteht die Möglichkeit die Virtual Reality-Apps auf die klassische Weise mit einer Game Engine zu entwickeln. Das kann entweder mit Unity und C\# geschehen oder der Unreal Engine mit C++ und Blueprints. Diese Game Engines können dann den Code für die jeweilige Plattform kompilieren und man erhält native Apps.
Genauere Ausführungen dazu sind wiederum in \ref{subsec:unity} beziehungsweise \ref{subsec:unreal} zu finden.

\bigskip

Die zweite Möglichkeit ist die Nutzung von Tools wie zum Beispiel InstaVR oder Vizor360. Mit diesen Programmen können auch Personen, die keinerlei Programmierkenntnisse haben einfache 360°-Anwendungen erstellen. Dazu werden einfach 360°-Fotos oder -Videos hochgeladen, mit dem Tool zu einer Tour verbunden und schon ist eine einfach Anwendung fertig. So könnte zum Beispiel eine kleine Tour über den Campus einer Uni erstellt werden, sodass mögliche zukünftige Studierende besser erfahren können, wie der Campus aussieht. \cite{Vizor2019} \cite{InstaVRInc}
Außerdem können auch noch Textelemente eingefügt werden, um zum Beispiel auf einem Foto zu markieren, welche Gebäude zu sehen sind, um dem Betrachter noch etwas mehr Kontext zu geben.

\subsection{3D 360°}
1. Wie entsteht ein 3D-Bild generell?

2. Was ist das Problem von 3D in 360?

3. Wie macht man 3D 360 Bilder?



https://www.spiedigitallibrary.org/journals/Journal-of-Electronic-Imaging/volume-22/issue-3/030902/Acquisition-of-omnidirectional-stereoscopic-images-and-videos-of-dynamic-scenes/10.1117/1.JEI.22.3.030902.full?SSO=1

https://www.youtube.com/watch?v=0OKBjkhdnOc
Nett erklärt, weirder Typ

https://vuze.camera/camera/vuze-xr-camera/
180 in 3D oder 360 in 2D

https://www.aerofotografie.de/3d-360-grad-video-produktion/
Wie funktioniert das?
https://www.aerofotografie.de/360-grad-3d-kamera/
Welche Kameras gibt es?

https://patents.google.com/patent/US7463280B2/en

http://blog.dsky.co/capturing-virtual-worlds-a-method-for-taking-360-virtual-photos-and-videos/

\subsection{360°-Inhalte in React360}
Die VR-App innerhalb von VRClassroom wurde mit React360 entwickelt. Nachfolgend wird darauf eingegangen, wie 360°-Inhalte in React360 eingebunden werden können.

\subsubsection{Fotos und Videos}
React360 ermöglicht sowohl die Nutzung von 180°- also auch 360°-Panoramas für Videos und für Fotos. Ebenfalls für beide Medientypen gilt, dass React360 mono und stereo Signale verarbeiten kann.

Die Fotopanoramas in 360° müssen entweder in equirectangularer Projektion oder im 1x6 Cubemap-Format vorliegen. Dabei besteht das Gesamtbild aus sechs unverzerrten Einzelbildern, die in der Reihenfolge rechts, links, oben, unten, vorne, hinten angeordnet sind.
In der equirectangularen Projektion kann das Bild auch stereo sein. Dazu müssen die zwei Bilder übereinander angeordnet sein, diese Anordnung ist als ``top/bottom''-Layout bekannt. (Cubemap-Panoramas können nur als mono-Signal verarbeitet werden.)

180°-Panoramen sind nur equirectangular möglich. Das Stereobild kann hierbei neben dem ``top/bottom''-Layout auch in ``left/right'', also nebeneinander vorliegen.

Bei den möglichen Formaten für Videos verhält es sich ganz ähnlich wie bei den Fotos: equirectangulare Panoramas können mono oder stereo sein. Sind sie 360°-Panoramen im ``top/bottom''-Layout, sind sie 180° im ``left/right''-Layout. 360° Panoramen können wiederum auch im Cubemap-Format sein, bei Videos allerdings in 3x2.

/// Hier noch genauer auf 3x2 (+ 1\% expansion) eingehen??

Wenn ein Foto stereo eingebunden werden soll, muss der Parameter ``format'' auf einen der folgenden Werte gesetzt werden:

\begin{itemize}
\item \textbf{2D}: [Defaultwert] Für Monobilder
\item \textbf{3DTB}: Stereobilder, bei denen das linke Bild oben ist und das rechte unten
\item \textbf{3DBT}: Stereobilder, bei denen das linke Bild unten ist und das rechte oben
\item \textbf{3DLR}: Stereobilder, bei denen das linke Bild links ist und das rechte rechts
\end{itemize}

Die Fotos und Videos können entweder von der Runtime oder aus der React360-App selbst gesetzt und gesteuert werden. Um sie im Panorama sehen zu können müssen sie als Hintergrundbild beziehungsweise -video gesetzt werden.

Um Videos abzuspielen muss zuerst ein player erstellt werden und der Pfad zur Video-Datei übergeben werden. Danach können auf dem Player die Steuerungsfunktionen zu den Videos aufgerufen werden. \cite{React360video}

\subsubsection{Sounds}
In React360 gibt es zwei grundlegende Arten Sounds einzubinden:

Zum Einen können Audiodateien als Hintergrundaudio oder Soundeffekt eingebunden werden. Hier wird auf dem AudioModule die Funktion playEnvironmental() mit dem Pfad und der gewünschten Lautstärke übergeben. Die Audiodateien werden dann als Loop abgespielt und erst beendet, wenn stopEnvironmental() aufgerufen wird. Soundeffekte werde mit .playOnShot() einmalig abgespielt.

Zum Anderen können Sounds in der 360°-Szene an einem bestimmten Ort installiert werden. Dazu muss die .createAudio()-Funktion aufgerufen werden. Sie erwartet einen ``handle'', den Pfad zur Audiodatei und ``is3d'' muss auf true gesetzt werden. Im Aufruf der play-Funktion wird dann die gewünschte Location übergeben, an der der Sound abgespielt werden soll. \cite{React360audio}

\subsubsection{3D-Modelle}
3D-Modell - in React360 als ``Entities'' bezeichnet - müssen an eine Location gerendert werden. Während Panoramabilder und -videos von überall als Hintergrundbild beziehungsweise -video gesetzt werden können, muss der Component, der die Entities beinhaltet in der auf eine Location gerendert werden.

Die Location des beinhaltenden Components ist das der Nullpunkt für die Entities im Component, sie werden dann also relativ dazu gesetzt.

Um Modell zu rendern muss nur eine Entity-Komponente erstellt werden und er Link zu dieser Entity übergeben werden. Mit einem transform-Attribut können sie dann noch skaliert, rotiert und verschoben werden.

3D-Modelle können als gltf2-Dateien (Dateiendung .gltf oder .glb) oder als OBJ-Dateien vorliegen. Bei OBJ-Modellen muss auch noch die zugehörige MTL-Datei übergeben werden, die die Materialdefinitionen enthält. \cite{React360models}

%\_____________________________________________________________________

\cleardoublepage
\section{Software Entwicklung von VR-Systemen}
\subsection{Native VR-Applikationen}
Um VR-Anwendungen zu entwickeln, gibt es momentan nur zwei verschiedene Möglichkeiten. Das sind die Game-Engines Unity und Unreal. Wie aus dem Namen schon erkennbar, wurden sie entworfen, um Computerspiele zu erstellen, wurden aber weiterentwickelt, sodass sie sich inzwischen auch sehr gut zur Entwicklung von VR-Anwendungen eignen.

Ingesamt sind sich die beiden Engines relativ ähnlich, verwendet man Unity, muss man C\# programmieren und die Unreal Engine verwendet so gennante Blueprints, um einfachere Sachen visuell programmieren zu können mit einem Editor, in dem Nodes mit Links verbunden werden können um Funktionen zu nutzen. Darunterliegend wird in der Unreal Engine mit C++ gearbeitet.

\subsubsection{Unity}
\label{subsec:unity}
Unity: C\#
\subsubsection{Unreal Engine}
\label{subsec:unreal}
Unreal: C++
\subsection{WebVR + Mobile VR: VR im Browser}
\label{subsec:webvr}
Was ist WebVR?
https://webvr.info
(https://en.wikipedia.org/wiki/WebVR)
Request a list of the available VR devices.
Checks to see if the desired device supports the presentation modes the application needs.
If so, application advertises VR functionality to the user.
User performs an action that indicates they want to enter VR mode.
Request a VR session to present VR content with.
Begin a render loop that produces graphical frames to be displayed on the VR device.
Continue producing frames until the user indicates that they wish to exit VR mode.
End the VR session.

getPose liefert die Position, Ausrichtung, Bewegung und weitere Daten über den Nutzer
Web Audio API erlaubt es, dreidimensionale Klangeindrücke zu erzeugen

Browser-Unterstützung!
https://webvr.rocks

Was ist Mobile VR?
auf Smartphones und Standalone-Geräten
https://www.slashgear.com/mobile-vr-what-it-needs-to-succeed-08530006/

\subsubsection{A-Frame}
https://aframe.io/docs/0.8.0/introduction/

Web (Three.js) Framework für VR
ursprünglich von Mozilla entwickelt, jetzt open-source
basiert auf html
entity componetn framework
deklarative, erweiterbare und zusammensetzbare Struktur auf Basis von three.js

A-Frame nutzt den DOM, aber Updates von 3D-Objekten werden alle im Speicher mit kleinem Overhead mit einem einzigen requestAnimationFrame Call berechnet. Dadurch sind 90+ FPS Apps möglich

Kompatibel mit fast alles Libraries und Frameworks, auch: React, Preact, Vue.js, d3.js, Ember.js, jQuery

Hat einen integrierten visuellen 3D inspector (<ctrl> + <alt> + i, um in der Szene umzusehen)

Viele Core-Components: geometries, materials, lights, animations, models, raycasters, shadows, positional audio, text, and Vive / Touch / Windows Motion / Daydream / GearVR / Cardboard controls. 
Get even further with community components such as particle systems, physics, multiuser, oceans, mountains, speech recognition, motion capture, teleportation, super hands, and augmented reality

Unterstützt Headseats: Vive, Rift, Windows Mixed Reality, Daydream, GearVR, Cardboard und im Web. Auch AR

kann positional tracking und unterstützt Controller
\subsubsection{React360}
\subsubsection{Vergleich von A-Frame und React360}
Warum für React360 entschieden?
\subsubsection{Three.js}
\subsubsection{WebGL}
Sollte ich das vielleicht auch noch erwähnen?

playcanvas?

%\_____________________________________________________________________



\cleardoublepage
\section{Software Projekt: VRClassroom}
Aufbauend auf den Erkenntnissen aus ANDEREN KAPITELN wurde im Rahmen dieser Arbeit VRClassroom entwickelt. VRClassroom ist ein zweiteiliges System, das es Lehrkräften ermöglichen soll 360°-Fotos, -Videos und 3D-Modelle im Unterricht zeigen zu können, dabei die Anzeige auf allen Geräten steuern zu können und trotzdem den Überblick in der Klasse behalten zu können. 

VRClassroom bietet dafür die Lehrer-App, die auf einem Rechner läuft und die Schüler-App, die über den Browser der mobilen Geräte beziehungsweise VR-Headsets erreicht werden kann.

Die Lehrer-App ist eine Electron-App, die wiederum eine React-App für das Interface enthält und die Schüler-App startet. Die Schüler-App ist eine React360-Applikation. Der Code von VRClassroom ist also komplett in Javascript gehalten, nur unterschieden durch die verschiedenen Libraries Node.js, React.js, React360. Im Anschluss an diese Arbeit wird VRClassroom unter einer Creative Commons Lizenz veröffentlicht und so kann ein Entwickler schnell alle Teile des Codes verstehen ohne sich in mehrere Programmiersprachen einarbeiten zu müssen.

Grafik \ref{fig:electron}

\begin{figure}[ht]
  \centering
  \includegraphics[width=\textwidth]{images/electron.eps}
 \caption{Aufbau von Electron-Appliaktionen.}
  \label{fig:electron}
\end{figure}

\subsection{Nutzungsszenario und Anwendungsfokus}
VRClassroom wurde speziell für die Nutzung in der Schule entwickelt.
Dabei kann die Lehrkraft das VR-Erlebnis für die SchülerInnen führen und ihnen so komplexe 3-dimensionale Inhalte besser vermitteln und eine spannende neue Art des Lernens zu erleben. 
Außerdem wurde Funktionen wie etwa die Seitenleiste mit den verbundenen Geräten entwickelt, die es der Lehrkraft erleichtern sollen VRClassroom in einer Schulklasse einzusetzen und sicher zu stellen, dass alle Geräte verbunden sind und die Inhalte angezeigt werden.

Da viele Schulen noch überhaupt keine Ausrüstung an VR-Headsets haben, lag der Fokus besonders auf der Nutzung des VRClassroom Systems mit einem Smartphone in einem Google Cardboard. Allerdings wurde es bewusst so entwickelt, dass auch mit ``echten'' VR-Headsets das System problemlos weiter genutzt werden kann. So wird den Schulen ermöglicht mit einer sehr geringen Investition zu testen, ob es für sie in Frage kommt und können zu einem späteren Zeitpunkt zu einem elaborierteren Hardware-System wechseln ohne auf neue Software umsteigen zu müssen.

Weitere Einsatzszenarien für VRClassroom könnten Besprechungen im Arbeitsumfeld gehen, bei denen es sich um plastische Inhalte handelt wie zum Beispiel Architekturbüros, Designagenturen oder Landschaftsgärtner. Mit einem 360°-Foto oder -Video könnte der Ist-Zustand besprochen werden und anschließend die Entwürfe in 3D-Modellen vorgeführt werden. Dadurch könnten sich Kunden besser in die Entwürfe hineinversetzen und bewusster entscheiden, was sie letztendlich haben möchten.

\subsection{Anwender}
Wie bereits weiter oben beschrieben wurde VRClassroom speziell dafür entwickelt im Schulunterricht verwendet zu werden. Damit sind die Hauptanwendergruppe Lehrkräfte und Schülerinnen und Schüler.
Die Schülerinnen und Schüler als Anwender der Schüler-App können dabei im Alter von Grundschulkindern bis hin zu Erwachsenen reichen, die Spanne ist damit also sehr weit und somit ist auch die Vorerfahrung mit VR-Headsets und die individuelle Technikaffinität äußerst unterschiedlich.
Den Schülern soll es also möglichst leicht gemacht werden die Schüler-App von VRClassroom zu verwenden. Da fast keine Schulen über Mengen an VR-Headsets verfügen, die für eine gesamte Klasse ausreichen, muss also dafür Sorge getragen werden, dass die Schüler-App auf allen Geräten, aber besonders auf Smartphone-gestützten VR-Headsets läuft.

Bei den Lehrkräfte verhält es sich ganz ähnlich: Sie kommen aus verschiedensten Fachrichtungen, mit unterschiedlichsten Leveln an Erfahrung und Technikaffinität, außerdem hat die Umfrage gezeigt, dass mehr als die Hälfte teilnehmenden Lehrkräfte selbst noch nie ein VR-Headset ausprobiert haben. 

Dementsprechend lag der Fokus bei der Entwicklung besonders darauf, dass die Bedienung intuitiv und einfach ist. Außerdem soll den Lehrkräften mit der Anzeige der verbundenen Geräte mit dem Aktivitätsindikator eine Möglichkeit an die Hand gegeben werden, zu sehen, ob die Schüler-Geräte verbunden sind und alle Inhalte bekommen und anzeigen. Bewusst wurde das System so entworfen, dass die Lehrkräfte keine VR-Brille tragen, damit sie einen Überblick im Klassenzimmer behalten können und trotzdem sehen können, was die Schüler angezeigt bekommen.

\subsection{Grundstruktur}
Das VRClassroom System besteht aus zwei verschiedenen Applikationen: Zum einem der Schüler-App, die auf den VR-Systemen läuft mit denen die Schüler sehen können, was der Lehrer ihnen präsentiert, und die Lehrer-App, in der die Lehrkraft die verschiedenen Inhalte hineinladen, Markierungen auf die Inhalte setzen kann und und sehen kann, welche Schüler-Geräte aktuell verbunden sind.

Da viele Lehrer in ihrer Ausbildung oft nicht mit vielen neuen Technologien in Berührung gekommen sind, sondern es gewohnt sind mit den ``klassischen'' Medien zu arbeiten, lag der Augenmerk bei der Entwicklung darauf, dass das benötigte technische Verständnis möglichst niedrig ist und auch Personen, die sich selbst als nicht technikaffin bezeichnen würden, keinerlei Probleme bei der Nutzung haben. Gleiches gilt selbstverständlich auch auf der Seite der Schüler. Da diese aber hauptsächlich passiv agieren, standen hier die Lehrkräfte im Mittelpunkt der Aufmerksamkeit.

Grafik \ref{fig:architecture}

\begin{figure}[ht]
  \includegraphics[width=\textwidth]{images/architecture1.eps}
 \caption{Aufbau des VRClassroom Systems.}
  \label{fig:architecture}
\end{figure}

\subsection{Lehrer-Applikation}
Die Lehrer-Applikation besteht aus einer Electron-App, die in zwei logische Teile zerlegt ist. Das ist zum einen der main-Prozess, der es erlaubt Zugriffe auf das File-System des Rechners zu machen und für rechenaufwändige Hintergrundprozesse genutzt wird, und zum Anderen der render-Prozess. Der render-Prozess ist der Teil des Programms, das die Lehrkraft letztendlich auf ihrem Bildschirm sieht.

Die Lehrer-App enthält genau genommen zwei render-Prozesse: Die teacher-App, in der die Lehrkraft alle verbundenen Geräte sehen kann und verschiedene Inhalte hineinladen kann, und die student-App, die als iFrame in die teacher-App eingebunden ist und auf den Geräten der Schülern läuft.

Die teacher-App enthält außerdem noch den QR-Code Generator, der in einem zweiten Fenster geladen wird.

Um eine Liste der verbundenen Geräte zu halten und Veränderungen der Inhalte auf die Schüler-Geräte zu synchronisieren, startet die teacher-App einen Websocket-Server, mit dem sich alle Schüler-Geräte verbinden. 

\subsubsection{Verbundene Geräte}
Wie in der Grafik zu sehen hält die teacher-App eine Liste mit allen verbundenen Geräte dieser Session. Sind die Geräte gerade aktiv, werden sie mit einem grünen Icon dargestellt, sind sie inaktiv, mit einem Roten.

Das soll der Lehrkraft erleichtern zu überprüfen, ob die Schüler den gezeigten Stoff verfolgen oder sich anderweitig beschäftigen.

Haben die Schüler bereits einen Namen eingegeben, wird dieser in der Liste angezeigt. Ist dies nicht der Fall wird aus dem user-agent versucht möglichst genau zu schließen, um welches Gerät es sich handelt, sodass der Lehrer zumindest einschränken kann, um welchen Schüler bzw welche Schülerin es sich handeln könnte. 

NEU: Bei Modellen und Videos der loading-Status der einzelnen Geräte

\bigskip

Ein User-Agent kann zum Beispiel wie folgt aussehen: 
\begin{framed}
Mozilla/5.0 (iPhone; CPU iPhone OS 5\_0 like Mac OS X) AppleWebKit/534.46 (KHTML, like Gecko) Version/5.1 Mobile/9A334 Safari/7534.48.3
\end{framed}

Daraus lässt sich schließen, dass es sich um eine iPhone handelt, welches Safari benutzt, um die Schüler-App zu laden. Die Oculus Geräte hingegen geben in ihrem User-Agent an, den Oculus Browser zu verwenden und sind so auch gut von den anderen verbundenen Geräten zu unterscheiden. Da aber hier nicht ersichtlich wird welches Oculus-Gerät es genau ist wird nur ``Oculus device'' angegeben und nicht genauer spezifiziert, ob es sich dabei um eine Go, Quest oder Rift handelt.
 %hier Grafik connected clients einfügen% 

\subsubsection{Hineinladen von Inhalten}
Um Inhalte in VRClassroom hineinzuladen kann mit ``STRG'' + ``o'' beziehungsweise Cmd+o oder in der Menüleiste über ``Datei'' und ``Öffnen...'' der FileBrowser geöffnet werden, in dem dann die gewünschte Datei ausgewählt wird. Ist eine Datei ausgewählt wird sie sofort in die App geladen und auf allen verbundenen Geräten angezeigt.
Wurde die VRClassroom App zum ersten Mal installiert wird ein Ordner ``.vrclassroom'' im Dateisystem angelegt, in den die verwendeten Dateien hineingeladen werden. Von dort aus werden sie dann auch für den Zugriff durch die verbundenen Schüler-Geräte freigegeben.

Bereits früher verwendete Dateien sind unter ``Datei'' und ``Zuletzt geöffnet'' zu finden und können so bequem wieder verwendet werden ohne sie lange im Dateisystem suchen zu müssen. Neu geöffnete Dateien werden beim Öffnen in den ``.vrclassroom''-Ordner im Dateisystem geladen und sind ab diesem Moment auch in der Liste der bereits geöffneten Dateien zu finden.

Drittens können auch noch die URLs von Google Streeview-Panoramen eingegeben werden, sodass dann allen verbundenen Geräten das verlinke Panorama angezeigt wird. Dazu muss ``Datei'' und ``StreetView...'' ausgewählt werden und in das Pop-Up die URL zum gewünschten Panorama eingegeben werden. Handelt es sich um eine korrekte URL wird dann das StreetView Bild geladen und angezeigt wie ein normales 360°-Foto. Für die Nutzer der Schüler-App ist kein Unterschied zu normalen 360°-Fotos erkennbar.
 
\subsubsection{Controls}
Wie in Grafik \ref{fig:controls} zu sehen sind die Controls, mit denen Aktionen des aktuell geladenen Inhalts ausgelöst werden können durch einen Overlay über dem iFrame der React360-App dargestellt. 

Für jeden der drei Typen an Inhalten, 360°-Fotos, -Videos und 3D-Modelle unterscheiden sich die Bedienelemente und Funktionen, das Setzen einer Markierung ist allerdings bei allen Medientypen gegeben.

\begin{figure}
  \includegraphics[width=0.9\linewidth]{images/controls-overlay.png}
  \caption{Die Kontrollleiste als Overlay über den iFrame der React360-App.}
  \label{fig:controls}
\end{figure}

\paragraph{Marker}
\label{subsec:marker}

\begin{SCfigure}
  \centering
  \caption{Die leuchtende Farbe und das Drehen der Marker erhöht die Erkennbarkeit.}
  		\includegraphics[width=0.4\textwidth]{images/marker.png}
	\label{fig:marker}
\end{SCfigure}

Soll ein Marker gesetzt werden, muss zuerst in den ``Markierung setzen''-Modus gewechselt werden, indem in der Control Bar auf ``Marker setzen'' geklickt wird. Danach kann an beliebiger Stelle eine Markierung gesetzt werden.

Um die 3D-Koordinaten des Markers zu bekommen wird mit Hilfe von Mouseposition und Fenstergröße ein Strahl berechnet, der aus der 2D-Koordinate der Mouseposition ausgeht. 
Bei 360°-Fotos und -Videos wird die Entfernung zur Kamera auf einen festen Wert gesetzt, der so gewählt wurde, dass er gerade noch im Zylinder der Welt liegt. Mit der Entfernung zur Kamera lässt sich die 3D-Koordinate dann leicht berechnen.

Um Markierungen auf 3D-Modellen setzen zu können muss ein Schnittpunkt des 3D-Modells mit dem virtuellen Strahl berechnet werden. 
(Soll das wirklich rein?) In React360 gibt es momentan keine Möglichkeit auf diese Weise auf die in der 3D-Welt platzierten 3D-Modelle zuzugreifen, sodass

Ist der Schnittpunkt des Strahls mit dem 3D-Modell berechnet, wird derjenige Schnittpunkt als Koordinate verwendet, der am nächsten zur Kamera ist. An dieser Stelle wird dann der Marker platziert. Zudem werden die Marker invers zur Nähe zur Kameraposition skaliert, sodass sie immer in der gleichen Größe dargestellt werden, egal wo am Modell sie gesetzt sind.

Das verwendete Modell erinnert wie in Grafik \ref{fig:marker} zu sehen an Stecknadeln, sodass Nutzern die Bedeutung direkt verständlich ist. Um die Markierungen leichter erkennbar zu machen sind sie in einer leuchtenden Farbe und drehen sich, sodass die Aufmerksamkeit direkt auf die markierte Stelle gelenkt wird.



\paragraph{Photo Controls}
Ist ein Foto geladen beinhaltet die Kontrollzeile nur den Dateinamen des 360°-Fotos und die Buttons zum Setzen von Markierungen, die wie bereits beschrieben bei allen Medientypen gegeben sind. Grafik \ref{fig:photocontrols} zeigt die Kontrollleiste für 360°-Fotos.

\begin{figure}[h]
  \includegraphics[width=\linewidth]{images/photocontrols.png}
  \caption{Die Kontrolleiste für 360°-Photos.}
  \label{fig:photocontrols}
\end{figure}


\paragraph{Video Controls}
Wird ein Video gezeigt, hat die Lehrkraft mehrere unterschiedliche Funktionen in der Kontrollleiste: 
Wie in Grafik \ref{fig:videocontrols} zu sehen sind die Elemente ähnlich wie bei bekannten Videoplayern wie Quicktime oder Netflix gehalten. Ganz links ein kombinierter Play/Pause-Button, der für alle verbundenen Geräte synchronisiert das Abspielen beziehungsweise Pausieren des Videos auslöst, daneben die aktuelle Abspielzeit, gefolgt von einem Slider, der grafisch die aktuelle Position im Video darstellt. Außerdem kann mit dem Slider zu anderen Zeitpunkten im Video gesprungen werden.
Rechts daneben wird die Gesamtdauer des Videos angezeigt. Der letzte Video-spezifische Button ist der Sound-Button, der im aktiven Zustand auf alle Geräten den Ton des Videos abspielt. Der Sound-Button ist standardmäßig deaktiviert. 

Zudem sind wiederum die Buttons für das Setzen von Markierungen vorhanden. Sie sind während dem Abspielen des Videos deaktiviert und können nur genutzt werden, wenn das Video pausiert ist. Wird das Video dann wieder weiter abgespielt, werden alle gesetzten Markierungen automatisch zurückgesetzt.

\begin{figure}[h]
  \includegraphics[width=\linewidth]{images/videocontrols.png}
  \caption{Die Kontrolleiste für 360°-Videos.}
  \label{fig:videocontrols}
\end{figure}

\paragraph{Model Controls}
Wie in Grafik \ref{fig:modelcontrols} zu sehen ist auch in der Kontrollleiste bei 3D-Modellen ein Slider vorhanden. Dieser kann sowohl zum Drehen des Modells als auch zum Skalieren benutzt werden. Dazu sind links vom Slider die Buttons ``Drehen'' und ``Skalieren'' mit denen zwischen den zwei Funktionalitäten des Sliders gewechselt werden kann. Der aktive Modus wird durch die blaue Farbe dargestellt.

Auch auf 3D-Modellen können Markierungen gesetzt werden. Dabei ist zu beachten, dass nur auf dem Modell eine Markierung gesetzt werden kann. Wird außerhalb des 3D-Modells geklickt, wird keine Markierung gesetzt.

\begin{figure}[h]
  \includegraphics[width=\linewidth]{images/modelcontrols.png}
  \caption{Die Kontrolleiste für 3D-Modelle.}
  \label{fig:modelcontrols}
\end{figure}
 
\subsection{Kommunikation zwischen Lehrer-App und Schüler-App}
Wie in Grafik XXX zu erkenne kommunizieren die Lehrer-App und die Schüler-App über eine Websocket-Verbindung. Beim Start der VRClassroom-App auf dem Computer der Lehrkraft werden sowohl die Lehrer-App, die Schüler-App und ein Asset-Server gestartet. Über den Asset-Server sind die Inhalte erreichbar, die die Lehrer-App an die Schüler-App schickt.

Bei jeder inhaltlichen Veränderung in der Lehrer-App wird eine Nachricht an alle verbundenen Geräte geschickt, sodass diese dann die Anzeige aktualisieren können.
Damit alle Geräte zu jedem Zeitpunkt über die gleichen Informationen verfügen und auch Geräte, die nach Beginn der Session hinzustoßen alle Information zur Anzeige haben, ist jede Nachricht in der Form wie in YYY zu sehen gestaltet.

Eine Nachricht enthält immer die Felder ``mediatype'', ``url'', ``markers'',``playing'',``playbackPosition'',``rotation'',``scaleFactor'' und``muted''.
Das Feld ``mediatype'' kann die Werte ``photo'',``video'' und``model'' enthalten und ist dafür zuständig, dass die Schüler-App weiß, welcher Medientyp angezeigt wird und damit auch, welche Controls angezeigt werde sollen. 
Im URL-Feld enthält die URL des aktuell anzuzeigenden Inhalts. Dieses Feld wird nur verändert, wenn ein neuer Medientyp hineingeladen wird. Geschieht dies wird auch das Array der ??marker''-Positionen zurückgesetzt. Das Array enthält die Positionen der Markierungen in Form von Tripeln innerhalb des Arrays.

Die Felder ``playing'',``playbackPosition'' und ``muted'' werden nur beim Anzeigen von 360°-Videos verwendet. ``playing'' gibt an, ob das betreffende Video in diesem Moment abgespielt oder pausiert werden soll, ``playbackPosition'' gibt die Stelle im Video an, an der es abgespielt werden soll. Werden Videos abgespielt schickt die Lehrer-App jede Sekunde eine Nachricht mit der aktuellen Abspielposition an alle Geräte und diejenigen, bei denen die Abspielposition um mehr als eine Zeit von PPPPP versetzt ist, springen dann an die mitgeschickte Position, um von dort weiter abzuspielen.
Das ``muted''-Feld gibt an, ob der Ton des Videos an den Geräten abgespielt werden soll oder nicht.

Nachrichten von Seite der Schüler-Geräte sind viel seltener als anders herum. Gibt der Schüler seinen Namen ein wird dieser als Nachricht an die Lehrer-App geschickt. Außerdem senden die Geräte eine Nachricht, wenn sich der visibility-Status ändern, also zum Beispiel der Browser geschlossen wird. Außerdem geben die Schüler-Geräte Rückmeldung an die Lehrer-App, solange sie Inhalte laden und wiederum, wenn sie die Inhalte fertig geladen beziehungsweise Videos genug vorgeladen haben. Das wird dann mit dem orangenen Status-Indikator neben dem Geräte-Namen in der Lehrer-App angezeigt. Wechselt hier die Farbe wieder auf grün, ist das Gerät bereit die Inhalte anzuzeigen.

\subsection{QR-Code Fenster}
Das QR-Code Fenster ist ein zweites Browserfenster, das aus dem main-Prozess der Electron App auf dem Lehrer-Computer gestartet wird. Es zeigt, wie in \ref{fig:qrcode} zu sehen, einen QR-Code, den die SchülerInnen mit ihren Smartphones scannen können, um bequem die URL zu laden, auf der sie die Schüler-Applikation erreichen können. Der QR-Code wird dynamisch beim Öffnen der App generiert.

Für Geräte, die keine Kamera haben oder wenn das Scannen des QR-Codes fehlschlägt, wird zudem unterhalb des QR-Codes die URL angezeigt, unter der die Schüler-Applikation zu erreichen ist. 

Der QR-Code wird in diesem extra Fenster generiert und angezeigt, damit die Lehrkraft dieses Fenster auf einem Beamer anzeigen kann, um den SchülerInnen den Zugang zur Schüler-App ohne umständliches URL-Abtippen zu ermöglichen.

\begin{figure}
  \includegraphics[width=0.8\linewidth]{images/QR-Code.png}
  \caption{Qr-Code Fenster mit Link zur Schüler-App.}
  \label{fig:qrcode}
\end{figure}

\subsection{Schüler-App}
In der VRClassroom App nehmen die Schüler eine passive Rolle ein und können selbst nicht in der 3D-Welt navigieren. 

Die WebApp für die Schüler ist eine React360-App, sie stellt eine 3D-Welt da, in deren Mitte sich die Kamera, also der Viewport der Geräte, befindet. Sie stellt die in der Lehrer-App hineingeladenen 360°-Fotos und Videos in einer Kugel um den Viewport da, sodass die Nutzer sich in alle Richtungen umsehen können. 3D-Modelle werden in kurzer Entfernung vor dem Viewport angezeigt, als befänden sich sich im Raum vor der Person.

Grafik \ref{fig:react360}

\begin{figure}[ht]
  \includegraphics[width=\textwidth]{images/react360.eps}
 \caption{Struktur der React360 App von VRClassroom.}
  \label{fig:react360}
\end{figure}

%% Screenshot VRClassroom App?

\subsubsection{Eingabe des Namens}
Läd ein Gerät zum ersten Mal die VRClassroom-App wird dem Nutzer neben der Begrüßung, die immer angezeigt wird bevor Inhalte hineingeladen werden, eine Tastatur im VR-Raum angezeigt. Damit können die Schüler ihren Namen eingeben, der dann in der Liste der verbundenen Geräte angezeigt wird. Wird ein Smartphone als VR-Headset genutzt, muss die Tastatur mittels der Toucheingabe auf dem Display gemacht werden, bevor die WebVR Ansicht geladen wird. VR-Headsets, die einen Controller haben, können den Namen mit dem Controller auf der Tastatur tippen. 

Damit die Schüler nicht jedes Mal wieder ihren Namen eingeben müssen, wird der eingegebene Name im Browser des Schüler-Gerätes gespeichert und beim erneuten Laden der VRClassroom Applikation direkt wieder an die Lehrer-App übermittelt.

%% Screenshot VRClassroom App mit Tastatur + Greeting

\subsubsection{Anzeigen von 360°-Fotos}
Um 360°-Fotos in der 3D-Szene anzuzeigen lädt die React360-App das Foto von dem Link, den die Teacher-App geschickt hat und setzt sobald das gesamte Foto geladen wurde das Bild als Hintergrund der Szene. Gleichzeitig wird bei erfolgreichem Ladens des Fotos eine Nachricht an die Teacher-App geschickt, um dem Status des Geräts von ``loading'' wieder auf ``active'' zu setzen.
Das 360°-Bild wird dazu von innen an die Kugel in der die React360-Szene ist projiziert und ergibt so durch seine equirectangulare Projektion ein unverzerrtes 360°-Bild.

Das Bild wird angezeigt bis VRClassroom von der Lehrkraft beendet wird oder etwas anderes in die App hineingeladen wird.

\subsubsection{Abspielen von 360°-Videos}
Sendet die Teacher-App die Nachricht, dass ein Video angezeigt werden soll, startet die React360-App sofort mit dem Laden des Videos und lädt soviel von dem Video wie es geht. 
Ist eine Zeit des Videos vorgeladen, sodass React360 davon ausgeht, dass das Video ruckelfrei abgespielt werden kann, sendet wird wiederum die Nachricht, dass das Gerät aus dem ``loading''-Status wieder auf ``active'' gesetzt wird.
Ähnlich wie bei 360°-Fotos wird das 360°-Video dann als Hintergrund-Video der Szene gesetzt, um als 360°-Video abgespielt werden zu können

Die React360-App spielt das Video nicht automatisch ab, sobald genug geladen ist, sondern wartet auf das Signal der Teacher-App, um das Abspielen zu starten. Alle Funktionen werden erst ausgeführt wenn das Signal der Teacher-App kommt, die Funktion auszuführen. Zu den Funktionen zählen: Play, Pause, Springen zu einer anderen Stelle im Video und den Ton des Videos abspielen.

Falls ein Schüler-Gerät sich verspätet verbindet oder ein Paket verloren gegangen ist, sendet die Teacher-App jede Sekunde eine neue Nachricht in der immer der Link zum aktuellen Video und die aktuelle Abspielposition enthalten ist. Unterscheidet sie sich mehr als eine Sekunde von der Abspielposition in der React360-App wird zu der Abspielposition, die in der Nachricht steht gesprungen.

\subsubsection{Abspielen von Ton in Videos}
Da die meisten Browser das automatische Abspielen von Videos mit Ton verbieten, werden 360°-Videos von React360 standardmäßig beim erstellen des Videoplayer-Komponenten ``muted'' auf ``true'' gesetzt. Die Hersteller wollen dadurch die Ablenkungen, die beim surfen auf den Nutzer zukommen, abmildern. Erst wenn der Nutzer ein ``user gesture click'' also einen Klick auf der Website gemacht hat, darf die Tonspur automatisiert abgespielt werden. Wird versucht ein Video mit Ton abzuspielen, ohne dass ein Klick gemacht wurde, wird der Videoplayer blockiert und kann nicht mehr abspielen. \cite{Decker2017}

Damit React360 in einer single-threaded Umgebung wie einem Webbrowser flüssig ablaufen kann und nicht durch ``blocking behavior'' irgendeiner Art das Rendern unterbrochen wird, ist eine React360-App in zwei Teile aufgeteilt: Die React-Applikation und den Code, der die React Komponenten in 3D Elemente auf dem Bildschirm umwandelt. Die App selbst läuft in einem Webworker, einem anderen Prozess als der des Hauptbrowserfensters. \cite{FacebookInc.2018}

Das führt dazu, das die React360-Elemente nicht als html-Elemente gelten und ein Click-Event auf einem VRButton nicht als Interaktion zählt, um die Erlaubnis zu haben Ton abzuspielen.

Um dieses Hindernis zu umgehen ist nun ein durchsichtiger, Bildschirm-füllender Button über die React360-App gelegt, der bei einem Klick verschwindet, um die Erlaubnis vom Browser zu bekommen Ton abzuspielen. Wurde der Button am Schüler-Gerät geklickt wird ein flag gesetzt, dass die App Ton abspielen darf. Ob dann tatsächlich Ton beim Video abgespielt wird, kann der Lehrer aus der teacher-App einstellen. Auch dort ist das Abspielen von Ton an den Schüler-Geräten standardmäßig erst einmal abgestellt, damit der Lehrer entscheiden kann, ob er nur den Ton aus dem Rechner über Boxen für Alle abspielen möchte oder aus jedem Schüler-Gerät einzeln der Ton kommen soll.

\subsubsection{Anzeigen von 3D-Modellen}
Wird die Nachricht empfangen, dass der ``mediatype'' auf ``model'' gesetzt wurde, wird als Hintergrund der Standardhintergrund gesetzt, wie er auch angezeigt wird, bevor Medien von der Lehrkraft in die App geladen wurden. Das Laden und Darstellen der 3D-Modelle passiert in der ModelView. Hier wird zuerst analysiert, ob es sich um eine gltf-Datei oder eine .obj-Datei handelt. Valide gltf-Dateien haben die Endung .gltf oder .glb, ein Container-Dateiformat, das alle Texturen und Styling-Dateien enthält. Handelt es sich um eine .obj-Datei wird versucht eine gleichnamige .mtl-Datei zu laden. Diese enhält alle Informationen über Texturen und Materialien, die .obj-Datei ist allein das Modell.

Im Anschluss wird die Datei geladen und angezeigt. Damit die Modelle zu erkennen sind und nicht schwarz gerendert werden wird zudem eine Punktlichtquelle installiert, die das Modell von oben rechts beleuchtet. Außerdem wird das Modell mit einem leichten Ambientlight versehen, damit auch Teile des Modells, die sonst im Schatten liegen, erkennbar werden.

Auf die ähnliche Weise werden die Marker gerendert, die von der Lehrkraft gesetzt werden können, wie bereits in ref{subsec:marker} beschrieben. Die Marker haben keine Punktlichtquelle, sondern nur ein Ambientlight. Sie werden zudem nicht an eine feste Position, sondern an die von der Lehrkraft ausgewählt Stelle in der VR-Szene gerendert und ihre Größe wird zudem in Abhängigkeit ihrer Entfernung zur Kamera skaliert.

\subsubsection{WebVR Polyfill}
WebVR ist eine Javascript-API, um Virtual Reality-Inhalte im Browser anzuzeigen. \cite{WebVR} 

Wie in Grafik \ref{fig:WebVRfig}a links zu sehen wird auf einer Web-App, die mit WebVR angesehen werden kann ein Icon in der unteren rechten Ecke des Fensters zu angezeigt, mit dem dann die WebVR-Ansicht geladen werden kann.

Wie Grafik \ref{fig:WebVRfig}b zeigt teilt WebVR dafür den Bildschirm in zwei Bilder für die Linsen in einem VR-Headset wie zum Beispiel dem Google Cardboard auf. Die zwei Bilder sind dabei nicht Bildschirm-füllend, sondern in einer annähernd ovalen Form, die von einem schwarzen Rand umgeben wird, sodass sie gut auf die Linsen passen. Durch die Krümmung der Linsen wird daraus dann eine drei dimensionale 360°-Welt, in der der Nutzer sich umsehen kann.

\begin{figure}[h!]
  \centering
  \begin{subfigure}[b]{0.4\linewidth}
    \includegraphics[width=\linewidth]{images/WebVR1.png}
    \subcaption{``View in VR'' Button in VRClassroom.}
  \end{subfigure}
  \begin{subfigure}[b]{0.4\linewidth}
    \includegraphics[width=\linewidth]{images/WebVR2.png}
    \subcaption{VR-Szene mit WebVR Ansicht.}
  \end{subfigure}
  \caption{VR-Szene mit und ohne WebVR Ansicht.}
  \label{fig:WebVRfig}
\end{figure}

Bisher ist weder in Chrome noch in Safari WebVR standardmäßig unterstützt. In Chrome auf Android kann es durch setzen der ``WebVR''- und ``Gamepad-Extension''-flags auf ``enabled'' eingeschaltet werden.  \cite{Jones2017}
In Safari ist es bisher gar nicht möglich WebVR zu nutzen.

Da aber VRClassroom auf allen mobilen Geräten funktionieren soll und möglichst auch keine Einstellungen auf der Nutzerseite benötigen soll, wurde ein Polyfill benutzt, um die WebVR Funktionen für die VRClassroom-App auf allen Geräten nutzen zu können. 
Dafür 

%______________________________________________________________________
\cleardoublepage

\section{Nutzerstudie und Evaluation}

\subsection{Online-Umfrage}
Um die Meinung möglichst vieler verschiedener Lehrkräfte zum entwickelten VRClassroom System zu erfahren, wurde neben der qualitativen Studie, in der die Studienteilnehmer das System ausprobieren und im Anschluss Feedback geben konnten, noch eine Online-Befragung von Lehrkräften in Bayern durchgeführt. 

Dafür wurde ein Video produziert, welches die Idee und Nutzung von VRClassroom erklärt und die einzelnen Funktionen zeigt, die die App anbietet. Im Anschluss haben die teilnehmenden Lehrkräfte einen kurzen Fragebogen ausgefüllt.

Alle Fragen waren optional gestellt, konnten also wenn gewünscht unbeantwortet bleiben, sodass davon ausgegangen werde kann, dass die gegebenen Antworten alle der Wahrheit entsprechen und nicht einfach eine der möglichen Antworten gewählt wurde, weil eine nötig war. 
Die Fragen wurden als Aussagen formuliert, zu denen mit Hilfe einer unzentrierten Lickert-Skala geantwortet werden konnte. Es wurde eine unzentrierte Skala verwendet um zu aussagekräftigeren Antworten zu ermuntern.

\subsubsection{Net Promotor Score}
Der Net Promotor Score, kurz NPS, ist bei Tech-Konzernen weit verbreitet und versucht darzustellen wie zufrieden und damit loyal die Nutzer eines Produkts sind.

Dabei sollen die Befragten auf einer Skala von 0 bis 10 angeben, wie wahrscheinlich sie das Produkt an einen Kollegen weiterempfehlen würden.
0 steht dabei für sehr unwahrscheinlich und 10 steht für extrem wahrscheinlich.
Aus den Zahlenwerten kann dann ein Wert berechnen, der die Loyalität der Kunden wiederspiegeln soll.

Außerdem werden die Werte in drei Gruppen eingeteilt. Personen, die mit einem Wert von 0 bis 5 angeben gelten als Detraktoren, also Personen, die ihren Kollegen davon abraten würden das Produkt zu verwenden. Bei Werten von 6-8 gelten die Personen als Indifferente, die das Produkt okay finden, allerdings auch nicht positiv Anderen gegenüber davon berichten würden. Die dritte Gruppe sind die Promotoren, die auf die Frage mit 9 oder 10 geantwortet haben. Promotoren werden ihren Kollegen positiv von ihren Erfahrungen mit dem Produkt berichten und dadurch dafür sorgen, dass das Produkt neue Kunden gewinnt und bekannter wird.

(Zahlen nochmal nachschauen)

% Quelle!

In der Studie zeigte sich, dass 56\% der Teilnehmer Detraktoren wären, also die Software ihren Kollegen nicht empfehlen würden, wäre VRClassroom ein marktreifes Produkt. 28\% sind indifferent und nur 16\% sind von VRClassroom überzeugt und würden auch ihre Kollegen ermuntern, die Software auszuprobieren. Grafik \ref{fig:survey6} veranschaulicht den NPS der Online-Evaluation von VRClassroom.

Kritikpunkte an VRClassroom:
- technische Ausstattung dafür nicht vorhanden: Schüler WLAN, Smartphones bei Schülerlnnnen bzw VR Headsets
- fehlende Inhalte/ sehen keine für das eigene Unterrichtsfach
- Kosten
- bei Grundschulen: viele Kinder haben keine Smartphones
- Brillenträger haben Probleme


Gut an VRClassroom:
- Aufmerksamkeit wird auf Inhalt fokussiert, keine Ablenkung
- mit Cardboards extrem günstig zur Anschaffung
- Motivation der Schüler
- gute Veranschaulichung; auch von Dingen, die anders schwierig darstellbar sind
- räumliche Darstellung räumlicher Dinge
- einfache Handhabung
- virtuelles Erkunden von Orten
- passt zu Realität der Schüler: aktuelle digitale Medien
- Kompatibilität mit allen VR-Plattformen


weitere Funktionen:
- Texte und Links in Markern hinterlegen
- einfache Skalierung der Modelle in Originalgröße
- Online Funktion: bräuchte dann aber auch Stream mit Audio der Lehrkraft

- Kontrollfunktion, ob Schüler Inhalte sehen: Gibt es eigentlich?
- Offline-Funktion: Ist so existent bis auf Streetview

gewünschte Inhalte:
- Mathematik: Vektoren, Ebenen und Schnitte von Körpern
- Kernkraftwerk und andere Einrichtungen, die man nicht besuchen kann im Rahmen einer Exkursion
- Menge an Modellen, auch mit didaktischer Erläuterung
- Körper, Organe, biologische Prozesse
- Moleküle, chemische Reaktionen auf Teilchenebene
- NuT
- Kunst: Architketur

das müsste sich ändern:
- technische Ausstatuung und betreuung an den Schulen
- Inhalte müssen existieren
- Schulungen für Lehrer
- Infrastrktur

\subsubsection{Demografie und persönliche Angaben}
Am Ende der Befragung wurden noch einige personenbezogene Fragen gestellt.
Es wurde nach dem Geschlecht, dem Alter (innerhalb von Altersgruppen) und der Schulart, an der die Person unterrichtet gefragt.
Zudem wurden die Teilnehmer gefragt, ob sie sich selbst als technikaffin bezeichnen würden und ob sie schon einmal eine VR-Brille benutzt haben.
Als Folgefrage sollte dann noch angegeben werden, welche VR-Brille schon einmal ausprobiert wurde. 
Insgesamt war die Teilnehmermenge in allen Aspekten sehr ausgewogen: Beim Geschlecht, dem Alter und den Schule, an denen die Personen unterrichten, waren die Teilnehmer sehr divers verteilt, sodass davon auszugehen ist, dass die Antworten der Studie die natürliche Verteilung der Gesellschaft gut abbilden.

Insgesamt haben an der Befragung 57 Lehrkräfte teilgenommen, wobei das Geschlechterverhältnis fast ausgewogen war.
48\% der teilnehmenden Lehrkräfte gaben an weiblich zu sein und 52\% männlich. 4 Personen haben die Frage nicht beantwortet. Die Altersverteilung war ebenfalls sehr ausgewogen, es nahmen aus allen Altersgruppen von unter 30 bis über 60 Lehrer an der Studie teil.

Die überwiegende Mehrheit mit 91 bezeichnet sich selbst als technikaffin, wie Grafik \ref{fig:survey9} zeigt. Allerdings haben nur 43\% der Teilnehmer schon einmal eine VR-Brille ausprobiert. Darin liegt vermutlich auch einer der Hauptgründe für das Misstrauen der Teilnehmer daran, dass Virtual Reality zukünftig eine große Rolle im Unterricht spielen wird.

An den Antworten auf die Frage der Schule lässt sich ableiten, dass auch hier aus vielen verschiedenen Schultypen Lehrer geantwortet haben. Es waren damit Lehrkräfte für alle Altersgruppen von Grundschulkindern bis Erwachsenen Schülern an Berufs- oder Fachoberschulen eingebunden. 

Grafik \ref{fig:survey1}

\begin{figure}[ht]
  \includegraphics[width=\textwidth]{images/survey1.eps}
 \caption{Ich brauche noch einen sinnvollen Text.}
  \label{fig:survey1}
\end{figure}


Grafik \ref{fig:survey2}

\begin{figure}[ht]
  \includegraphics[width=\textwidth]{images/survey2.eps}
 \caption{Ich brauche noch einen sinnvollen Text.}
  \label{fig:survey2}
\end{figure}


Grafik \ref{fig:survey3}

\begin{figure}[ht]
  \includegraphics[width=\textwidth]{images/survey3.eps}
 \caption{Ich brauche noch einen sinnvollen Text.}
  \label{fig:survey3}
\end{figure}


Grafik \ref{fig:survey4}

\begin{figure}[ht]
  \includegraphics[width=\textwidth]{images/survey4.eps}
 \caption{Ich brauche noch einen sinnvollen Text.}
  \label{fig:survey4}
\end{figure}


Grafik \ref{fig:survey5}

\begin{figure}[ht]
  \includegraphics[width=\textwidth]{images/survey5.eps}
 \caption{Eignung der Klassenstufen für VRClassroom.}
  \label{fig:survey5}
\end{figure}




\begin{figure}[ht]
  \includegraphics[width=\textwidth]{images/survey6.eps}
 \caption{Net Promotor Score von VRClassrooms.}
  \label{fig:survey6}
\end{figure}


Grafik \ref{fig:survey7}

\begin{figure}[ht]
  \includegraphics[width=\textwidth]{images/survey7.eps}
 \caption{Einschätzungen zur Zukunft von VR und VRClassroom.}
  \label{fig:survey7}
\end{figure}


Grafik \ref{fig:survey8}

\begin{figure}[ht]
  \includegraphics[width=\textwidth]{images/survey8.eps}
 \caption{Ich brauche noch einen sinnvollen Text.}
  \label{fig:survey8}
\end{figure}


Grafik \ref{fig:survey9}

\begin{figure}[ht]
  \includegraphics[width=\textwidth]{images/survey9.eps}
 \caption{Demografische und persönliche Angaben.}
  \label{fig:survey9}
\end{figure}

\subsection{VRClassroom Studie in Schulen}
Da das System speziell für die Nutzung im Schulunterricht entwickelt wurde, sollte auch die Nutzerstudie in diesem Szenario durchgeführt werden. Dafür wurden Lehrkräfte angefragt, die dann einen Teil ihrer Unterrichtszeit für einen Test der Software verwenden wollten und im Anschluss bereit waren einen kurzen Fragebogen dazu zu beantworten.

Insgesamt hatten sich 15 Lehrkräfte bereit erklärt mit ihrer Klasse das VRClassroom System auszuprobieren und im Anschluss einen Fragebogen auszufüllen. 

\bigskip

Für den Test sollte allen SchülerInnen ein Cardboard zur Verfügung gestellt werden, das die Kinder mir ihrem eigenen Smartphone als VR-Brille genutzt sollten. 
Der Rechner, auf dem das Programm installiert war, sollte den Lehrkräften bereitgestellt und den Lehrkräften im Vorfeld eine kurze Einführung in die Software gegeben werden. Dies ist trotz Absage der Studie durch ein Video geschehen. Das Video wurde zudem für die Online-Umfrage benutzt. 

\bigskip

Für die Durchführung der Studie war ein Zeitrahmen von 15 bis 20 Minuten veranschlagt. In dieser Zeit sollte die Lehrkraft die App starten, sich alle Schüler-Geräte damit verbinden und die Lehrkraft die eigentlichen Inhalte präsentieren.

Damit alle Studienteilnehmer, Lehrer und Schüler gleichermaßen, die gleichen Erlebnisse haben und sich zu allen Funktionen des Systems eine Meinung bilden können, hatte jede Lehrkraft ein vollständiges Set an Inhalten mit jeweils mindestens einem 360°-Fotos, einem 360°-Video und einem 3D-Modell.

\subsubsection{Freigabe Kulturministerium}
Da alle Studien, die an Schulen gemacht werden, einer Freigabe des Kultusministeriums bedürfen, wurde im Vorfeld der Studie beim Kultusministerium eine solche Freigabe beantragt. Für den Antrag werden alle Fragebögen und der gesamte Ablauf der Studie an das Kultusministerium zur Prüfung vorgelegt. Das Kultusministerium hat die Durchführung von Studien beschränkt, um den Datenschutz der Schülerinnen und Schüler zu sichern. Da allerdings in der Studie nur die Lehrkräfte befragt werden und die Fragebögen komplett anonym gehalten sind, sind die Daten der Schüler zu keinem Zeitpunkt in Gefahr.

Leider wurde der Antrag nie beantwortet, sodass die gesamte Studie kurzfristig abgesagt werden musste. Es hatten sich bereits 15 Lehrkräfte bereit erklärt mit ihrer Klasse das VRClassroom System auszuprobieren und im Anschluss den Fragebogen zu beantworten, die sehr an dem System interessiert waren. Da das VRClassroom-System speziell für den Einsatz im Unterricht entwickelt wurde, wäre eine Nutzerstudie im Unterricht sehr sinnvoll gewesen, um abschätzen zu können, ob es im aktuellen Zustand einen guten Mehrwert bietet oder noch Anpassungen braucht um gut im Unterrichtsablauf zu funktionieren.



%______________________________________________________________________
\cleardoublepage

\section{Ausblick}
Allen Prognosen nach wird VR (Quelle dazu?) in den kommenden Jahren eine immer größere Rolle einnehmen, von Ausbildung, Beruf bis in die Freizeit hinein, wird es immer mehr VR-Geräte und -Programme geben. 
Im Bildungsbereich bietet VRClassroom bereits jetzt eine Möglichkeit für Lehrkräfte ohne großen finanziellen Aufwand und ohne viel Vorwissen zu benötigen, VR-Inhalte in ihren Unterricht einfließen zu lassen. Im Folgenden wird darauf eingegangen, wie die Zukunft von Virtual Reality aussehen kann beziehungsweise was an nächsten Entwicklungsstufen passieren muss, damit VR den prognostizierten Durchbruch auch haben wird.
Außerdem werden bestehende Probleme von VRClassroom besprochen und wie das System in der Zukunft weiterentwickelt werden könnte.

\subsection{Zukunft von VR}
VR wird sich noch weiter verbreiten, da Geräte günstiger werden und mehr Inhalte da sind
-> Artikel mit Prognosen?

Trotz dieser steigenden Zahlen und positive Prognosen ist davon auszugehen, dass es trotzdem noch eine Weile dauern wird, bis Virtual Reality den Weg in die Schulen finden wird. Denn wie auch in den Ergebnissen der Studie zu sehen sind, glauben nur 38\%, dass Virtual Reality - in welcher Form auch immer - eine große Rolle im Unterricht spielen wird. Außerdem haben selbst von den teilnehmenden Lehrkräften bis dato nur 43\% jemals selber eine VR-Brille ausprobiert. Erst wenn ein übermäßiger Teil der Lehrkräfte selbst Erfahrungen sammeln konnten und genug Unterrichts-geeignetes Material zur Verfügung steht, besteht die Chance, dass Virtual Reality im Unterrichtsgeschehen einbezogen werden wird. VRClassroom soll dabei einen Möglichkeit bieten, mit den Schülerinnen und Schülern erste Erfahrungen mit Unterrichtsmaterialien zu sammeln.

Nochmal Artikel von davor aufgreifen?
Was muss sich ändern, damit es sich noch mehr verbreitet?
 -> standalone Geräte, müssen mehr Rechenleistung, bessere Displays und weniger Gewicht haben
 -> besseres Tracking?
 -> mehr Anwendungen abseits von Games
 
 Geräteweiterentwicklungen
 - Gestenerkennung ohne Controller?
 - Gazeinput wie zB EyeVR Paper?

\subsection{Probleme von VRClassroom}
In Tests haben sich noch ein paar Probleme gezeigt, die das VRClassroom System in seinem aktuellen Zustand noch hat, die beim ``echten'' Einsatz im Schulunterricht das Erlebnis stören könnten.
Diese Problem konnten aber leider nicht im Rahmen dieser Arbeit gelöst werden, das sie nicht wirklich Probleme der entwickelten Software sind, sondern technischen Grenzen von Geräten, die sich in der Zukunft aller Voraussicht nach weiter verschieben werden, sodass sie dann kein Problem mehr darstellen.

\subsubsection{Anzeigen großer Dateien}
Eines der größten Probleme, die VRClassroom momentan hat, ist die Anzeige von komplexen 3D-Modellen oder 360°-Videos mit einer großen Dateigröße. 360°-Videos sind besonders problematisch, wenn sie eine extrem hohe Auflösung (>4096) und eine hohe Framerate haben. 

Für diese Probleme gibt es allerdings momentan nicht wirklich eine Lösung, da die Daten über das WLAN übertragen werden müssen und bei 25-30 Geräten, die verfügbare Bandbreite zwischen allen Geräten geteilt werden muss und das einfach nicht genug pro Gerät hergibt, um so hohe Datenmengen schnell genug zu übertragen. 

(Hier vielleicht noch Vorrechnen mit einem geläufigen Router einfügen?)

Zudem war bei Tests vereinzelt zu sehen, dass bei extrem komplexen Modellen WebVR crasht und ein Neuladen der Schüler-App herbeigeführt wird, sodass das Gerät aus dem Cardboard genommen und die WebVR-Ansicht neu aktiviert werden muss.

\subsubsection{Synchronisierung}
Ein weiteres Problem bei VRClassroom ist es, alle Veränderungen exakt synchron auf allen Geräten darzustellen. Besonders beim Abspielen von 360°-Videos fällt dieses Problem auf. 

Der Websocket-Server broadcastet die Nachricht über den veränderten Inhalt an alle Clients und da die Nachricht nur eine kurze JSON-Datei ist, wird sie auch instantan auf allen Geräten empfangen. Trotzdem kommt es zu geringen Verschiebungen der Anzeigeaktualisierung bei den einzelnen Geräten.
Der Ton der Videos ist dann um einen Bruchteil einer Sekunde verschoben, was zu unangenehmen Vermischungen der Sounds der verschiedenen Geräte und Halleffekten führt. 

Das kann dadurch umgangen werden, dass nur der Sound aus der Teacher-App abgespielt wird, die Schüler Ohrstöpsel tragen oder sie die Abspiellautstärke auf ihren Geräten weit herunterdrehen, dass es nicht durch den ganzen Raum schallt. Eine echte Lösung dafür gibt es allerdings nicht.

\subsection{Mögliche Weiterentwicklungen an VRClassroom}
VRClassroom ist ein vollfunktionsfähiges System, dass laut Umfrageergebnissen bereits bei einigen Lehrkräften das Interesse geweckt hat, es in ihrem Unterricht einzusetzen. Trotzdem sind in der Entwicklung und bei der Online-Befragung noch mögliche Weiterentwicklungen von VRClassroom aufgefallen, die zukünftig noch integriert werden könnten.

\subsubsection{Hosten von VRClassroom Online statt Verbindung mit Lehrer-Rechner}
Eine mögliche Weiterentwicklung von VRClassroom könnte sein das ganze VRClassroom System nicht lokal auf dem Rechner der Lehrkraft laufen zu lassen, mit dem sich dann alle VR-Geräte verbinden, sondern auch eine Version anzubieten, die Online gehostet ist.

Die Entscheidung VRClassroom so zu entwickeln, dass es lokal läuft wurde allerdings ganz bewusste gefällt: Durch Vorgespräche mit den Lehrkräften, die an der Studie teilnehmen wollten und aus eigener Erfahrung wurde schnell klar, dass jede Schule unterschiedlich gut mit Netzwerk versorgt ist. Es gibt Schulen, die für Schüler gar kein WLAN anbieten oder dieses stark beschränken, sodass direkte Verbindungen zwischen den Geräten im Netzwerk unterdrückt werden, oder andere Restriktionen in ihrem Netz haben.
Um diese Probleme zu umgehen, wurde dann die Entscheidung gefällt, VRClassroom so zu entwickeln, dass es unabhängig vom Internet funktionieren kann. Einzig die Streetview-Funktion fällt dann weg. So kann dann einfach ein lokales Netzwerk eröffnet werden mit dem sich alle Geräte verbinden und VRClassroom problemlos genutzt werden. Dies könnte zum Beispiel mit einem Aufbau gelöst werden, bei dem der Rechner auf dem VRClassroom installiert ist und ein vorkonfigurierter Router immer gemeinsam genutzt werden und sobald Strom angesteckt wird der Router sein lokales Netzwerk aufmacht.

Ein weiteres Argument für die aktuelle Implementierung ist das schon zuvor in (HIER REF) besprochene Problem des Ladens von komplexen Modellen oder hochaufgelösten Videos, also großen Datenmengen. Laden alle Geräte lokal vom Lehrercomputer wird nicht die gesamte Internetverbindung der Schule lahmgelegt und auch die Übertragungsrate ist höher als beim Laden aus dem Internet. 

Wird VRClassroom nur in einem lokalen Netzwerk benutzt fällt allerdings wie bereits oben erwähnt die Funktionen zum Anzeigen von Google Streetview Fotos weg, denn dafür wird die Verbindung zum Internet benötigt. 
Zudem könnte es für technisch weniger verzierte Lehrkräfte schwieriger sein nachzuvollziehen, wo das Problem liegt, wenn etwa ein Schüler Gerät sich nicht verbinden kann, weil es nicht im Netzwerk ist, in dem VRClassroom läuft. 
Diese Punkte würden wiederum eher dafür sprechen die Struktur zu ändern, sodass VRClassroom online gehostet wird. 

Für den Rahmen der Arbeit ist die Entscheidung dafür gefallen, VRClassroom über den Rechner der Lehrkraft zu hosten, aber eine Online-Version wäre defintiv eine interessante Weiterentwicklung.

\subsubsection{Skalierung von 3D-Modellen auf echte Größe}
Um den Schülern zu vermitteln wie groß beziehungsweise klein die Dinge in echt sind, die gerade besprochen werden, könnte eine Funktion in der Lehrer-App eingefügt werden, die die 3D-Modelle auf Originalgröße skaliert. Dazu müsste selbstverständlich alle 3D-Modelle in Originalgröße vorliegen, was nur sehr selten der Fall ist, da diese Funktion für die meisten Verwendungszwecke nicht benötigt wird und daher einfach unbeachtet bleibt.

Um die gezeigten Inhalte gut zu begreifen und auch in der echten Welt einschätzen zu können, würden Schüler allerdings sehr davon profitieren diesen Einblick zu haben. Denn erst wenn man direkt davor steht, kann man begreifen wie groß der Eiffelturm ist oder wie klein eine extrem giftige Spinne wie die schwarze Witwe in Wahrheit ist.

Wie schon erwähnt müssen dazu zum einen die Modelle in der richtigen Größe vorliegen, aber zum anderen müsste auch die VR-Classroom-App angepasst werden, um diese Funktion sinnvoll zu erfüllen. Wird das Modell auf Originalgröße skaliert, die kleiner ist als die Größe, in der man sie genauer betrachten würde ist das kein Problem und könnte direkt erfüllt werden. Problematisch wird die Skalierung wenn sie extrem viel größer wird als das Modell normal angesehen wird, wie zum Beispiel beim Betrachten eines Gebäudes. Wird das Modell in der App extrem groß skaliert, wird es ab einer gewissen Größe nicht mehr vollständig angezeigt und Teile abgeschnitten. Das passiert, da React360 darauf ausgelegt ist, dass die Kamera in einer Art Kugel ist, in der sie sehen kann. Das dient dazu den Rendering-Aufwand nicht zu extrem werden zu lassen. (QUELLE dafür?) Alles was die Kugel durchbricht wird nur bis zum Rand der Kugel gerendert und alles was außerhalb liegt abgeschnitten. 

\subsubsection{Navigation von 3D-Modellen aus Schüler-App}
Momentan können Schüler keinen Einfluss darauf nehmen, welchen Teil eines 3D-Modells sie sehen können und welchen nicht, da nur aus der Teacher-App gesteuert werden kann wie das Modell skaliert und gedreht wird. 
Eine mögliche Erweiterung von VRClassroom könnte sein, dass die Schüler die 3D-Modelle selbstständig drehen und skalieren können beziehungsweise zu anderen Punkten am Modell springen können. 


\subsubsection{Ausfragemodus: Ausgewählter Schüler setzt Markierung}
Eine mögliche Weiterentwicklung von VRClassroom wäre eine Art ``Ausfragemodus'', bei dem die Lehrkraft aus der Liste der verbundenen Geräte eins auswählen kann, das dann eine Markierung setzen kann. Das ausgewählte Gerät hätte dann die Möglichkeit einmalig eine Markierungen zu setzen.

Dafür wäre es notwendig für die Schüler-Geräte eine Möglichkeit zu geben einen Punkt auszuwählen, an dem die Markierung gesetzt werden soll. Bei der aktuellen Implementierung mit einem Smartphone in einem Google Cardboard ist das noch nicht möglich. Dazu müsste mit dem Touch-Event vom Cardboard an der aktuellen Gaze-Position des Nutzers an dieser Stelle ein Marker gesetzt werden. Mit einem Headset, das bereits einen zugehörigen Controller hat, wäre das einfacher zu lösen, da lediglich die Position des ``Rays'' des Controllers abgefragt werden müsste. 

Da die Markierungen auch in der Teacher App über das iFrame mit der Student-App gesetzt werden ist die Kommunikation der Markerposition an die Teacher-App bereits implementiert und müsste nur leicht abgeändert werden. Möglicherweise wäre es gut die von Schülern gesetzten Marker noch in einer anderen Farbe darzustellen wie die der Lehrkraft, um für andere Schüler leichter erkennbar zu machen, welche Marker von welcher Person kommen. 

Momentan können Schüler nicht beeinflussen wie sie 3D-Modelle sehen, sondern nur den von der Lehrkraft ausgewählten Blickwinkel. Um auch auf 3D-Modellen sinnvoll Markierungen setzen zu können wäre es dann auch sehr sinnvoll den Schülern die Möglichkeit zu geben den Blickwinkel des 3D-Modells zu verändern. 

%______________________________________________________________________

\cleardoublepage
\fancyhead[LE,RO,LO,RE]{} % Keine Kopfzeile mehr oben auf jeder Seite
\section*{Inhalt der beigelegten CD}
%______________________________________________________________________

\cleardoublepage

\bibliographystyle{IEEEtran}
\bibliography{literature}

\end{document}
